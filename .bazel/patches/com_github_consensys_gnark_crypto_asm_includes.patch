diff --git a/ecc/bls12-377/fp/BUILD.bazel b/ecc/bls12-377/fp/BUILD.bazel
index 3565a7d44b..08f932e72f 100644
--- a/ecc/bls12-377/fp/BUILD.bazel
+++ b/ecc/bls12-377/fp/BUILD.bazel
@@ -10,6 +10,8 @@ go_library(
         "element_amd64.s",
         "element_arm64.go",
         "element_arm64.s",
+        "//field/asm/element_6w:element_6w_amd64.h",
+        "//field/asm/element_6w:element_6w_arm64.h",
         "element_exp.go",
         "element_purego.go",
         "element_utils.go",
diff --git a/ecc/bls12-377/fp/element_amd64.s b/ecc/bls12-377/fp/element_amd64.s
index 822bdf56a5..c789c5ceda 100644
--- a/ecc/bls12-377/fp/element_amd64.s
+++ b/ecc/bls12-377/fp/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 18408829383245254329
-#include "../../../field/asm/element_6w/element_6w_amd64.s"
+#include "../../../field/asm/element_6w/element_6w_amd64.h"

diff --git a/ecc/bls12-377/fp/element_arm64.s b/ecc/bls12-377/fp/element_arm64.s
index 2defbe4c21..42bfbe2ab5 100644
--- a/ecc/bls12-377/fp/element_arm64.s
+++ b/ecc/bls12-377/fp/element_arm64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 15397482240260640864
-#include "../../../field/asm/element_6w/element_6w_arm64.s"
+#include "../../../field/asm/element_6w/element_6w_arm64.h"

diff --git a/ecc/bls12-377/fr/BUILD.bazel b/ecc/bls12-377/fr/BUILD.bazel
index f6f2516d22..be53298d6f 100644
--- a/ecc/bls12-377/fr/BUILD.bazel
+++ b/ecc/bls12-377/fr/BUILD.bazel
@@ -10,6 +10,8 @@ go_library(
         "element_amd64.s",
         "element_arm64.go",
         "element_arm64.s",
+        "//field/asm/element_4w:element_4w_amd64.h",
+        "//field/asm/element_4w:element_4w_arm64.h",
         "element_exp.go",
         "element_purego.go",
         "generator.go",
diff --git a/ecc/bls12-377/fr/element_amd64.s b/ecc/bls12-377/fr/element_amd64.s
index b45615aa36..87718f818d 100644
--- a/ecc/bls12-377/fr/element_amd64.s
+++ b/ecc/bls12-377/fr/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
+#include "../../../field/asm/element_4w/element_4w_amd64.h"

diff --git a/ecc/bls12-377/fr/element_arm64.s b/ecc/bls12-377/fr/element_arm64.s
index c8df07e345..bd4d877c43 100644
--- a/ecc/bls12-377/fr/element_arm64.s
+++ b/ecc/bls12-377/fr/element_arm64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
+#include "../../../field/asm/element_4w/element_4w_arm64.h"

diff --git a/ecc/bls12-381/fp/BUILD.bazel b/ecc/bls12-381/fp/BUILD.bazel
index 8d7396e9b4..d43e5978b1 100644
--- a/ecc/bls12-381/fp/BUILD.bazel
+++ b/ecc/bls12-381/fp/BUILD.bazel
@@ -10,6 +10,8 @@ go_library(
         "element_amd64.s",
         "element_arm64.go",
         "element_arm64.s",
+        "//field/asm/element_6w:element_6w_amd64.h",
+        "//field/asm/element_6w:element_6w_arm64.h",
         "element_exp.go",
         "element_purego.go",
         "vector.go",
diff --git a/ecc/bls12-381/fp/element_amd64.s b/ecc/bls12-381/fp/element_amd64.s
index 822bdf56a5..c789c5ceda 100644
--- a/ecc/bls12-381/fp/element_amd64.s
+++ b/ecc/bls12-381/fp/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 18408829383245254329
-#include "../../../field/asm/element_6w/element_6w_amd64.s"
+#include "../../../field/asm/element_6w/element_6w_amd64.h"

diff --git a/ecc/bls12-381/fp/element_arm64.s b/ecc/bls12-381/fp/element_arm64.s
index 2defbe4c21..42bfbe2ab5 100644
--- a/ecc/bls12-381/fp/element_arm64.s
+++ b/ecc/bls12-381/fp/element_arm64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 15397482240260640864
-#include "../../../field/asm/element_6w/element_6w_arm64.s"
+#include "../../../field/asm/element_6w/element_6w_arm64.h"

diff --git a/ecc/bls12-381/fr/BUILD.bazel b/ecc/bls12-381/fr/BUILD.bazel
index 6864d93f2f..c1bfe47ee1 100644
--- a/ecc/bls12-381/fr/BUILD.bazel
+++ b/ecc/bls12-381/fr/BUILD.bazel
@@ -10,6 +10,8 @@ go_library(
         "element_amd64.s",
         "element_arm64.go",
         "element_arm64.s",
+        "//field/asm/element_4w:element_4w_amd64.h",
+        "//field/asm/element_4w:element_4w_arm64.h",
         "element_exp.go",
         "element_purego.go",
         "generator.go",
diff --git a/ecc/bls12-381/fr/element_amd64.s b/ecc/bls12-381/fr/element_amd64.s
index b45615aa36..87718f818d 100644
--- a/ecc/bls12-381/fr/element_amd64.s
+++ b/ecc/bls12-381/fr/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
+#include "../../../field/asm/element_4w/element_4w_amd64.h"

diff --git a/ecc/bls12-381/fr/element_arm64.s b/ecc/bls12-381/fr/element_arm64.s
index c8df07e345..bd4d877c43 100644
--- a/ecc/bls12-381/fr/element_arm64.s
+++ b/ecc/bls12-381/fr/element_arm64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
+#include "../../../field/asm/element_4w/element_4w_arm64.h"

diff --git a/ecc/bls24-315/fp/BUILD.bazel b/ecc/bls24-315/fp/BUILD.bazel
index 169647dc27..287e979bca 100644
--- a/ecc/bls24-315/fp/BUILD.bazel
+++ b/ecc/bls24-315/fp/BUILD.bazel
@@ -8,6 +8,7 @@ go_library(
         "element.go",
         "element_amd64.go",
         "element_amd64.s",
+        "//field/asm/element_5w:element_5w_amd64.h",
         "element_exp.go",
         "element_purego.go",
         "vector.go",
diff --git a/ecc/bls24-315/fp/element_amd64.s b/ecc/bls24-315/fp/element_amd64.s
index ad0b92b14a..b26dc283c1 100644
--- a/ecc/bls24-315/fp/element_amd64.s
+++ b/ecc/bls24-315/fp/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 4600622797032586825
-#include "../../../field/asm/element_5w/element_5w_amd64.s"
+#include "../../../field/asm/element_5w/element_5w_amd64.h"

diff --git a/ecc/bls24-315/fr/BUILD.bazel b/ecc/bls24-315/fr/BUILD.bazel
index 633843dea3..3ffae78ddf 100644
--- a/ecc/bls24-315/fr/BUILD.bazel
+++ b/ecc/bls24-315/fr/BUILD.bazel
@@ -10,6 +10,8 @@ go_library(
         "element_amd64.s",
         "element_arm64.go",
         "element_arm64.s",
+        "//field/asm/element_4w:element_4w_amd64.h",
+        "//field/asm/element_4w:element_4w_arm64.h",
         "element_exp.go",
         "element_purego.go",
         "generator.go",
diff --git a/ecc/bls24-315/fr/element_amd64.s b/ecc/bls24-315/fr/element_amd64.s
index b45615aa36..87718f818d 100644
--- a/ecc/bls24-315/fr/element_amd64.s
+++ b/ecc/bls24-315/fr/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
+#include "../../../field/asm/element_4w/element_4w_amd64.h"

diff --git a/ecc/bls24-315/fr/element_arm64.s b/ecc/bls24-315/fr/element_arm64.s
index c8df07e345..bd4d877c43 100644
--- a/ecc/bls24-315/fr/element_arm64.s
+++ b/ecc/bls24-315/fr/element_arm64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
+#include "../../../field/asm/element_4w/element_4w_arm64.h"

diff --git a/ecc/bls24-317/fp/BUILD.bazel b/ecc/bls24-317/fp/BUILD.bazel
index 07fec9c3e8..3554987051 100644
--- a/ecc/bls24-317/fp/BUILD.bazel
+++ b/ecc/bls24-317/fp/BUILD.bazel
@@ -8,6 +8,7 @@ go_library(
         "element.go",
         "element_amd64.go",
         "element_amd64.s",
+        "//field/asm/element_5w:element_5w_amd64.h",
         "element_exp.go",
         "element_purego.go",
         "vector.go",
diff --git a/ecc/bls24-317/fp/element_amd64.s b/ecc/bls24-317/fp/element_amd64.s
index ad0b92b14a..b26dc283c1 100644
--- a/ecc/bls24-317/fp/element_amd64.s
+++ b/ecc/bls24-317/fp/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 4600622797032586825
-#include "../../../field/asm/element_5w/element_5w_amd64.s"
+#include "../../../field/asm/element_5w/element_5w_amd64.h"

diff --git a/ecc/bls24-317/fr/BUILD.bazel b/ecc/bls24-317/fr/BUILD.bazel
index 06d878c466..bcceb92183 100644
--- a/ecc/bls24-317/fr/BUILD.bazel
+++ b/ecc/bls24-317/fr/BUILD.bazel
@@ -10,6 +10,8 @@ go_library(
         "element_amd64.s",
         "element_arm64.go",
         "element_arm64.s",
+        "//field/asm/element_4w:element_4w_amd64.h",
+        "//field/asm/element_4w:element_4w_arm64.h",
         "element_exp.go",
         "element_purego.go",
         "generator.go",
diff --git a/ecc/bls24-317/fr/element_amd64.s b/ecc/bls24-317/fr/element_amd64.s
index b45615aa36..87718f818d 100644
--- a/ecc/bls24-317/fr/element_amd64.s
+++ b/ecc/bls24-317/fr/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
+#include "../../../field/asm/element_4w/element_4w_amd64.h"

diff --git a/ecc/bls24-317/fr/element_arm64.s b/ecc/bls24-317/fr/element_arm64.s
index c8df07e345..bd4d877c43 100644
--- a/ecc/bls24-317/fr/element_arm64.s
+++ b/ecc/bls24-317/fr/element_arm64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
+#include "../../../field/asm/element_4w/element_4w_arm64.h"

diff --git a/ecc/bn254/fp/BUILD.bazel b/ecc/bn254/fp/BUILD.bazel
index 969c8b0c61..2300c44796 100644
--- a/ecc/bn254/fp/BUILD.bazel
+++ b/ecc/bn254/fp/BUILD.bazel
@@ -10,6 +10,8 @@ go_library(
         "element_amd64.s",
         "element_arm64.go",
         "element_arm64.s",
+        "//field/asm/element_4w:element_4w_amd64.h",
+        "//field/asm/element_4w:element_4w_arm64.h",
         "element_exp.go",
         "element_purego.go",
         "vector.go",
diff --git a/ecc/bn254/fp/element_amd64.s b/ecc/bn254/fp/element_amd64.s
index b45615aa36..87718f818d 100644
--- a/ecc/bn254/fp/element_amd64.s
+++ b/ecc/bn254/fp/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
+#include "../../../field/asm/element_4w/element_4w_amd64.h"

diff --git a/ecc/bn254/fp/element_arm64.s b/ecc/bn254/fp/element_arm64.s
index c8df07e345..bd4d877c43 100644
--- a/ecc/bn254/fp/element_arm64.s
+++ b/ecc/bn254/fp/element_arm64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
+#include "../../../field/asm/element_4w/element_4w_arm64.h"

diff --git a/ecc/bn254/fr/BUILD.bazel b/ecc/bn254/fr/BUILD.bazel
index d7f31483b6..7bf435b4fb 100644
--- a/ecc/bn254/fr/BUILD.bazel
+++ b/ecc/bn254/fr/BUILD.bazel
@@ -10,6 +10,8 @@ go_library(
         "element_amd64.s",
         "element_arm64.go",
         "element_arm64.s",
+        "//field/asm/element_4w:element_4w_amd64.h",
+        "//field/asm/element_4w:element_4w_arm64.h",
         "element_exp.go",
         "element_purego.go",
         "generator.go",
diff --git a/ecc/bn254/fr/element_amd64.s b/ecc/bn254/fr/element_amd64.s
index b45615aa36..87718f818d 100644
--- a/ecc/bn254/fr/element_amd64.s
+++ b/ecc/bn254/fr/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
+#include "../../../field/asm/element_4w/element_4w_amd64.h"

diff --git a/ecc/bn254/fr/element_arm64.s b/ecc/bn254/fr/element_arm64.s
index c8df07e345..bd4d877c43 100644
--- a/ecc/bn254/fr/element_arm64.s
+++ b/ecc/bn254/fr/element_arm64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
+#include "../../../field/asm/element_4w/element_4w_arm64.h"

diff --git a/ecc/bw6-633/fp/BUILD.bazel b/ecc/bw6-633/fp/BUILD.bazel
index 268054ddbe..a0581036be 100644
--- a/ecc/bw6-633/fp/BUILD.bazel
+++ b/ecc/bw6-633/fp/BUILD.bazel
@@ -11,6 +11,8 @@ go_library(
         "element_amd64.s",
         "element_arm64.go",
         "element_arm64.s",
+        "//field/asm/element_10w:element_10w_amd64.h",
+        "//field/asm/element_10w:element_10w_arm64.h",
         "element_exp.go",
         "element_purego.go",
         "vector.go",
diff --git a/ecc/bw6-633/fp/element_amd64.s b/ecc/bw6-633/fp/element_amd64.s
index 1c33ea1f4d..e335eae970 100644
--- a/ecc/bw6-633/fp/element_amd64.s
+++ b/ecc/bw6-633/fp/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 15320537321640126458
-#include "../../../field/asm/element_10w/element_10w_amd64.s"
+#include "../../../field/asm/element_10w/element_10w_amd64.h"

diff --git a/ecc/bw6-633/fp/element_arm64.s b/ecc/bw6-633/fp/element_arm64.s
index 15ac147c2c..7c0dc3febe 100644
--- a/ecc/bw6-633/fp/element_arm64.s
+++ b/ecc/bw6-633/fp/element_arm64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 4283725514119985738
-#include "../../../field/asm/element_10w/element_10w_arm64.s"
+#include "../../../field/asm/element_10w/element_10w_arm64.h"

diff --git a/ecc/bw6-633/fr/BUILD.bazel b/ecc/bw6-633/fr/BUILD.bazel
index e873b16306..05d996c7a8 100644
--- a/ecc/bw6-633/fr/BUILD.bazel
+++ b/ecc/bw6-633/fr/BUILD.bazel
@@ -8,6 +8,7 @@ go_library(
         "element.go",
         "element_amd64.go",
         "element_amd64.s",
+        "//field/asm/element_5w:element_5w_amd64.h",
         "element_exp.go",
         "element_purego.go",
         "generator.go",
diff --git a/ecc/bw6-633/fr/element_amd64.s b/ecc/bw6-633/fr/element_amd64.s
index ad0b92b14a..b26dc283c1 100644
--- a/ecc/bw6-633/fr/element_amd64.s
+++ b/ecc/bw6-633/fr/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 4600622797032586825
-#include "../../../field/asm/element_5w/element_5w_amd64.s"
+#include "../../../field/asm/element_5w/element_5w_amd64.h"

diff --git a/ecc/bw6-761/fp/BUILD.bazel b/ecc/bw6-761/fp/BUILD.bazel
index 999d6359a6..142e82b7fb 100644
--- a/ecc/bw6-761/fp/BUILD.bazel
+++ b/ecc/bw6-761/fp/BUILD.bazel
@@ -11,6 +11,8 @@ go_library(
         "element_amd64.s",
         "element_arm64.go",
         "element_arm64.s",
+        "//field/asm/element_12w:element_12w_amd64.h",
+        "//field/asm/element_12w:element_12w_arm64.h",
         "element_exp.go",
         "element_purego.go",
         "vector.go",
diff --git a/ecc/bw6-761/fp/element_amd64.s b/ecc/bw6-761/fp/element_amd64.s
index a62fdba194..9ef490d6a6 100644
--- a/ecc/bw6-761/fp/element_amd64.s
+++ b/ecc/bw6-761/fp/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 9592094295531092101
-#include "../../../field/asm/element_12w/element_12w_amd64.s"
+#include "../../../field/asm/element_12w/element_12w_amd64.h"

diff --git a/ecc/bw6-761/fp/element_arm64.s b/ecc/bw6-761/fp/element_arm64.s
index e90c39f822..996b684017 100644
--- a/ecc/bw6-761/fp/element_arm64.s
+++ b/ecc/bw6-761/fp/element_arm64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 17465962485072383759
-#include "../../../field/asm/element_12w/element_12w_arm64.s"
+#include "../../../field/asm/element_12w/element_12w_arm64.h"

diff --git a/ecc/bw6-761/fr/BUILD.bazel b/ecc/bw6-761/fr/BUILD.bazel
index 93d11492dd..3f05af3d59 100644
--- a/ecc/bw6-761/fr/BUILD.bazel
+++ b/ecc/bw6-761/fr/BUILD.bazel
@@ -10,6 +10,8 @@ go_library(
         "element_amd64.s",
         "element_arm64.go",
         "element_arm64.s",
+        "//field/asm/element_6w:element_6w_amd64.h",
+        "//field/asm/element_6w:element_6w_arm64.h",
         "element_exp.go",
         "element_purego.go",
         "generator.go",
diff --git a/ecc/bw6-761/fr/element_amd64.s b/ecc/bw6-761/fr/element_amd64.s
index 822bdf56a5..c789c5ceda 100644
--- a/ecc/bw6-761/fr/element_amd64.s
+++ b/ecc/bw6-761/fr/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 18408829383245254329
-#include "../../../field/asm/element_6w/element_6w_amd64.s"
+#include "../../../field/asm/element_6w/element_6w_amd64.h"

diff --git a/ecc/bw6-761/fr/element_arm64.s b/ecc/bw6-761/fr/element_arm64.s
index 2defbe4c21..42bfbe2ab5 100644
--- a/ecc/bw6-761/fr/element_arm64.s
+++ b/ecc/bw6-761/fr/element_arm64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 15397482240260640864
-#include "../../../field/asm/element_6w/element_6w_arm64.s"
+#include "../../../field/asm/element_6w/element_6w_arm64.h"

diff --git a/ecc/grumpkin/fp/BUILD.bazel b/ecc/grumpkin/fp/BUILD.bazel
index c0e049b3e1..a711b90689 100644
--- a/ecc/grumpkin/fp/BUILD.bazel
+++ b/ecc/grumpkin/fp/BUILD.bazel
@@ -10,6 +10,8 @@ go_library(
         "element_amd64.s",
         "element_arm64.go",
         "element_arm64.s",
+        "//field/asm/element_4w:element_4w_amd64.h",
+        "//field/asm/element_4w:element_4w_arm64.h",
         "element_exp.go",
         "element_purego.go",
         "vector.go",
diff --git a/ecc/grumpkin/fp/element_amd64.s b/ecc/grumpkin/fp/element_amd64.s
index b45615aa36..87718f818d 100644
--- a/ecc/grumpkin/fp/element_amd64.s
+++ b/ecc/grumpkin/fp/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
+#include "../../../field/asm/element_4w/element_4w_amd64.h"

diff --git a/ecc/grumpkin/fp/element_arm64.s b/ecc/grumpkin/fp/element_arm64.s
index c8df07e345..bd4d877c43 100644
--- a/ecc/grumpkin/fp/element_arm64.s
+++ b/ecc/grumpkin/fp/element_arm64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
+#include "../../../field/asm/element_4w/element_4w_arm64.h"

diff --git a/ecc/grumpkin/fr/BUILD.bazel b/ecc/grumpkin/fr/BUILD.bazel
index 126c87edf1..8d51aa8a6a 100644
--- a/ecc/grumpkin/fr/BUILD.bazel
+++ b/ecc/grumpkin/fr/BUILD.bazel
@@ -10,6 +10,8 @@ go_library(
         "element_amd64.s",
         "element_arm64.go",
         "element_arm64.s",
+        "//field/asm/element_4w:element_4w_amd64.h",
+        "//field/asm/element_4w:element_4w_arm64.h",
         "element_exp.go",
         "element_purego.go",
         "vector.go",
diff --git a/ecc/grumpkin/fr/element_amd64.s b/ecc/grumpkin/fr/element_amd64.s
index b45615aa36..87718f818d 100644
--- a/ecc/grumpkin/fr/element_amd64.s
+++ b/ecc/grumpkin/fr/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
+#include "../../../field/asm/element_4w/element_4w_amd64.h"

diff --git a/ecc/grumpkin/fr/element_arm64.s b/ecc/grumpkin/fr/element_arm64.s
index c8df07e345..bd4d877c43 100644
--- a/ecc/grumpkin/fr/element_arm64.s
+++ b/ecc/grumpkin/fr/element_arm64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
+#include "../../../field/asm/element_4w/element_4w_arm64.h"

diff --git a/ecc/stark-curve/fp/BUILD.bazel b/ecc/stark-curve/fp/BUILD.bazel
index bf68d0b973..c54b3c5eeb 100644
--- a/ecc/stark-curve/fp/BUILD.bazel
+++ b/ecc/stark-curve/fp/BUILD.bazel
@@ -10,6 +10,8 @@ go_library(
         "element_amd64.s",
         "element_arm64.go",
         "element_arm64.s",
+        "//field/asm/element_4w:element_4w_amd64.h",
+        "//field/asm/element_4w:element_4w_arm64.h",
         "element_exp.go",
         "element_purego.go",
         "vector.go",
diff --git a/ecc/stark-curve/fp/element_amd64.s b/ecc/stark-curve/fp/element_amd64.s
index b45615aa36..87718f818d 100644
--- a/ecc/stark-curve/fp/element_amd64.s
+++ b/ecc/stark-curve/fp/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
+#include "../../../field/asm/element_4w/element_4w_amd64.h"

diff --git a/ecc/stark-curve/fp/element_arm64.s b/ecc/stark-curve/fp/element_arm64.s
index c8df07e345..bd4d877c43 100644
--- a/ecc/stark-curve/fp/element_arm64.s
+++ b/ecc/stark-curve/fp/element_arm64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
+#include "../../../field/asm/element_4w/element_4w_arm64.h"

diff --git a/ecc/stark-curve/fr/BUILD.bazel b/ecc/stark-curve/fr/BUILD.bazel
index e89c4231e3..2348fe6250 100644
--- a/ecc/stark-curve/fr/BUILD.bazel
+++ b/ecc/stark-curve/fr/BUILD.bazel
@@ -10,6 +10,8 @@ go_library(
         "element_amd64.s",
         "element_arm64.go",
         "element_arm64.s",
+        "//field/asm/element_4w:element_4w_amd64.h",
+        "//field/asm/element_4w:element_4w_arm64.h",
         "element_purego.go",
         "vector.go",
         "vector_amd64.go",
diff --git a/ecc/stark-curve/fr/element_amd64.s b/ecc/stark-curve/fr/element_amd64.s
index b45615aa36..87718f818d 100644
--- a/ecc/stark-curve/fr/element_amd64.s
+++ b/ecc/stark-curve/fr/element_amd64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
+#include "../../../field/asm/element_4w/element_4w_amd64.h"

diff --git a/ecc/stark-curve/fr/element_arm64.s b/ecc/stark-curve/fr/element_arm64.s
index c8df07e345..bd4d877c43 100644
--- a/ecc/stark-curve/fr/element_arm64.s
+++ b/ecc/stark-curve/fr/element_arm64.s
@@ -6,5 +6,5 @@
 // Code generated by consensys/gnark-crypto DO NOT EDIT

 // We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
+#include "../../../field/asm/element_4w/element_4w_arm64.h"

diff --git a/field/asm/element_10w/BUILD.bazel b/field/asm/element_10w/BUILD.bazel
index 337130a0a7..1d0a2cde24 100644
--- a/field/asm/element_10w/BUILD.bazel
+++ b/field/asm/element_10w/BUILD.bazel
@@ -16,3 +16,9 @@ alias(
     actual = ":element_10w",
     visibility = ["//visibility:public"],
 )
+
+exports_files([
+    "element_10w_amd64.h",
+    "element_10w_arm64.h",
+])
+
diff --git a/tmp/gnark-crypto-new/field/asm/element_10w/element_10w_amd64.h b/field/asm/element_10w/element_10w_amd64.h
new file mode 100644
index 0000000000..c45206cc5c
--- /dev/null
+++ b/field/asm/element_10w/element_10w_amd64.h
@@ -0,0 +1,1187 @@
+// Code generated by gnark-crypto/generator. DO NOT EDIT.
+#include "textflag.h"
+#include "funcdata.h"
+#include "go_asm.h"
+
+#define REDUCE(ra0, ra1, ra2, ra3, ra4, ra5, ra6, ra7, ra8, ra9, rb0, rb1, rb2, rb3, rb4, rb5, rb6, rb7, rb8, rb9) \
+	MOVQ    ra0, rb0;              \
+	SUBQ    ·qElement(SB), ra0;    \
+	MOVQ    ra1, rb1;              \
+	SBBQ    ·qElement+8(SB), ra1;  \
+	MOVQ    ra2, rb2;              \
+	SBBQ    ·qElement+16(SB), ra2; \
+	MOVQ    ra3, rb3;              \
+	SBBQ    ·qElement+24(SB), ra3; \
+	MOVQ    ra4, rb4;              \
+	SBBQ    ·qElement+32(SB), ra4; \
+	MOVQ    ra5, rb5;              \
+	SBBQ    ·qElement+40(SB), ra5; \
+	MOVQ    ra6, rb6;              \
+	SBBQ    ·qElement+48(SB), ra6; \
+	MOVQ    ra7, rb7;              \
+	SBBQ    ·qElement+56(SB), ra7; \
+	MOVQ    ra8, rb8;              \
+	SBBQ    ·qElement+64(SB), ra8; \
+	MOVQ    ra9, rb9;              \
+	SBBQ    ·qElement+72(SB), ra9; \
+	CMOVQCS rb0, ra0;              \
+	CMOVQCS rb1, ra1;              \
+	CMOVQCS rb2, ra2;              \
+	CMOVQCS rb3, ra3;              \
+	CMOVQCS rb4, ra4;              \
+	CMOVQCS rb5, ra5;              \
+	CMOVQCS rb6, ra6;              \
+	CMOVQCS rb7, ra7;              \
+	CMOVQCS rb8, ra8;              \
+	CMOVQCS rb9, ra9;              \
+
+TEXT ·reduce(SB), $56-8
+	MOVQ res+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	MOVQ 40(AX), R8
+	MOVQ 48(AX), R9
+	MOVQ 56(AX), R10
+	MOVQ 64(AX), R11
+	MOVQ 72(AX), R12
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	MOVQ R8, 40(AX)
+	MOVQ R9, 48(AX)
+	MOVQ R10, 56(AX)
+	MOVQ R11, 64(AX)
+	MOVQ R12, 72(AX)
+	RET
+
+// MulBy3(x *Element)
+TEXT ·MulBy3(SB), $56-8
+	MOVQ x+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	MOVQ 40(AX), R8
+	MOVQ 48(AX), R9
+	MOVQ 56(AX), R10
+	MOVQ 64(AX), R11
+	MOVQ 72(AX), R12
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+	ADCQ R9, R9
+	ADCQ R10, R10
+	ADCQ R11, R11
+	ADCQ R12, R12
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+
+	ADDQ 0(AX), DX
+	ADCQ 8(AX), CX
+	ADCQ 16(AX), BX
+	ADCQ 24(AX), SI
+	ADCQ 32(AX), DI
+	ADCQ 40(AX), R8
+	ADCQ 48(AX), R9
+	ADCQ 56(AX), R10
+	ADCQ 64(AX), R11
+	ADCQ 72(AX), R12
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	MOVQ R8, 40(AX)
+	MOVQ R9, 48(AX)
+	MOVQ R10, 56(AX)
+	MOVQ R11, 64(AX)
+	MOVQ R12, 72(AX)
+	RET
+
+// MulBy5(x *Element)
+TEXT ·MulBy5(SB), $56-8
+	MOVQ x+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	MOVQ 40(AX), R8
+	MOVQ 48(AX), R9
+	MOVQ 56(AX), R10
+	MOVQ 64(AX), R11
+	MOVQ 72(AX), R12
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+	ADCQ R9, R9
+	ADCQ R10, R10
+	ADCQ R11, R11
+	ADCQ R12, R12
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+	ADCQ R9, R9
+	ADCQ R10, R10
+	ADCQ R11, R11
+	ADCQ R12, R12
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+
+	ADDQ 0(AX), DX
+	ADCQ 8(AX), CX
+	ADCQ 16(AX), BX
+	ADCQ 24(AX), SI
+	ADCQ 32(AX), DI
+	ADCQ 40(AX), R8
+	ADCQ 48(AX), R9
+	ADCQ 56(AX), R10
+	ADCQ 64(AX), R11
+	ADCQ 72(AX), R12
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	MOVQ R8, 40(AX)
+	MOVQ R9, 48(AX)
+	MOVQ R10, 56(AX)
+	MOVQ R11, 64(AX)
+	MOVQ R12, 72(AX)
+	RET
+
+// MulBy13(x *Element)
+TEXT ·MulBy13(SB), $136-8
+	MOVQ x+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	MOVQ 40(AX), R8
+	MOVQ 48(AX), R9
+	MOVQ 56(AX), R10
+	MOVQ 64(AX), R11
+	MOVQ 72(AX), R12
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+	ADCQ R9, R9
+	ADCQ R10, R10
+	ADCQ R11, R11
+	ADCQ R12, R12
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+	ADCQ R9, R9
+	ADCQ R10, R10
+	ADCQ R11, R11
+	ADCQ R12, R12
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP),s11-96(SP),s12-104(SP),s13-112(SP),s14-120(SP),s15-128(SP),s16-136(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP),s11-96(SP),s12-104(SP),s13-112(SP),s14-120(SP),s15-128(SP),s16-136(SP))
+
+	MOVQ DX, s7-64(SP)
+	MOVQ CX, s8-72(SP)
+	MOVQ BX, s9-80(SP)
+	MOVQ SI, s10-88(SP)
+	MOVQ DI, s11-96(SP)
+	MOVQ R8, s12-104(SP)
+	MOVQ R9, s13-112(SP)
+	MOVQ R10, s14-120(SP)
+	MOVQ R11, s15-128(SP)
+	MOVQ R12, s16-136(SP)
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+	ADCQ R9, R9
+	ADCQ R10, R10
+	ADCQ R11, R11
+	ADCQ R12, R12
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+
+	ADDQ s7-64(SP), DX
+	ADCQ s8-72(SP), CX
+	ADCQ s9-80(SP), BX
+	ADCQ s10-88(SP), SI
+	ADCQ s11-96(SP), DI
+	ADCQ s12-104(SP), R8
+	ADCQ s13-112(SP), R9
+	ADCQ s14-120(SP), R10
+	ADCQ s15-128(SP), R11
+	ADCQ s16-136(SP), R12
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+
+	ADDQ 0(AX), DX
+	ADCQ 8(AX), CX
+	ADCQ 16(AX), BX
+	ADCQ 24(AX), SI
+	ADCQ 32(AX), DI
+	ADCQ 40(AX), R8
+	ADCQ 48(AX), R9
+	ADCQ 56(AX), R10
+	ADCQ 64(AX), R11
+	ADCQ 72(AX), R12
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	MOVQ R8, 40(AX)
+	MOVQ R9, 48(AX)
+	MOVQ R10, 56(AX)
+	MOVQ R11, 64(AX)
+	MOVQ R12, 72(AX)
+	RET
+
+// Butterfly(a, b *Element) sets a = a + b; b = a - b
+TEXT ·Butterfly(SB), $56-16
+	MOVQ b+8(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	MOVQ 40(AX), R8
+	MOVQ 48(AX), R9
+	MOVQ 56(AX), R10
+	MOVQ 64(AX), R11
+	MOVQ 72(AX), R12
+	MOVQ a+0(FP), AX
+	ADDQ 0(AX), DX
+	ADCQ 8(AX), CX
+	ADCQ 16(AX), BX
+	ADCQ 24(AX), SI
+	ADCQ 32(AX), DI
+	ADCQ 40(AX), R8
+	ADCQ 48(AX), R9
+	ADCQ 56(AX), R10
+	ADCQ 64(AX), R11
+	ADCQ 72(AX), R12
+	MOVQ DX, R13
+	MOVQ CX, R14
+	MOVQ BX, R15
+	MOVQ SI, s0-8(SP)
+	MOVQ DI, s1-16(SP)
+	MOVQ R8, s2-24(SP)
+	MOVQ R9, s3-32(SP)
+	MOVQ R10, s4-40(SP)
+	MOVQ R11, s5-48(SP)
+	MOVQ R12, s6-56(SP)
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	MOVQ 40(AX), R8
+	MOVQ 48(AX), R9
+	MOVQ 56(AX), R10
+	MOVQ 64(AX), R11
+	MOVQ 72(AX), R12
+	MOVQ b+8(FP), AX
+	SUBQ 0(AX), DX
+	SBBQ 8(AX), CX
+	SBBQ 16(AX), BX
+	SBBQ 24(AX), SI
+	SBBQ 32(AX), DI
+	SBBQ 40(AX), R8
+	SBBQ 48(AX), R9
+	SBBQ 56(AX), R10
+	SBBQ 64(AX), R11
+	SBBQ 72(AX), R12
+	JCC  noReduce_1
+	MOVQ $const_q0, AX
+	ADDQ AX, DX
+	MOVQ $const_q1, AX
+	ADCQ AX, CX
+	MOVQ $const_q2, AX
+	ADCQ AX, BX
+	MOVQ $const_q3, AX
+	ADCQ AX, SI
+	MOVQ $const_q4, AX
+	ADCQ AX, DI
+	MOVQ $const_q5, AX
+	ADCQ AX, R8
+	MOVQ $const_q6, AX
+	ADCQ AX, R9
+	MOVQ $const_q7, AX
+	ADCQ AX, R10
+	MOVQ $const_q8, AX
+	ADCQ AX, R11
+	MOVQ $const_q9, AX
+	ADCQ AX, R12
+
+noReduce_1:
+	MOVQ b+8(FP), AX
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	MOVQ R8, 40(AX)
+	MOVQ R9, 48(AX)
+	MOVQ R10, 56(AX)
+	MOVQ R11, 64(AX)
+	MOVQ R12, 72(AX)
+	MOVQ R13, DX
+	MOVQ R14, CX
+	MOVQ R15, BX
+	MOVQ s0-8(SP), SI
+	MOVQ s1-16(SP), DI
+	MOVQ s2-24(SP), R8
+	MOVQ s3-32(SP), R9
+	MOVQ s4-40(SP), R10
+	MOVQ s5-48(SP), R11
+	MOVQ s6-56(SP), R12
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
+
+	MOVQ a+0(FP), AX
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	MOVQ R8, 40(AX)
+	MOVQ R9, 48(AX)
+	MOVQ R10, 56(AX)
+	MOVQ R11, 64(AX)
+	MOVQ R12, 72(AX)
+	RET
+
+// mul(res, x, y *Element)
+TEXT ·mul(SB), $64-24
+
+	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
+	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
+	// See github.com/Consensys/gnark-crypto/field/generator for more comments.
+
+	NO_LOCAL_POINTERS
+	CMPB ·supportAdx(SB), $1
+	JNE  noAdx_2
+	MOVQ x+8(FP), R12
+	MOVQ y+16(FP), R13
+
+	// A -> BP
+	// t[0] -> R14
+	// t[1] -> R15
+	// t[2] -> CX
+	// t[3] -> BX
+	// t[4] -> SI
+	// t[5] -> DI
+	// t[6] -> R8
+	// t[7] -> R9
+	// t[8] -> R10
+	// t[9] -> R11
+#define MACC(in0, in1, in2) \
+	ADCXQ in0, in1     \
+	MULXQ in2, AX, in0 \
+	ADOXQ AX, in1      \
+
+#define DIV_SHIFT() \
+	PUSHQ BP                         \
+	MOVQ  $const_qInvNeg, DX         \
+	IMULQ R14, DX                    \
+	XORQ  AX, AX                     \
+	MULXQ ·qElement+0(SB), AX, BP    \
+	ADCXQ R14, AX                    \
+	MOVQ  BP, R14                    \
+	POPQ  BP                         \
+	MACC(R15, R14, ·qElement+8(SB))  \
+	MACC(CX, R15, ·qElement+16(SB))  \
+	MACC(BX, CX, ·qElement+24(SB))   \
+	MACC(SI, BX, ·qElement+32(SB))   \
+	MACC(DI, SI, ·qElement+40(SB))   \
+	MACC(R8, DI, ·qElement+48(SB))   \
+	MACC(R9, R8, ·qElement+56(SB))   \
+	MACC(R10, R9, ·qElement+64(SB))  \
+	MACC(R11, R10, ·qElement+72(SB)) \
+	MOVQ  $0, AX                     \
+	ADCXQ AX, R11                    \
+	ADOXQ BP, R11                    \
+
+#define MUL_WORD_0() \
+	XORQ  AX, AX           \
+	MULXQ 0(R12), R14, R15 \
+	MULXQ 8(R12), AX, CX   \
+	ADOXQ AX, R15          \
+	MULXQ 16(R12), AX, BX  \
+	ADOXQ AX, CX           \
+	MULXQ 24(R12), AX, SI  \
+	ADOXQ AX, BX           \
+	MULXQ 32(R12), AX, DI  \
+	ADOXQ AX, SI           \
+	MULXQ 40(R12), AX, R8  \
+	ADOXQ AX, DI           \
+	MULXQ 48(R12), AX, R9  \
+	ADOXQ AX, R8           \
+	MULXQ 56(R12), AX, R10 \
+	ADOXQ AX, R9           \
+	MULXQ 64(R12), AX, R11 \
+	ADOXQ AX, R10          \
+	MULXQ 72(R12), AX, BP  \
+	ADOXQ AX, R11          \
+	MOVQ  $0, AX           \
+	ADOXQ AX, BP           \
+	DIV_SHIFT()            \
+
+#define MUL_WORD_N() \
+	XORQ  AX, AX           \
+	MULXQ 0(R12), AX, BP   \
+	ADOXQ AX, R14          \
+	MACC(BP, R15, 8(R12))  \
+	MACC(BP, CX, 16(R12))  \
+	MACC(BP, BX, 24(R12))  \
+	MACC(BP, SI, 32(R12))  \
+	MACC(BP, DI, 40(R12))  \
+	MACC(BP, R8, 48(R12))  \
+	MACC(BP, R9, 56(R12))  \
+	MACC(BP, R10, 64(R12)) \
+	MACC(BP, R11, 72(R12)) \
+	MOVQ  $0, AX           \
+	ADCXQ AX, BP           \
+	ADOXQ AX, BP           \
+	DIV_SHIFT()            \
+
+	// mul body
+	MOVQ 0(R13), DX
+	MUL_WORD_0()
+	MOVQ 8(R13), DX
+	MUL_WORD_N()
+	MOVQ 16(R13), DX
+	MUL_WORD_N()
+	MOVQ 24(R13), DX
+	MUL_WORD_N()
+	MOVQ 32(R13), DX
+	MUL_WORD_N()
+	MOVQ 40(R13), DX
+	MUL_WORD_N()
+	MOVQ 48(R13), DX
+	MUL_WORD_N()
+	MOVQ 56(R13), DX
+	MUL_WORD_N()
+	MOVQ 64(R13), DX
+	MUL_WORD_N()
+	MOVQ 72(R13), DX
+	MUL_WORD_N()
+
+	// reduce element(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11) using temp registers (R12,R13,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP))
+	REDUCE(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP))
+
+	MOVQ res+0(FP), AX
+	MOVQ R14, 0(AX)
+	MOVQ R15, 8(AX)
+	MOVQ CX, 16(AX)
+	MOVQ BX, 24(AX)
+	MOVQ SI, 32(AX)
+	MOVQ DI, 40(AX)
+	MOVQ R8, 48(AX)
+	MOVQ R9, 56(AX)
+	MOVQ R10, 64(AX)
+	MOVQ R11, 72(AX)
+	RET
+
+noAdx_2:
+	MOVQ res+0(FP), AX
+	MOVQ AX, (SP)
+	MOVQ x+8(FP), AX
+	MOVQ AX, 8(SP)
+	MOVQ y+16(FP), AX
+	MOVQ AX, 16(SP)
+	CALL ·_mulGeneric(SB)
+	RET
+
+TEXT ·fromMont(SB), $64-8
+	NO_LOCAL_POINTERS
+
+	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
+	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
+	// when y = 1 we have:
+	// for i=0 to N-1
+	// 		t[i] = x[i]
+	// for i=0 to N-1
+	// 		m := t[0]*q'[0] mod W
+	// 		C,_ := t[0] + m*q[0]
+	// 		for j=1 to N-1
+	// 		    (C,t[j-1]) := t[j] + m*q[j] + C
+	// 		t[N-1] = C
+	CMPB ·supportAdx(SB), $1
+	JNE  noAdx_3
+	MOVQ res+0(FP), DX
+	MOVQ 0(DX), R14
+	MOVQ 8(DX), R15
+	MOVQ 16(DX), CX
+	MOVQ 24(DX), BX
+	MOVQ 32(DX), SI
+	MOVQ 40(DX), DI
+	MOVQ 48(DX), R8
+	MOVQ 56(DX), R9
+	MOVQ 64(DX), R10
+	MOVQ 72(DX), R11
+	XORQ DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+	MOVQ  $0, AX
+	ADCXQ AX, R11
+	ADOXQ AX, R11
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+	MOVQ  $0, AX
+	ADCXQ AX, R11
+	ADOXQ AX, R11
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+	MOVQ  $0, AX
+	ADCXQ AX, R11
+	ADOXQ AX, R11
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+	MOVQ  $0, AX
+	ADCXQ AX, R11
+	ADOXQ AX, R11
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+	MOVQ  $0, AX
+	ADCXQ AX, R11
+	ADOXQ AX, R11
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+	MOVQ  $0, AX
+	ADCXQ AX, R11
+	ADOXQ AX, R11
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+	MOVQ  $0, AX
+	ADCXQ AX, R11
+	ADOXQ AX, R11
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+	MOVQ  $0, AX
+	ADCXQ AX, R11
+	ADOXQ AX, R11
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+	MOVQ  $0, AX
+	ADCXQ AX, R11
+	ADOXQ AX, R11
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+	MOVQ  $0, AX
+	ADCXQ AX, R11
+	ADOXQ AX, R11
+
+	// reduce element(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11) using temp registers (R12,R13,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP))
+	REDUCE(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP))
+
+	MOVQ res+0(FP), AX
+	MOVQ R14, 0(AX)
+	MOVQ R15, 8(AX)
+	MOVQ CX, 16(AX)
+	MOVQ BX, 24(AX)
+	MOVQ SI, 32(AX)
+	MOVQ DI, 40(AX)
+	MOVQ R8, 48(AX)
+	MOVQ R9, 56(AX)
+	MOVQ R10, 64(AX)
+	MOVQ R11, 72(AX)
+	RET
+
+noAdx_3:
+	MOVQ res+0(FP), AX
+	MOVQ AX, (SP)
+	CALL ·_fromMontGeneric(SB)
+	RET
diff --git a/tmp/gnark-crypto-new/field/asm/element_10w/element_10w_arm64.h b/field/asm/element_10w/element_10w_arm64.h
new file mode 100644
index 0000000000..529fcae61d
--- /dev/null
+++ b/field/asm/element_10w/element_10w_arm64.h
@@ -0,0 +1,266 @@
+// Code generated by gnark-crypto/generator. DO NOT EDIT.
+#include "textflag.h"
+#include "funcdata.h"
+#include "go_asm.h"
+
+// mul(res, x, y *Element)
+// Algorithm 2 of Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS
+// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
+TEXT ·mul(SB), NOFRAME|NOSPLIT, $0-24
+#define DIVSHIFT() \
+	MOVD  $const_qInvNeg, R0   \
+	MUL   R12, R0, R1          \
+	MOVD  ·qElement+0(SB), R0  \
+	MUL   R0, R1, R0           \
+	ADDS  R0, R12, R12         \
+	MOVD  ·qElement+8(SB), R0  \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R13, R13         \
+	MOVD  ·qElement+16(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R14, R14         \
+	MOVD  ·qElement+24(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R15, R15         \
+	MOVD  ·qElement+32(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R16, R16         \
+	MOVD  ·qElement+40(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R17, R17         \
+	MOVD  ·qElement+48(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R19, R19         \
+	MOVD  ·qElement+56(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R20, R20         \
+	MOVD  ·qElement+64(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R21, R21         \
+	MOVD  ·qElement+72(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R22, R22         \
+	ADC   R23, ZR, R23         \
+	MOVD  ·qElement+0(SB), R0  \
+	UMULH R0, R1, R0           \
+	ADDS  R0, R13, R12         \
+	MOVD  ·qElement+8(SB), R0  \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R14, R13         \
+	MOVD  ·qElement+16(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R15, R14         \
+	MOVD  ·qElement+24(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R16, R15         \
+	MOVD  ·qElement+32(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R17, R16         \
+	MOVD  ·qElement+40(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R19, R17         \
+	MOVD  ·qElement+48(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R20, R19         \
+	MOVD  ·qElement+56(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R21, R20         \
+	MOVD  ·qElement+64(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R22, R21         \
+	MOVD  ·qElement+72(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R23, R22         \
+
+#define MUL_WORD_N() \
+	MUL   R2, R1, R0   \
+	ADDS  R0, R12, R12 \
+	MUL   R3, R1, R0   \
+	ADCS  R0, R13, R13 \
+	MUL   R4, R1, R0   \
+	ADCS  R0, R14, R14 \
+	MUL   R5, R1, R0   \
+	ADCS  R0, R15, R15 \
+	MUL   R6, R1, R0   \
+	ADCS  R0, R16, R16 \
+	MUL   R7, R1, R0   \
+	ADCS  R0, R17, R17 \
+	MUL   R8, R1, R0   \
+	ADCS  R0, R19, R19 \
+	MUL   R9, R1, R0   \
+	ADCS  R0, R20, R20 \
+	MUL   R10, R1, R0  \
+	ADCS  R0, R21, R21 \
+	MUL   R11, R1, R0  \
+	ADCS  R0, R22, R22 \
+	ADC   ZR, ZR, R23  \
+	UMULH R2, R1, R0   \
+	ADDS  R0, R13, R13 \
+	UMULH R3, R1, R0   \
+	ADCS  R0, R14, R14 \
+	UMULH R4, R1, R0   \
+	ADCS  R0, R15, R15 \
+	UMULH R5, R1, R0   \
+	ADCS  R0, R16, R16 \
+	UMULH R6, R1, R0   \
+	ADCS  R0, R17, R17 \
+	UMULH R7, R1, R0   \
+	ADCS  R0, R19, R19 \
+	UMULH R8, R1, R0   \
+	ADCS  R0, R20, R20 \
+	UMULH R9, R1, R0   \
+	ADCS  R0, R21, R21 \
+	UMULH R10, R1, R0  \
+	ADCS  R0, R22, R22 \
+	UMULH R11, R1, R0  \
+	ADC   R0, R23, R23 \
+	DIVSHIFT()         \
+
+#define MUL_WORD_0() \
+	MUL   R2, R1, R12  \
+	MUL   R3, R1, R13  \
+	MUL   R4, R1, R14  \
+	MUL   R5, R1, R15  \
+	MUL   R6, R1, R16  \
+	MUL   R7, R1, R17  \
+	MUL   R8, R1, R19  \
+	MUL   R9, R1, R20  \
+	MUL   R10, R1, R21 \
+	MUL   R11, R1, R22 \
+	UMULH R2, R1, R0   \
+	ADDS  R0, R13, R13 \
+	UMULH R3, R1, R0   \
+	ADCS  R0, R14, R14 \
+	UMULH R4, R1, R0   \
+	ADCS  R0, R15, R15 \
+	UMULH R5, R1, R0   \
+	ADCS  R0, R16, R16 \
+	UMULH R6, R1, R0   \
+	ADCS  R0, R17, R17 \
+	UMULH R7, R1, R0   \
+	ADCS  R0, R19, R19 \
+	UMULH R8, R1, R0   \
+	ADCS  R0, R20, R20 \
+	UMULH R9, R1, R0   \
+	ADCS  R0, R21, R21 \
+	UMULH R10, R1, R0  \
+	ADCS  R0, R22, R22 \
+	UMULH R11, R1, R0  \
+	ADC   R0, ZR, R23  \
+	DIVSHIFT()         \
+
+	MOVD y+16(FP), R1
+	MOVD x+8(FP), R0
+	LDP  0(R0), (R2, R3)
+	LDP  16(R0), (R4, R5)
+	LDP  32(R0), (R6, R7)
+	LDP  48(R0), (R8, R9)
+	LDP  64(R0), (R10, R11)
+	MOVD y+16(FP), R1
+	MOVD 0(R1), R1
+	MUL_WORD_0()
+	MOVD y+16(FP), R1
+	MOVD 8(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 16(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 24(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 32(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 40(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 48(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 56(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 64(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 72(R1), R1
+	MUL_WORD_N()
+	LDP  ·qElement+0(SB), (R2, R3)
+	LDP  ·qElement+16(SB), (R4, R5)
+	LDP  ·qElement+32(SB), (R6, R7)
+	LDP  ·qElement+48(SB), (R8, R9)
+	LDP  ·qElement+64(SB), (R10, R11)
+
+	// reduce if necessary
+	SUBS R2, R12, R2
+	SBCS R3, R13, R3
+	SBCS R4, R14, R4
+	SBCS R5, R15, R5
+	SBCS R6, R16, R6
+	SBCS R7, R17, R7
+	SBCS R8, R19, R8
+	SBCS R9, R20, R9
+	SBCS R10, R21, R10
+	SBCS R11, R22, R11
+	MOVD res+0(FP), R0
+	CSEL CS, R2, R12, R12
+	CSEL CS, R3, R13, R13
+	STP  (R12, R13), 0(R0)
+	CSEL CS, R4, R14, R14
+	CSEL CS, R5, R15, R15
+	STP  (R14, R15), 16(R0)
+	CSEL CS, R6, R16, R16
+	CSEL CS, R7, R17, R17
+	STP  (R16, R17), 32(R0)
+	CSEL CS, R8, R19, R19
+	CSEL CS, R9, R20, R20
+	STP  (R19, R20), 48(R0)
+	CSEL CS, R10, R21, R21
+	CSEL CS, R11, R22, R22
+	STP  (R21, R22), 64(R0)
+	RET
+
+// reduce(res *Element)
+TEXT ·reduce(SB), NOFRAME|NOSPLIT, $0-8
+	LDP  ·qElement+0(SB), (R10, R11)
+	LDP  ·qElement+16(SB), (R12, R13)
+	LDP  ·qElement+32(SB), (R14, R15)
+	LDP  ·qElement+48(SB), (R16, R17)
+	LDP  ·qElement+64(SB), (R19, R20)
+	MOVD res+0(FP), R21
+	LDP  0(R21), (R0, R1)
+	LDP  16(R21), (R2, R3)
+	LDP  32(R21), (R4, R5)
+	LDP  48(R21), (R6, R7)
+	LDP  64(R21), (R8, R9)
+
+	// q = t - q
+	SUBS R10, R0, R10
+	SBCS R11, R1, R11
+	SBCS R12, R2, R12
+	SBCS R13, R3, R13
+	SBCS R14, R4, R14
+	SBCS R15, R5, R15
+	SBCS R16, R6, R16
+	SBCS R17, R7, R17
+	SBCS R19, R8, R19
+	SBCS R20, R9, R20
+
+	// if no borrow, return q, else return t
+	CSEL CS, R10, R0, R0
+	CSEL CS, R11, R1, R1
+	STP  (R0, R1), 0(R21)
+	CSEL CS, R12, R2, R2
+	CSEL CS, R13, R3, R3
+	STP  (R2, R3), 16(R21)
+	CSEL CS, R14, R4, R4
+	CSEL CS, R15, R5, R5
+	STP  (R4, R5), 32(R21)
+	CSEL CS, R16, R6, R6
+	CSEL CS, R17, R7, R7
+	STP  (R6, R7), 48(R21)
+	CSEL CS, R19, R8, R8
+	CSEL CS, R20, R9, R9
+	STP  (R8, R9), 64(R21)
+	RET
diff --git a/field/asm/element_12w/BUILD.bazel b/field/asm/element_12w/BUILD.bazel
index d883a6a130..16a1afc05d 100644
--- a/field/asm/element_12w/BUILD.bazel
+++ b/field/asm/element_12w/BUILD.bazel
@@ -16,3 +16,9 @@ alias(
     actual = ":element_12w",
     visibility = ["//visibility:public"],
 )
+
+exports_files([
+    "element_12w_amd64.h",
+    "element_12w_arm64.h",
+])
+
diff --git a/tmp/gnark-crypto-new/field/asm/element_12w/element_12w_amd64.h b/field/asm/element_12w/element_12w_amd64.h
new file mode 100644
index 0000000000..9aa37956c0
--- /dev/null
+++ b/field/asm/element_12w/element_12w_amd64.h
@@ -0,0 +1,1557 @@
+// Code generated by gnark-crypto/generator. DO NOT EDIT.
+#include "textflag.h"
+#include "funcdata.h"
+#include "go_asm.h"
+
+#define REDUCE(ra0, ra1, ra2, ra3, ra4, ra5, ra6, ra7, ra8, ra9, ra10, ra11, rb0, rb1, rb2, rb3, rb4, rb5, rb6, rb7, rb8, rb9, rb10, rb11) \
+	MOVQ    ra0, rb0;               \
+	SUBQ    ·qElement(SB), ra0;     \
+	MOVQ    ra1, rb1;               \
+	SBBQ    ·qElement+8(SB), ra1;   \
+	MOVQ    ra2, rb2;               \
+	SBBQ    ·qElement+16(SB), ra2;  \
+	MOVQ    ra3, rb3;               \
+	SBBQ    ·qElement+24(SB), ra3;  \
+	MOVQ    ra4, rb4;               \
+	SBBQ    ·qElement+32(SB), ra4;  \
+	MOVQ    ra5, rb5;               \
+	SBBQ    ·qElement+40(SB), ra5;  \
+	MOVQ    ra6, rb6;               \
+	SBBQ    ·qElement+48(SB), ra6;  \
+	MOVQ    ra7, rb7;               \
+	SBBQ    ·qElement+56(SB), ra7;  \
+	MOVQ    ra8, rb8;               \
+	SBBQ    ·qElement+64(SB), ra8;  \
+	MOVQ    ra9, rb9;               \
+	SBBQ    ·qElement+72(SB), ra9;  \
+	MOVQ    ra10, rb10;             \
+	SBBQ    ·qElement+80(SB), ra10; \
+	MOVQ    ra11, rb11;             \
+	SBBQ    ·qElement+88(SB), ra11; \
+	CMOVQCS rb0, ra0;               \
+	CMOVQCS rb1, ra1;               \
+	CMOVQCS rb2, ra2;               \
+	CMOVQCS rb3, ra3;               \
+	CMOVQCS rb4, ra4;               \
+	CMOVQCS rb5, ra5;               \
+	CMOVQCS rb6, ra6;               \
+	CMOVQCS rb7, ra7;               \
+	CMOVQCS rb8, ra8;               \
+	CMOVQCS rb9, ra9;               \
+	CMOVQCS rb10, ra10;             \
+	CMOVQCS rb11, ra11;             \
+
+TEXT ·reduce(SB), $88-8
+	MOVQ res+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	MOVQ 40(AX), R8
+	MOVQ 48(AX), R9
+	MOVQ 56(AX), R10
+	MOVQ 64(AX), R11
+	MOVQ 72(AX), R12
+	MOVQ 80(AX), R13
+	MOVQ 88(AX), R14
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	MOVQ R8, 40(AX)
+	MOVQ R9, 48(AX)
+	MOVQ R10, 56(AX)
+	MOVQ R11, 64(AX)
+	MOVQ R12, 72(AX)
+	MOVQ R13, 80(AX)
+	MOVQ R14, 88(AX)
+	RET
+
+// MulBy3(x *Element)
+TEXT ·MulBy3(SB), $88-8
+	MOVQ x+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	MOVQ 40(AX), R8
+	MOVQ 48(AX), R9
+	MOVQ 56(AX), R10
+	MOVQ 64(AX), R11
+	MOVQ 72(AX), R12
+	MOVQ 80(AX), R13
+	MOVQ 88(AX), R14
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+	ADCQ R9, R9
+	ADCQ R10, R10
+	ADCQ R11, R11
+	ADCQ R12, R12
+	ADCQ R13, R13
+	ADCQ R14, R14
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+
+	ADDQ 0(AX), DX
+	ADCQ 8(AX), CX
+	ADCQ 16(AX), BX
+	ADCQ 24(AX), SI
+	ADCQ 32(AX), DI
+	ADCQ 40(AX), R8
+	ADCQ 48(AX), R9
+	ADCQ 56(AX), R10
+	ADCQ 64(AX), R11
+	ADCQ 72(AX), R12
+	ADCQ 80(AX), R13
+	ADCQ 88(AX), R14
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	MOVQ R8, 40(AX)
+	MOVQ R9, 48(AX)
+	MOVQ R10, 56(AX)
+	MOVQ R11, 64(AX)
+	MOVQ R12, 72(AX)
+	MOVQ R13, 80(AX)
+	MOVQ R14, 88(AX)
+	RET
+
+// MulBy5(x *Element)
+TEXT ·MulBy5(SB), $88-8
+	MOVQ x+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	MOVQ 40(AX), R8
+	MOVQ 48(AX), R9
+	MOVQ 56(AX), R10
+	MOVQ 64(AX), R11
+	MOVQ 72(AX), R12
+	MOVQ 80(AX), R13
+	MOVQ 88(AX), R14
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+	ADCQ R9, R9
+	ADCQ R10, R10
+	ADCQ R11, R11
+	ADCQ R12, R12
+	ADCQ R13, R13
+	ADCQ R14, R14
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+	ADCQ R9, R9
+	ADCQ R10, R10
+	ADCQ R11, R11
+	ADCQ R12, R12
+	ADCQ R13, R13
+	ADCQ R14, R14
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+
+	ADDQ 0(AX), DX
+	ADCQ 8(AX), CX
+	ADCQ 16(AX), BX
+	ADCQ 24(AX), SI
+	ADCQ 32(AX), DI
+	ADCQ 40(AX), R8
+	ADCQ 48(AX), R9
+	ADCQ 56(AX), R10
+	ADCQ 64(AX), R11
+	ADCQ 72(AX), R12
+	ADCQ 80(AX), R13
+	ADCQ 88(AX), R14
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	MOVQ R8, 40(AX)
+	MOVQ R9, 48(AX)
+	MOVQ R10, 56(AX)
+	MOVQ R11, 64(AX)
+	MOVQ R12, 72(AX)
+	MOVQ R13, 80(AX)
+	MOVQ R14, 88(AX)
+	RET
+
+// MulBy13(x *Element)
+TEXT ·MulBy13(SB), $184-8
+	MOVQ x+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	MOVQ 40(AX), R8
+	MOVQ 48(AX), R9
+	MOVQ 56(AX), R10
+	MOVQ 64(AX), R11
+	MOVQ 72(AX), R12
+	MOVQ 80(AX), R13
+	MOVQ 88(AX), R14
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+	ADCQ R9, R9
+	ADCQ R10, R10
+	ADCQ R11, R11
+	ADCQ R12, R12
+	ADCQ R13, R13
+	ADCQ R14, R14
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+	ADCQ R9, R9
+	ADCQ R10, R10
+	ADCQ R11, R11
+	ADCQ R12, R12
+	ADCQ R13, R13
+	ADCQ R14, R14
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (s11-96(SP),s12-104(SP),s13-112(SP),s14-120(SP),s15-128(SP),s16-136(SP),s17-144(SP),s18-152(SP),s19-160(SP),s20-168(SP),s21-176(SP),s22-184(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,s11-96(SP),s12-104(SP),s13-112(SP),s14-120(SP),s15-128(SP),s16-136(SP),s17-144(SP),s18-152(SP),s19-160(SP),s20-168(SP),s21-176(SP),s22-184(SP))
+
+	MOVQ DX, s11-96(SP)
+	MOVQ CX, s12-104(SP)
+	MOVQ BX, s13-112(SP)
+	MOVQ SI, s14-120(SP)
+	MOVQ DI, s15-128(SP)
+	MOVQ R8, s16-136(SP)
+	MOVQ R9, s17-144(SP)
+	MOVQ R10, s18-152(SP)
+	MOVQ R11, s19-160(SP)
+	MOVQ R12, s20-168(SP)
+	MOVQ R13, s21-176(SP)
+	MOVQ R14, s22-184(SP)
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+	ADCQ R9, R9
+	ADCQ R10, R10
+	ADCQ R11, R11
+	ADCQ R12, R12
+	ADCQ R13, R13
+	ADCQ R14, R14
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+
+	ADDQ s11-96(SP), DX
+	ADCQ s12-104(SP), CX
+	ADCQ s13-112(SP), BX
+	ADCQ s14-120(SP), SI
+	ADCQ s15-128(SP), DI
+	ADCQ s16-136(SP), R8
+	ADCQ s17-144(SP), R9
+	ADCQ s18-152(SP), R10
+	ADCQ s19-160(SP), R11
+	ADCQ s20-168(SP), R12
+	ADCQ s21-176(SP), R13
+	ADCQ s22-184(SP), R14
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+
+	ADDQ 0(AX), DX
+	ADCQ 8(AX), CX
+	ADCQ 16(AX), BX
+	ADCQ 24(AX), SI
+	ADCQ 32(AX), DI
+	ADCQ 40(AX), R8
+	ADCQ 48(AX), R9
+	ADCQ 56(AX), R10
+	ADCQ 64(AX), R11
+	ADCQ 72(AX), R12
+	ADCQ 80(AX), R13
+	ADCQ 88(AX), R14
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	MOVQ R8, 40(AX)
+	MOVQ R9, 48(AX)
+	MOVQ R10, 56(AX)
+	MOVQ R11, 64(AX)
+	MOVQ R12, 72(AX)
+	MOVQ R13, 80(AX)
+	MOVQ R14, 88(AX)
+	RET
+
+// Butterfly(a, b *Element) sets a = a + b; b = a - b
+TEXT ·Butterfly(SB), $88-16
+	MOVQ b+8(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	MOVQ 40(AX), R8
+	MOVQ 48(AX), R9
+	MOVQ 56(AX), R10
+	MOVQ 64(AX), R11
+	MOVQ 72(AX), R12
+	MOVQ 80(AX), R13
+	MOVQ 88(AX), R14
+	MOVQ a+0(FP), AX
+	ADDQ 0(AX), DX
+	ADCQ 8(AX), CX
+	ADCQ 16(AX), BX
+	ADCQ 24(AX), SI
+	ADCQ 32(AX), DI
+	ADCQ 40(AX), R8
+	ADCQ 48(AX), R9
+	ADCQ 56(AX), R10
+	ADCQ 64(AX), R11
+	ADCQ 72(AX), R12
+	ADCQ 80(AX), R13
+	ADCQ 88(AX), R14
+	MOVQ DX, R15
+	MOVQ CX, s0-8(SP)
+	MOVQ BX, s1-16(SP)
+	MOVQ SI, s2-24(SP)
+	MOVQ DI, s3-32(SP)
+	MOVQ R8, s4-40(SP)
+	MOVQ R9, s5-48(SP)
+	MOVQ R10, s6-56(SP)
+	MOVQ R11, s7-64(SP)
+	MOVQ R12, s8-72(SP)
+	MOVQ R13, s9-80(SP)
+	MOVQ R14, s10-88(SP)
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	MOVQ 40(AX), R8
+	MOVQ 48(AX), R9
+	MOVQ 56(AX), R10
+	MOVQ 64(AX), R11
+	MOVQ 72(AX), R12
+	MOVQ 80(AX), R13
+	MOVQ 88(AX), R14
+	MOVQ b+8(FP), AX
+	SUBQ 0(AX), DX
+	SBBQ 8(AX), CX
+	SBBQ 16(AX), BX
+	SBBQ 24(AX), SI
+	SBBQ 32(AX), DI
+	SBBQ 40(AX), R8
+	SBBQ 48(AX), R9
+	SBBQ 56(AX), R10
+	SBBQ 64(AX), R11
+	SBBQ 72(AX), R12
+	SBBQ 80(AX), R13
+	SBBQ 88(AX), R14
+	JCC  noReduce_1
+	MOVQ $const_q0, AX
+	ADDQ AX, DX
+	MOVQ $const_q1, AX
+	ADCQ AX, CX
+	MOVQ $const_q2, AX
+	ADCQ AX, BX
+	MOVQ $const_q3, AX
+	ADCQ AX, SI
+	MOVQ $const_q4, AX
+	ADCQ AX, DI
+	MOVQ $const_q5, AX
+	ADCQ AX, R8
+	MOVQ $const_q6, AX
+	ADCQ AX, R9
+	MOVQ $const_q7, AX
+	ADCQ AX, R10
+	MOVQ $const_q8, AX
+	ADCQ AX, R11
+	MOVQ $const_q9, AX
+	ADCQ AX, R12
+	MOVQ $const_q10, AX
+	ADCQ AX, R13
+	MOVQ $const_q11, AX
+	ADCQ AX, R14
+
+noReduce_1:
+	MOVQ b+8(FP), AX
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	MOVQ R8, 40(AX)
+	MOVQ R9, 48(AX)
+	MOVQ R10, 56(AX)
+	MOVQ R11, 64(AX)
+	MOVQ R12, 72(AX)
+	MOVQ R13, 80(AX)
+	MOVQ R14, 88(AX)
+	MOVQ R15, DX
+	MOVQ s0-8(SP), CX
+	MOVQ s1-16(SP), BX
+	MOVQ s2-24(SP), SI
+	MOVQ s3-32(SP), DI
+	MOVQ s4-40(SP), R8
+	MOVQ s5-48(SP), R9
+	MOVQ s6-56(SP), R10
+	MOVQ s7-64(SP), R11
+	MOVQ s8-72(SP), R12
+	MOVQ s9-80(SP), R13
+	MOVQ s10-88(SP), R14
+
+	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
+
+	MOVQ a+0(FP), AX
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	MOVQ R8, 40(AX)
+	MOVQ R9, 48(AX)
+	MOVQ R10, 56(AX)
+	MOVQ R11, 64(AX)
+	MOVQ R12, 72(AX)
+	MOVQ R13, 80(AX)
+	MOVQ R14, 88(AX)
+	RET
+
+// mul(res, x, y *Element)
+TEXT ·mul(SB), $96-24
+
+	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
+	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
+	// See github.com/Consensys/gnark-crypto/field/generator for more comments.
+
+	NO_LOCAL_POINTERS
+	CMPB ·supportAdx(SB), $1
+	JNE  noAdx_2
+	MOVQ x+8(FP), AX
+
+	// x[0] -> s0-8(SP)
+	// x[1] -> s1-16(SP)
+	// x[2] -> s2-24(SP)
+	// x[3] -> s3-32(SP)
+	// x[4] -> s4-40(SP)
+	// x[5] -> s5-48(SP)
+	// x[6] -> s6-56(SP)
+	// x[7] -> s7-64(SP)
+	// x[8] -> s8-72(SP)
+	// x[9] -> s9-80(SP)
+	// x[10] -> s10-88(SP)
+	// x[11] -> s11-96(SP)
+	MOVQ 0(AX), R14
+	MOVQ 8(AX), R15
+	MOVQ 16(AX), CX
+	MOVQ 24(AX), BX
+	MOVQ 32(AX), SI
+	MOVQ 40(AX), DI
+	MOVQ 48(AX), R8
+	MOVQ 56(AX), R9
+	MOVQ 64(AX), R10
+	MOVQ 72(AX), R11
+	MOVQ 80(AX), R12
+	MOVQ 88(AX), R13
+	MOVQ R14, s0-8(SP)
+	MOVQ R15, s1-16(SP)
+	MOVQ CX, s2-24(SP)
+	MOVQ BX, s3-32(SP)
+	MOVQ SI, s4-40(SP)
+	MOVQ DI, s5-48(SP)
+	MOVQ R8, s6-56(SP)
+	MOVQ R9, s7-64(SP)
+	MOVQ R10, s8-72(SP)
+	MOVQ R11, s9-80(SP)
+	MOVQ R12, s10-88(SP)
+	MOVQ R13, s11-96(SP)
+
+	// A -> BP
+	// t[0] -> R14
+	// t[1] -> R15
+	// t[2] -> CX
+	// t[3] -> BX
+	// t[4] -> SI
+	// t[5] -> DI
+	// t[6] -> R8
+	// t[7] -> R9
+	// t[8] -> R10
+	// t[9] -> R11
+	// t[10] -> R12
+	// t[11] -> R13
+#define MACC(in0, in1, in2) \
+	ADCXQ in0, in1     \
+	MULXQ in2, AX, in0 \
+	ADOXQ AX, in1      \
+
+#define DIV_SHIFT() \
+	PUSHQ BP                         \
+	MOVQ  $const_qInvNeg, DX         \
+	IMULQ R14, DX                    \
+	XORQ  AX, AX                     \
+	MULXQ ·qElement+0(SB), AX, BP    \
+	ADCXQ R14, AX                    \
+	MOVQ  BP, R14                    \
+	POPQ  BP                         \
+	MACC(R15, R14, ·qElement+8(SB))  \
+	MACC(CX, R15, ·qElement+16(SB))  \
+	MACC(BX, CX, ·qElement+24(SB))   \
+	MACC(SI, BX, ·qElement+32(SB))   \
+	MACC(DI, SI, ·qElement+40(SB))   \
+	MACC(R8, DI, ·qElement+48(SB))   \
+	MACC(R9, R8, ·qElement+56(SB))   \
+	MACC(R10, R9, ·qElement+64(SB))  \
+	MACC(R11, R10, ·qElement+72(SB)) \
+	MACC(R12, R11, ·qElement+80(SB)) \
+	MACC(R13, R12, ·qElement+88(SB)) \
+	MOVQ  $0, AX                     \
+	ADCXQ AX, R13                    \
+	ADOXQ BP, R13                    \
+
+#define MUL_WORD_0() \
+	XORQ  AX, AX              \
+	MULXQ s0-8(SP), R14, R15  \
+	MULXQ s1-16(SP), AX, CX   \
+	ADOXQ AX, R15             \
+	MULXQ s2-24(SP), AX, BX   \
+	ADOXQ AX, CX              \
+	MULXQ s3-32(SP), AX, SI   \
+	ADOXQ AX, BX              \
+	MULXQ s4-40(SP), AX, DI   \
+	ADOXQ AX, SI              \
+	MULXQ s5-48(SP), AX, R8   \
+	ADOXQ AX, DI              \
+	MULXQ s6-56(SP), AX, R9   \
+	ADOXQ AX, R8              \
+	MULXQ s7-64(SP), AX, R10  \
+	ADOXQ AX, R9              \
+	MULXQ s8-72(SP), AX, R11  \
+	ADOXQ AX, R10             \
+	MULXQ s9-80(SP), AX, R12  \
+	ADOXQ AX, R11             \
+	MULXQ s10-88(SP), AX, R13 \
+	ADOXQ AX, R12             \
+	MULXQ s11-96(SP), AX, BP  \
+	ADOXQ AX, R13             \
+	MOVQ  $0, AX              \
+	ADOXQ AX, BP              \
+	DIV_SHIFT()               \
+
+#define MUL_WORD_N() \
+	XORQ  AX, AX              \
+	MULXQ s0-8(SP), AX, BP    \
+	ADOXQ AX, R14             \
+	MACC(BP, R15, s1-16(SP))  \
+	MACC(BP, CX, s2-24(SP))   \
+	MACC(BP, BX, s3-32(SP))   \
+	MACC(BP, SI, s4-40(SP))   \
+	MACC(BP, DI, s5-48(SP))   \
+	MACC(BP, R8, s6-56(SP))   \
+	MACC(BP, R9, s7-64(SP))   \
+	MACC(BP, R10, s8-72(SP))  \
+	MACC(BP, R11, s9-80(SP))  \
+	MACC(BP, R12, s10-88(SP)) \
+	MACC(BP, R13, s11-96(SP)) \
+	MOVQ  $0, AX              \
+	ADCXQ AX, BP              \
+	ADOXQ AX, BP              \
+	DIV_SHIFT()               \
+
+	// mul body
+	MOVQ y+16(FP), AX
+	MOVQ 0(AX), DX
+	MUL_WORD_0()
+	MOVQ y+16(FP), AX
+	MOVQ 8(AX), DX
+	MUL_WORD_N()
+	MOVQ y+16(FP), AX
+	MOVQ 16(AX), DX
+	MUL_WORD_N()
+	MOVQ y+16(FP), AX
+	MOVQ 24(AX), DX
+	MUL_WORD_N()
+	MOVQ y+16(FP), AX
+	MOVQ 32(AX), DX
+	MUL_WORD_N()
+	MOVQ y+16(FP), AX
+	MOVQ 40(AX), DX
+	MUL_WORD_N()
+	MOVQ y+16(FP), AX
+	MOVQ 48(AX), DX
+	MUL_WORD_N()
+	MOVQ y+16(FP), AX
+	MOVQ 56(AX), DX
+	MUL_WORD_N()
+	MOVQ y+16(FP), AX
+	MOVQ 64(AX), DX
+	MUL_WORD_N()
+	MOVQ y+16(FP), AX
+	MOVQ 72(AX), DX
+	MUL_WORD_N()
+	MOVQ y+16(FP), AX
+	MOVQ 80(AX), DX
+	MUL_WORD_N()
+	MOVQ y+16(FP), AX
+	MOVQ 88(AX), DX
+	MUL_WORD_N()
+
+	// reduce element(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13) using temp registers (s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP),s11-96(SP))
+	REDUCE(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP),s11-96(SP))
+
+	MOVQ res+0(FP), AX
+	MOVQ R14, 0(AX)
+	MOVQ R15, 8(AX)
+	MOVQ CX, 16(AX)
+	MOVQ BX, 24(AX)
+	MOVQ SI, 32(AX)
+	MOVQ DI, 40(AX)
+	MOVQ R8, 48(AX)
+	MOVQ R9, 56(AX)
+	MOVQ R10, 64(AX)
+	MOVQ R11, 72(AX)
+	MOVQ R12, 80(AX)
+	MOVQ R13, 88(AX)
+	RET
+
+noAdx_2:
+	MOVQ res+0(FP), AX
+	MOVQ AX, (SP)
+	MOVQ x+8(FP), AX
+	MOVQ AX, 8(SP)
+	MOVQ y+16(FP), AX
+	MOVQ AX, 16(SP)
+	CALL ·_mulGeneric(SB)
+	RET
+
+TEXT ·fromMont(SB), $96-8
+	NO_LOCAL_POINTERS
+
+	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
+	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
+	// when y = 1 we have:
+	// for i=0 to N-1
+	// 		t[i] = x[i]
+	// for i=0 to N-1
+	// 		m := t[0]*q'[0] mod W
+	// 		C,_ := t[0] + m*q[0]
+	// 		for j=1 to N-1
+	// 		    (C,t[j-1]) := t[j] + m*q[j] + C
+	// 		t[N-1] = C
+	CMPB ·supportAdx(SB), $1
+	JNE  noAdx_3
+	MOVQ res+0(FP), DX
+	MOVQ 0(DX), R14
+	MOVQ 8(DX), R15
+	MOVQ 16(DX), CX
+	MOVQ 24(DX), BX
+	MOVQ 32(DX), SI
+	MOVQ 40(DX), DI
+	MOVQ 48(DX), R8
+	MOVQ 56(DX), R9
+	MOVQ 64(DX), R10
+	MOVQ 72(DX), R11
+	MOVQ 80(DX), R12
+	MOVQ 88(DX), R13
+	XORQ DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+
+	// (C,t[9]) := t[10] + m*q[10] + C
+	ADCXQ R12, R11
+	MULXQ ·qElement+80(SB), AX, R12
+	ADOXQ AX, R11
+
+	// (C,t[10]) := t[11] + m*q[11] + C
+	ADCXQ R13, R12
+	MULXQ ·qElement+88(SB), AX, R13
+	ADOXQ AX, R12
+	MOVQ  $0, AX
+	ADCXQ AX, R13
+	ADOXQ AX, R13
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+
+	// (C,t[9]) := t[10] + m*q[10] + C
+	ADCXQ R12, R11
+	MULXQ ·qElement+80(SB), AX, R12
+	ADOXQ AX, R11
+
+	// (C,t[10]) := t[11] + m*q[11] + C
+	ADCXQ R13, R12
+	MULXQ ·qElement+88(SB), AX, R13
+	ADOXQ AX, R12
+	MOVQ  $0, AX
+	ADCXQ AX, R13
+	ADOXQ AX, R13
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+
+	// (C,t[9]) := t[10] + m*q[10] + C
+	ADCXQ R12, R11
+	MULXQ ·qElement+80(SB), AX, R12
+	ADOXQ AX, R11
+
+	// (C,t[10]) := t[11] + m*q[11] + C
+	ADCXQ R13, R12
+	MULXQ ·qElement+88(SB), AX, R13
+	ADOXQ AX, R12
+	MOVQ  $0, AX
+	ADCXQ AX, R13
+	ADOXQ AX, R13
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+
+	// (C,t[9]) := t[10] + m*q[10] + C
+	ADCXQ R12, R11
+	MULXQ ·qElement+80(SB), AX, R12
+	ADOXQ AX, R11
+
+	// (C,t[10]) := t[11] + m*q[11] + C
+	ADCXQ R13, R12
+	MULXQ ·qElement+88(SB), AX, R13
+	ADOXQ AX, R12
+	MOVQ  $0, AX
+	ADCXQ AX, R13
+	ADOXQ AX, R13
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+
+	// (C,t[9]) := t[10] + m*q[10] + C
+	ADCXQ R12, R11
+	MULXQ ·qElement+80(SB), AX, R12
+	ADOXQ AX, R11
+
+	// (C,t[10]) := t[11] + m*q[11] + C
+	ADCXQ R13, R12
+	MULXQ ·qElement+88(SB), AX, R13
+	ADOXQ AX, R12
+	MOVQ  $0, AX
+	ADCXQ AX, R13
+	ADOXQ AX, R13
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+
+	// (C,t[9]) := t[10] + m*q[10] + C
+	ADCXQ R12, R11
+	MULXQ ·qElement+80(SB), AX, R12
+	ADOXQ AX, R11
+
+	// (C,t[10]) := t[11] + m*q[11] + C
+	ADCXQ R13, R12
+	MULXQ ·qElement+88(SB), AX, R13
+	ADOXQ AX, R12
+	MOVQ  $0, AX
+	ADCXQ AX, R13
+	ADOXQ AX, R13
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+
+	// (C,t[9]) := t[10] + m*q[10] + C
+	ADCXQ R12, R11
+	MULXQ ·qElement+80(SB), AX, R12
+	ADOXQ AX, R11
+
+	// (C,t[10]) := t[11] + m*q[11] + C
+	ADCXQ R13, R12
+	MULXQ ·qElement+88(SB), AX, R13
+	ADOXQ AX, R12
+	MOVQ  $0, AX
+	ADCXQ AX, R13
+	ADOXQ AX, R13
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+
+	// (C,t[9]) := t[10] + m*q[10] + C
+	ADCXQ R12, R11
+	MULXQ ·qElement+80(SB), AX, R12
+	ADOXQ AX, R11
+
+	// (C,t[10]) := t[11] + m*q[11] + C
+	ADCXQ R13, R12
+	MULXQ ·qElement+88(SB), AX, R13
+	ADOXQ AX, R12
+	MOVQ  $0, AX
+	ADCXQ AX, R13
+	ADOXQ AX, R13
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+
+	// (C,t[9]) := t[10] + m*q[10] + C
+	ADCXQ R12, R11
+	MULXQ ·qElement+80(SB), AX, R12
+	ADOXQ AX, R11
+
+	// (C,t[10]) := t[11] + m*q[11] + C
+	ADCXQ R13, R12
+	MULXQ ·qElement+88(SB), AX, R13
+	ADOXQ AX, R12
+	MOVQ  $0, AX
+	ADCXQ AX, R13
+	ADOXQ AX, R13
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+
+	// (C,t[9]) := t[10] + m*q[10] + C
+	ADCXQ R12, R11
+	MULXQ ·qElement+80(SB), AX, R12
+	ADOXQ AX, R11
+
+	// (C,t[10]) := t[11] + m*q[11] + C
+	ADCXQ R13, R12
+	MULXQ ·qElement+88(SB), AX, R13
+	ADOXQ AX, R12
+	MOVQ  $0, AX
+	ADCXQ AX, R13
+	ADOXQ AX, R13
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+
+	// (C,t[9]) := t[10] + m*q[10] + C
+	ADCXQ R12, R11
+	MULXQ ·qElement+80(SB), AX, R12
+	ADOXQ AX, R11
+
+	// (C,t[10]) := t[11] + m*q[11] + C
+	ADCXQ R13, R12
+	MULXQ ·qElement+88(SB), AX, R13
+	ADOXQ AX, R12
+	MOVQ  $0, AX
+	ADCXQ AX, R13
+	ADOXQ AX, R13
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+
+	// (C,t[5]) := t[6] + m*q[6] + C
+	ADCXQ R8, DI
+	MULXQ ·qElement+48(SB), AX, R8
+	ADOXQ AX, DI
+
+	// (C,t[6]) := t[7] + m*q[7] + C
+	ADCXQ R9, R8
+	MULXQ ·qElement+56(SB), AX, R9
+	ADOXQ AX, R8
+
+	// (C,t[7]) := t[8] + m*q[8] + C
+	ADCXQ R10, R9
+	MULXQ ·qElement+64(SB), AX, R10
+	ADOXQ AX, R9
+
+	// (C,t[8]) := t[9] + m*q[9] + C
+	ADCXQ R11, R10
+	MULXQ ·qElement+72(SB), AX, R11
+	ADOXQ AX, R10
+
+	// (C,t[9]) := t[10] + m*q[10] + C
+	ADCXQ R12, R11
+	MULXQ ·qElement+80(SB), AX, R12
+	ADOXQ AX, R11
+
+	// (C,t[10]) := t[11] + m*q[11] + C
+	ADCXQ R13, R12
+	MULXQ ·qElement+88(SB), AX, R13
+	ADOXQ AX, R12
+	MOVQ  $0, AX
+	ADCXQ AX, R13
+	ADOXQ AX, R13
+
+	// reduce element(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13) using temp registers (s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP),s11-96(SP))
+	REDUCE(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP),s11-96(SP))
+
+	MOVQ res+0(FP), AX
+	MOVQ R14, 0(AX)
+	MOVQ R15, 8(AX)
+	MOVQ CX, 16(AX)
+	MOVQ BX, 24(AX)
+	MOVQ SI, 32(AX)
+	MOVQ DI, 40(AX)
+	MOVQ R8, 48(AX)
+	MOVQ R9, 56(AX)
+	MOVQ R10, 64(AX)
+	MOVQ R11, 72(AX)
+	MOVQ R12, 80(AX)
+	MOVQ R13, 88(AX)
+	RET
+
+noAdx_3:
+	MOVQ res+0(FP), AX
+	MOVQ AX, (SP)
+	CALL ·_fromMontGeneric(SB)
+	RET
diff --git a/tmp/gnark-crypto-new/field/asm/element_12w/element_12w_arm64.h b/field/asm/element_12w/element_12w_arm64.h
new file mode 100644
index 0000000000..a03e790ae6
--- /dev/null
+++ b/field/asm/element_12w/element_12w_arm64.h
@@ -0,0 +1,312 @@
+// Code generated by gnark-crypto/generator. DO NOT EDIT.
+#include "textflag.h"
+#include "funcdata.h"
+#include "go_asm.h"
+
+// mul(res, x, y *Element)
+// Algorithm 2 of Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS
+// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
+TEXT ·mul(SB), NOFRAME|NOSPLIT, $0-24
+#define DIVSHIFT() \
+	MOVD  $const_qInvNeg, R0   \
+	MUL   R14, R0, R1          \
+	MOVD  ·qElement+0(SB), R0  \
+	MUL   R0, R1, R0           \
+	ADDS  R0, R14, R14         \
+	MOVD  ·qElement+8(SB), R0  \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R15, R15         \
+	MOVD  ·qElement+16(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R16, R16         \
+	MOVD  ·qElement+24(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R17, R17         \
+	MOVD  ·qElement+32(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R19, R19         \
+	MOVD  ·qElement+40(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R20, R20         \
+	MOVD  ·qElement+48(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R21, R21         \
+	MOVD  ·qElement+56(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R22, R22         \
+	MOVD  ·qElement+64(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R23, R23         \
+	MOVD  ·qElement+72(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R24, R24         \
+	MOVD  ·qElement+80(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R25, R25         \
+	MOVD  ·qElement+88(SB), R0 \
+	MUL   R0, R1, R0           \
+	ADCS  R0, R26, R26         \
+	ADC   R29, ZR, R29         \
+	MOVD  ·qElement+0(SB), R0  \
+	UMULH R0, R1, R0           \
+	ADDS  R0, R15, R14         \
+	MOVD  ·qElement+8(SB), R0  \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R16, R15         \
+	MOVD  ·qElement+16(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R17, R16         \
+	MOVD  ·qElement+24(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R19, R17         \
+	MOVD  ·qElement+32(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R20, R19         \
+	MOVD  ·qElement+40(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R21, R20         \
+	MOVD  ·qElement+48(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R22, R21         \
+	MOVD  ·qElement+56(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R23, R22         \
+	MOVD  ·qElement+64(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R24, R23         \
+	MOVD  ·qElement+72(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R25, R24         \
+	MOVD  ·qElement+80(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R26, R25         \
+	MOVD  ·qElement+88(SB), R0 \
+	UMULH R0, R1, R0           \
+	ADCS  R0, R29, R26         \
+
+#define MUL_WORD_N() \
+	MUL   R2, R1, R0   \
+	ADDS  R0, R14, R14 \
+	MUL   R3, R1, R0   \
+	ADCS  R0, R15, R15 \
+	MUL   R4, R1, R0   \
+	ADCS  R0, R16, R16 \
+	MUL   R5, R1, R0   \
+	ADCS  R0, R17, R17 \
+	MUL   R6, R1, R0   \
+	ADCS  R0, R19, R19 \
+	MUL   R7, R1, R0   \
+	ADCS  R0, R20, R20 \
+	MUL   R8, R1, R0   \
+	ADCS  R0, R21, R21 \
+	MUL   R9, R1, R0   \
+	ADCS  R0, R22, R22 \
+	MUL   R10, R1, R0  \
+	ADCS  R0, R23, R23 \
+	MUL   R11, R1, R0  \
+	ADCS  R0, R24, R24 \
+	MUL   R12, R1, R0  \
+	ADCS  R0, R25, R25 \
+	MUL   R13, R1, R0  \
+	ADCS  R0, R26, R26 \
+	ADC   ZR, ZR, R29  \
+	UMULH R2, R1, R0   \
+	ADDS  R0, R15, R15 \
+	UMULH R3, R1, R0   \
+	ADCS  R0, R16, R16 \
+	UMULH R4, R1, R0   \
+	ADCS  R0, R17, R17 \
+	UMULH R5, R1, R0   \
+	ADCS  R0, R19, R19 \
+	UMULH R6, R1, R0   \
+	ADCS  R0, R20, R20 \
+	UMULH R7, R1, R0   \
+	ADCS  R0, R21, R21 \
+	UMULH R8, R1, R0   \
+	ADCS  R0, R22, R22 \
+	UMULH R9, R1, R0   \
+	ADCS  R0, R23, R23 \
+	UMULH R10, R1, R0  \
+	ADCS  R0, R24, R24 \
+	UMULH R11, R1, R0  \
+	ADCS  R0, R25, R25 \
+	UMULH R12, R1, R0  \
+	ADCS  R0, R26, R26 \
+	UMULH R13, R1, R0  \
+	ADC   R0, R29, R29 \
+	DIVSHIFT()         \
+
+#define MUL_WORD_0() \
+	MUL   R2, R1, R14  \
+	MUL   R3, R1, R15  \
+	MUL   R4, R1, R16  \
+	MUL   R5, R1, R17  \
+	MUL   R6, R1, R19  \
+	MUL   R7, R1, R20  \
+	MUL   R8, R1, R21  \
+	MUL   R9, R1, R22  \
+	MUL   R10, R1, R23 \
+	MUL   R11, R1, R24 \
+	MUL   R12, R1, R25 \
+	MUL   R13, R1, R26 \
+	UMULH R2, R1, R0   \
+	ADDS  R0, R15, R15 \
+	UMULH R3, R1, R0   \
+	ADCS  R0, R16, R16 \
+	UMULH R4, R1, R0   \
+	ADCS  R0, R17, R17 \
+	UMULH R5, R1, R0   \
+	ADCS  R0, R19, R19 \
+	UMULH R6, R1, R0   \
+	ADCS  R0, R20, R20 \
+	UMULH R7, R1, R0   \
+	ADCS  R0, R21, R21 \
+	UMULH R8, R1, R0   \
+	ADCS  R0, R22, R22 \
+	UMULH R9, R1, R0   \
+	ADCS  R0, R23, R23 \
+	UMULH R10, R1, R0  \
+	ADCS  R0, R24, R24 \
+	UMULH R11, R1, R0  \
+	ADCS  R0, R25, R25 \
+	UMULH R12, R1, R0  \
+	ADCS  R0, R26, R26 \
+	UMULH R13, R1, R0  \
+	ADC   R0, ZR, R29  \
+	DIVSHIFT()         \
+
+	MOVD y+16(FP), R1
+	MOVD x+8(FP), R0
+	LDP  0(R0), (R2, R3)
+	LDP  16(R0), (R4, R5)
+	LDP  32(R0), (R6, R7)
+	LDP  48(R0), (R8, R9)
+	LDP  64(R0), (R10, R11)
+	LDP  80(R0), (R12, R13)
+	MOVD y+16(FP), R1
+	MOVD 0(R1), R1
+	MUL_WORD_0()
+	MOVD y+16(FP), R1
+	MOVD 8(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 16(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 24(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 32(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 40(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 48(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 56(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 64(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 72(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 80(R1), R1
+	MUL_WORD_N()
+	MOVD y+16(FP), R1
+	MOVD 88(R1), R1
+	MUL_WORD_N()
+	LDP  ·qElement+0(SB), (R2, R3)
+	LDP  ·qElement+16(SB), (R4, R5)
+	LDP  ·qElement+32(SB), (R6, R7)
+	LDP  ·qElement+48(SB), (R8, R9)
+	LDP  ·qElement+64(SB), (R10, R11)
+	LDP  ·qElement+80(SB), (R12, R13)
+
+	// reduce if necessary
+	SUBS R2, R14, R2
+	SBCS R3, R15, R3
+	SBCS R4, R16, R4
+	SBCS R5, R17, R5
+	SBCS R6, R19, R6
+	SBCS R7, R20, R7
+	SBCS R8, R21, R8
+	SBCS R9, R22, R9
+	SBCS R10, R23, R10
+	SBCS R11, R24, R11
+	SBCS R12, R25, R12
+	SBCS R13, R26, R13
+	MOVD res+0(FP), R0
+	CSEL CS, R2, R14, R14
+	CSEL CS, R3, R15, R15
+	STP  (R14, R15), 0(R0)
+	CSEL CS, R4, R16, R16
+	CSEL CS, R5, R17, R17
+	STP  (R16, R17), 16(R0)
+	CSEL CS, R6, R19, R19
+	CSEL CS, R7, R20, R20
+	STP  (R19, R20), 32(R0)
+	CSEL CS, R8, R21, R21
+	CSEL CS, R9, R22, R22
+	STP  (R21, R22), 48(R0)
+	CSEL CS, R10, R23, R23
+	CSEL CS, R11, R24, R24
+	STP  (R23, R24), 64(R0)
+	CSEL CS, R12, R25, R25
+	CSEL CS, R13, R26, R26
+	STP  (R25, R26), 80(R0)
+	RET
+
+// reduce(res *Element)
+TEXT ·reduce(SB), NOFRAME|NOSPLIT, $0-8
+	LDP  ·qElement+0(SB), (R12, R13)
+	LDP  ·qElement+16(SB), (R14, R15)
+	LDP  ·qElement+32(SB), (R16, R17)
+	LDP  ·qElement+48(SB), (R19, R20)
+	LDP  ·qElement+64(SB), (R21, R22)
+	LDP  ·qElement+80(SB), (R23, R24)
+	MOVD res+0(FP), R25
+	LDP  0(R25), (R0, R1)
+	LDP  16(R25), (R2, R3)
+	LDP  32(R25), (R4, R5)
+	LDP  48(R25), (R6, R7)
+	LDP  64(R25), (R8, R9)
+	LDP  80(R25), (R10, R11)
+
+	// q = t - q
+	SUBS R12, R0, R12
+	SBCS R13, R1, R13
+	SBCS R14, R2, R14
+	SBCS R15, R3, R15
+	SBCS R16, R4, R16
+	SBCS R17, R5, R17
+	SBCS R19, R6, R19
+	SBCS R20, R7, R20
+	SBCS R21, R8, R21
+	SBCS R22, R9, R22
+	SBCS R23, R10, R23
+	SBCS R24, R11, R24
+
+	// if no borrow, return q, else return t
+	CSEL CS, R12, R0, R0
+	CSEL CS, R13, R1, R1
+	STP  (R0, R1), 0(R25)
+	CSEL CS, R14, R2, R2
+	CSEL CS, R15, R3, R3
+	STP  (R2, R3), 16(R25)
+	CSEL CS, R16, R4, R4
+	CSEL CS, R17, R5, R5
+	STP  (R4, R5), 32(R25)
+	CSEL CS, R19, R6, R6
+	CSEL CS, R20, R7, R7
+	STP  (R6, R7), 48(R25)
+	CSEL CS, R21, R8, R8
+	CSEL CS, R22, R9, R9
+	STP  (R8, R9), 64(R25)
+	CSEL CS, R23, R10, R10
+	CSEL CS, R24, R11, R11
+	STP  (R10, R11), 80(R25)
+	RET
diff --git a/field/asm/element_4w/BUILD.bazel b/field/asm/element_4w/BUILD.bazel
index 8831623d8c..5e48f4b4f5 100644
--- a/field/asm/element_4w/BUILD.bazel
+++ b/field/asm/element_4w/BUILD.bazel
@@ -16,3 +16,9 @@ alias(
     actual = ":element_4w",
     visibility = ["//visibility:public"],
 )
+
+exports_files([
+    "element_4w_amd64.h",
+    "element_4w_arm64.h",
+])
+
diff --git a/tmp/gnark-crypto-new/field/asm/element_4w/element_4w_amd64.h b/field/asm/element_4w/element_4w_amd64.h
new file mode 100644
index 0000000000..6d7a31b6a2
--- /dev/null
+++ b/field/asm/element_4w/element_4w_amd64.h
@@ -0,0 +1,2438 @@
+// Code generated by gnark-crypto/generator. DO NOT EDIT.
+#include "textflag.h"
+#include "funcdata.h"
+#include "go_asm.h"
+
+#define REDUCE(ra0, ra1, ra2, ra3, rb0, rb1, rb2, rb3) \
+	MOVQ    ra0, rb0;              \
+	SUBQ    ·qElement(SB), ra0;    \
+	MOVQ    ra1, rb1;              \
+	SBBQ    ·qElement+8(SB), ra1;  \
+	MOVQ    ra2, rb2;              \
+	SBBQ    ·qElement+16(SB), ra2; \
+	MOVQ    ra3, rb3;              \
+	SBBQ    ·qElement+24(SB), ra3; \
+	CMOVQCS rb0, ra0;              \
+	CMOVQCS rb1, ra1;              \
+	CMOVQCS rb2, ra2;              \
+	CMOVQCS rb3, ra3;              \
+
+TEXT ·reduce(SB), NOSPLIT, $0-8
+	MOVQ res+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+
+	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10)
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	RET
+
+// MulBy3(x *Element)
+TEXT ·MulBy3(SB), NOSPLIT, $0-8
+	MOVQ x+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+
+	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10)
+
+	ADDQ 0(AX), DX
+	ADCQ 8(AX), CX
+	ADCQ 16(AX), BX
+	ADCQ 24(AX), SI
+
+	// reduce element(DX,CX,BX,SI) using temp registers (R11,R12,R13,R14)
+	REDUCE(DX,CX,BX,SI,R11,R12,R13,R14)
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	RET
+
+// MulBy5(x *Element)
+TEXT ·MulBy5(SB), NOSPLIT, $0-8
+	MOVQ x+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+
+	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10)
+
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+
+	// reduce element(DX,CX,BX,SI) using temp registers (R11,R12,R13,R14)
+	REDUCE(DX,CX,BX,SI,R11,R12,R13,R14)
+
+	ADDQ 0(AX), DX
+	ADCQ 8(AX), CX
+	ADCQ 16(AX), BX
+	ADCQ 24(AX), SI
+
+	// reduce element(DX,CX,BX,SI) using temp registers (R15,DI,R8,R9)
+	REDUCE(DX,CX,BX,SI,R15,DI,R8,R9)
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	RET
+
+// MulBy13(x *Element)
+TEXT ·MulBy13(SB), NOSPLIT, $0-8
+	MOVQ x+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+
+	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10)
+
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+
+	// reduce element(DX,CX,BX,SI) using temp registers (R11,R12,R13,R14)
+	REDUCE(DX,CX,BX,SI,R11,R12,R13,R14)
+
+	MOVQ DX, R11
+	MOVQ CX, R12
+	MOVQ BX, R13
+	MOVQ SI, R14
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+
+	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10)
+
+	ADDQ R11, DX
+	ADCQ R12, CX
+	ADCQ R13, BX
+	ADCQ R14, SI
+
+	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10)
+
+	ADDQ 0(AX), DX
+	ADCQ 8(AX), CX
+	ADCQ 16(AX), BX
+	ADCQ 24(AX), SI
+
+	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10)
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	RET
+
+// Butterfly(a, b *Element) sets a = a + b; b = a - b
+TEXT ·Butterfly(SB), NOSPLIT, $0-16
+	MOVQ    a+0(FP), AX
+	MOVQ    0(AX), CX
+	MOVQ    8(AX), BX
+	MOVQ    16(AX), SI
+	MOVQ    24(AX), DI
+	MOVQ    CX, R8
+	MOVQ    BX, R9
+	MOVQ    SI, R10
+	MOVQ    DI, R11
+	XORQ    AX, AX
+	MOVQ    b+8(FP), DX
+	ADDQ    0(DX), CX
+	ADCQ    8(DX), BX
+	ADCQ    16(DX), SI
+	ADCQ    24(DX), DI
+	SUBQ    0(DX), R8
+	SBBQ    8(DX), R9
+	SBBQ    16(DX), R10
+	SBBQ    24(DX), R11
+	MOVQ    $const_q0, R12
+	MOVQ    $const_q1, R13
+	MOVQ    $const_q2, R14
+	MOVQ    $const_q3, R15
+	CMOVQCC AX, R12
+	CMOVQCC AX, R13
+	CMOVQCC AX, R14
+	CMOVQCC AX, R15
+	ADDQ    R12, R8
+	ADCQ    R13, R9
+	ADCQ    R14, R10
+	ADCQ    R15, R11
+	MOVQ    R8, 0(DX)
+	MOVQ    R9, 8(DX)
+	MOVQ    R10, 16(DX)
+	MOVQ    R11, 24(DX)
+
+	// reduce element(CX,BX,SI,DI) using temp registers (R8,R9,R10,R11)
+	REDUCE(CX,BX,SI,DI,R8,R9,R10,R11)
+
+	MOVQ a+0(FP), AX
+	MOVQ CX, 0(AX)
+	MOVQ BX, 8(AX)
+	MOVQ SI, 16(AX)
+	MOVQ DI, 24(AX)
+	RET
+
+// mul(res, x, y *Element)
+TEXT ·mul(SB), $24-24
+
+	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
+	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
+	// See github.com/Consensys/gnark-crypto/field/generator for more comments.
+
+	NO_LOCAL_POINTERS
+	CMPB ·supportAdx(SB), $1
+	JNE  noAdx_1
+	MOVQ x+8(FP), SI
+
+	// x[0] -> DI
+	// x[1] -> R8
+	// x[2] -> R9
+	// x[3] -> R10
+	MOVQ 0(SI), DI
+	MOVQ 8(SI), R8
+	MOVQ 16(SI), R9
+	MOVQ 24(SI), R10
+	MOVQ y+16(FP), R11
+
+	// A -> BP
+	// t[0] -> R14
+	// t[1] -> R13
+	// t[2] -> CX
+	// t[3] -> BX
+#define MACC(in0, in1, in2) \
+	ADCXQ in0, in1     \
+	MULXQ in2, AX, in0 \
+	ADOXQ AX, in1      \
+
+#define DIV_SHIFT() \
+	MOVQ  $const_qInvNeg, DX        \
+	IMULQ R14, DX                   \
+	XORQ  AX, AX                    \
+	MULXQ ·qElement+0(SB), AX, R12  \
+	ADCXQ R14, AX                   \
+	MOVQ  R12, R14                  \
+	MACC(R13, R14, ·qElement+8(SB)) \
+	MACC(CX, R13, ·qElement+16(SB)) \
+	MACC(BX, CX, ·qElement+24(SB))  \
+	MOVQ  $0, AX                    \
+	ADCXQ AX, BX                    \
+	ADOXQ BP, BX                    \
+
+#define MUL_WORD_0() \
+	XORQ  AX, AX       \
+	MULXQ DI, R14, R13 \
+	MULXQ R8, AX, CX   \
+	ADOXQ AX, R13      \
+	MULXQ R9, AX, BX   \
+	ADOXQ AX, CX       \
+	MULXQ R10, AX, BP  \
+	ADOXQ AX, BX       \
+	MOVQ  $0, AX       \
+	ADOXQ AX, BP       \
+	DIV_SHIFT()        \
+
+#define MUL_WORD_N() \
+	XORQ  AX, AX      \
+	MULXQ DI, AX, BP  \
+	ADOXQ AX, R14     \
+	MACC(BP, R13, R8) \
+	MACC(BP, CX, R9)  \
+	MACC(BP, BX, R10) \
+	MOVQ  $0, AX      \
+	ADCXQ AX, BP      \
+	ADOXQ AX, BP      \
+	DIV_SHIFT()       \
+
+	// mul body
+	MOVQ 0(R11), DX
+	MUL_WORD_0()
+	MOVQ 8(R11), DX
+	MUL_WORD_N()
+	MOVQ 16(R11), DX
+	MUL_WORD_N()
+	MOVQ 24(R11), DX
+	MUL_WORD_N()
+
+	// reduce element(R14,R13,CX,BX) using temp registers (SI,R12,R11,DI)
+	REDUCE(R14,R13,CX,BX,SI,R12,R11,DI)
+
+	MOVQ res+0(FP), AX
+	MOVQ R14, 0(AX)
+	MOVQ R13, 8(AX)
+	MOVQ CX, 16(AX)
+	MOVQ BX, 24(AX)
+	RET
+
+noAdx_1:
+	MOVQ res+0(FP), AX
+	MOVQ AX, (SP)
+	MOVQ x+8(FP), AX
+	MOVQ AX, 8(SP)
+	MOVQ y+16(FP), AX
+	MOVQ AX, 16(SP)
+	CALL ·_mulGeneric(SB)
+	RET
+
+TEXT ·fromMont(SB), $8-8
+	NO_LOCAL_POINTERS
+
+	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
+	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
+	// when y = 1 we have:
+	// for i=0 to N-1
+	// 		t[i] = x[i]
+	// for i=0 to N-1
+	// 		m := t[0]*q'[0] mod W
+	// 		C,_ := t[0] + m*q[0]
+	// 		for j=1 to N-1
+	// 		    (C,t[j-1]) := t[j] + m*q[j] + C
+	// 		t[N-1] = C
+	CMPB ·supportAdx(SB), $1
+	JNE  noAdx_2
+	MOVQ res+0(FP), DX
+	MOVQ 0(DX), R14
+	MOVQ 8(DX), R13
+	MOVQ 16(DX), CX
+	MOVQ 24(DX), BX
+	XORQ DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R13, R14
+	MULXQ ·qElement+8(SB), AX, R13
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R13
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R13
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+	MOVQ  $0, AX
+	ADCXQ AX, BX
+	ADOXQ AX, BX
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R13, R14
+	MULXQ ·qElement+8(SB), AX, R13
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R13
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R13
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+	MOVQ  $0, AX
+	ADCXQ AX, BX
+	ADOXQ AX, BX
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R13, R14
+	MULXQ ·qElement+8(SB), AX, R13
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R13
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R13
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+	MOVQ  $0, AX
+	ADCXQ AX, BX
+	ADOXQ AX, BX
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R13, R14
+	MULXQ ·qElement+8(SB), AX, R13
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R13
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R13
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+	MOVQ  $0, AX
+	ADCXQ AX, BX
+	ADOXQ AX, BX
+
+	// reduce element(R14,R13,CX,BX) using temp registers (SI,DI,R8,R9)
+	REDUCE(R14,R13,CX,BX,SI,DI,R8,R9)
+
+	MOVQ res+0(FP), AX
+	MOVQ R14, 0(AX)
+	MOVQ R13, 8(AX)
+	MOVQ CX, 16(AX)
+	MOVQ BX, 24(AX)
+	RET
+
+noAdx_2:
+	MOVQ res+0(FP), AX
+	MOVQ AX, (SP)
+	CALL ·_fromMontGeneric(SB)
+	RET
+
+// Vector operations are partially derived from Dag Arne Osvik's work in github.com/a16z/vectorized-fields
+
+// addVec(res, a, b *Element, n uint64) res[0...n] = a[0...n] + b[0...n]
+TEXT ·addVec(SB), NOSPLIT, $0-32
+	MOVQ res+0(FP), CX
+	MOVQ a+8(FP), AX
+	MOVQ b+16(FP), DX
+	MOVQ n+24(FP), BX
+
+loop_3:
+	TESTQ BX, BX
+	JEQ   done_4 // n == 0, we are done
+
+	// a[0] -> SI
+	// a[1] -> DI
+	// a[2] -> R8
+	// a[3] -> R9
+	MOVQ       0(AX), SI
+	MOVQ       8(AX), DI
+	MOVQ       16(AX), R8
+	MOVQ       24(AX), R9
+	ADDQ       0(DX), SI
+	ADCQ       8(DX), DI
+	ADCQ       16(DX), R8
+	ADCQ       24(DX), R9
+	PREFETCHT0 2048(AX)
+	PREFETCHT0 2048(DX)
+
+	// reduce element(SI,DI,R8,R9) using temp registers (R10,R11,R12,R13)
+	REDUCE(SI,DI,R8,R9,R10,R11,R12,R13)
+
+	MOVQ SI, 0(CX)
+	MOVQ DI, 8(CX)
+	MOVQ R8, 16(CX)
+	MOVQ R9, 24(CX)
+
+	// increment pointers to visit next element
+	ADDQ $32, AX
+	ADDQ $32, DX
+	ADDQ $32, CX
+	DECQ BX      // decrement n
+	JMP  loop_3
+
+done_4:
+	RET
+
+// subVec(res, a, b *Element, n uint64) res[0...n] = a[0...n] - b[0...n]
+TEXT ·subVec(SB), NOSPLIT, $0-32
+	MOVQ res+0(FP), CX
+	MOVQ a+8(FP), AX
+	MOVQ b+16(FP), DX
+	MOVQ n+24(FP), BX
+	XORQ SI, SI
+
+loop_5:
+	TESTQ BX, BX
+	JEQ   done_6 // n == 0, we are done
+
+	// a[0] -> DI
+	// a[1] -> R8
+	// a[2] -> R9
+	// a[3] -> R10
+	MOVQ       0(AX), DI
+	MOVQ       8(AX), R8
+	MOVQ       16(AX), R9
+	MOVQ       24(AX), R10
+	SUBQ       0(DX), DI
+	SBBQ       8(DX), R8
+	SBBQ       16(DX), R9
+	SBBQ       24(DX), R10
+	PREFETCHT0 2048(AX)
+	PREFETCHT0 2048(DX)
+
+	// reduce (a-b) mod q
+	// q[0] -> R11
+	// q[1] -> R12
+	// q[2] -> R13
+	// q[3] -> R14
+	MOVQ    $const_q0, R11
+	MOVQ    $const_q1, R12
+	MOVQ    $const_q2, R13
+	MOVQ    $const_q3, R14
+	CMOVQCC SI, R11
+	CMOVQCC SI, R12
+	CMOVQCC SI, R13
+	CMOVQCC SI, R14
+
+	// add registers (q or 0) to a, and set to result
+	ADDQ R11, DI
+	ADCQ R12, R8
+	ADCQ R13, R9
+	ADCQ R14, R10
+	MOVQ DI, 0(CX)
+	MOVQ R8, 8(CX)
+	MOVQ R9, 16(CX)
+	MOVQ R10, 24(CX)
+
+	// increment pointers to visit next element
+	ADDQ $32, AX
+	ADDQ $32, DX
+	ADDQ $32, CX
+	DECQ BX      // decrement n
+	JMP  loop_5
+
+done_6:
+	RET
+
+// sumVec(res, a *Element, n uint64) res = sum(a[0...n])
+TEXT ·sumVec(SB), NOSPLIT, $0-24
+
+	// Derived from https://github.com/a16z/vectorized-fields
+	// The idea is to use Z registers to accumulate the sum of elements, 8 by 8
+	// first, we handle the case where n % 8 != 0
+	// then, we loop over the elements 8 by 8 and accumulate the sum in the Z registers
+	// finally, we reduce the sum and store it in res
+	//
+	// when we move an element of a into a Z register, we use VPMOVZXDQ
+	// let's note w0...w3 the 4 64bits words of ai: w0 = ai[0], w1 = ai[1], w2 = ai[2], w3 = ai[3]
+	// VPMOVZXDQ(ai, Z0) will result in
+	// Z0= [hi(w3), lo(w3), hi(w2), lo(w2), hi(w1), lo(w1), hi(w0), lo(w0)]
+	// with hi(wi) the high 32 bits of wi and lo(wi) the low 32 bits of wi
+	// we can safely add 2^32+1 times Z registers constructed this way without overflow
+	// since each of this lo/hi bits are moved into a "64bits" slot
+	// N = 2^64-1 / 2^32-1 = 2^32+1
+	//
+	// we then propagate the carry using ADOXQ and ADCXQ
+	// r0 = w0l + lo(woh)
+	// r1 = carry + hi(woh) + w1l + lo(w1h)
+	// r2 = carry + hi(w1h) + w2l + lo(w2h)
+	// r3 = carry + hi(w2h) + w3l + lo(w3h)
+	// r4 = carry + hi(w3h)
+	// we then reduce the sum using a single-word Barrett reduction
+	// we pick mu = 2^288 / q; which correspond to 4.5 words max.
+	// meaning we must guarantee that r4 fits in 32bits.
+	// To do so, we reduce N to 2^32-1 (since r4 receives 2 carries max)
+
+	MOVQ a+8(FP), R14
+	MOVQ n+16(FP), R15
+
+	// initialize accumulators Z0, Z1, Z2, Z3, Z4, Z5, Z6, Z7
+	VXORPS    Z0, Z0, Z0
+	VMOVDQA64 Z0, Z1
+	VMOVDQA64 Z0, Z2
+	VMOVDQA64 Z0, Z3
+	VMOVDQA64 Z0, Z4
+	VMOVDQA64 Z0, Z5
+	VMOVDQA64 Z0, Z6
+	VMOVDQA64 Z0, Z7
+
+	// n % 8 -> CX
+	// n / 8 -> R15
+	MOVQ R15, CX
+	ANDQ $7, CX
+	SHRQ $3, R15
+
+loop_single_9:
+	TESTQ     CX, CX
+	JEQ       loop8by8_7    // n % 8 == 0, we are going to loop over 8 by 8
+	VPMOVZXDQ 0(R14), Z8
+	VPADDQ    Z8, Z0, Z0
+	ADDQ      $32, R14
+	DECQ      CX            // decrement nMod8
+	JMP       loop_single_9
+
+loop8by8_7:
+	TESTQ      R15, R15
+	JEQ        accumulate_10  // n == 0, we are going to accumulate
+	VPMOVZXDQ  0*32(R14), Z8
+	VPMOVZXDQ  1*32(R14), Z9
+	VPMOVZXDQ  2*32(R14), Z10
+	VPMOVZXDQ  3*32(R14), Z11
+	VPMOVZXDQ  4*32(R14), Z12
+	VPMOVZXDQ  5*32(R14), Z13
+	VPMOVZXDQ  6*32(R14), Z14
+	VPMOVZXDQ  7*32(R14), Z15
+	PREFETCHT0 4096(R14)
+	VPADDQ     Z8, Z0, Z0
+	VPADDQ     Z9, Z1, Z1
+	VPADDQ     Z10, Z2, Z2
+	VPADDQ     Z11, Z3, Z3
+	VPADDQ     Z12, Z4, Z4
+	VPADDQ     Z13, Z5, Z5
+	VPADDQ     Z14, Z6, Z6
+	VPADDQ     Z15, Z7, Z7
+
+	// increment pointers to visit next 8 elements
+	ADDQ $256, R14
+	DECQ R15        // decrement n
+	JMP  loop8by8_7
+
+accumulate_10:
+	// accumulate the 8 Z registers into Z0
+	VPADDQ Z7, Z6, Z6
+	VPADDQ Z6, Z5, Z5
+	VPADDQ Z5, Z4, Z4
+	VPADDQ Z4, Z3, Z3
+	VPADDQ Z3, Z2, Z2
+	VPADDQ Z2, Z1, Z1
+	VPADDQ Z1, Z0, Z0
+
+	// carry propagation
+	// lo(w0) -> BX
+	// hi(w0) -> SI
+	// lo(w1) -> DI
+	// hi(w1) -> R8
+	// lo(w2) -> R9
+	// hi(w2) -> R10
+	// lo(w3) -> R11
+	// hi(w3) -> R12
+	VMOVQ   X0, BX
+	VALIGNQ $1, Z0, Z0, Z0
+	VMOVQ   X0, SI
+	VALIGNQ $1, Z0, Z0, Z0
+	VMOVQ   X0, DI
+	VALIGNQ $1, Z0, Z0, Z0
+	VMOVQ   X0, R8
+	VALIGNQ $1, Z0, Z0, Z0
+	VMOVQ   X0, R9
+	VALIGNQ $1, Z0, Z0, Z0
+	VMOVQ   X0, R10
+	VALIGNQ $1, Z0, Z0, Z0
+	VMOVQ   X0, R11
+	VALIGNQ $1, Z0, Z0, Z0
+	VMOVQ   X0, R12
+
+	// lo(hi(wo)) -> R13
+	// lo(hi(w1)) -> CX
+	// lo(hi(w2)) -> R15
+	// lo(hi(w3)) -> R14
+#define SPLIT_LO_HI(in0, in1) \
+	MOVQ in1, in0         \
+	ANDQ $0xffffffff, in0 \
+	SHLQ $32, in0         \
+	SHRQ $32, in1         \
+
+	SPLIT_LO_HI(R13, SI)
+	SPLIT_LO_HI(CX, R8)
+	SPLIT_LO_HI(R15, R10)
+	SPLIT_LO_HI(R14, R12)
+
+	// r0 = w0l + lo(woh)
+	// r1 = carry + hi(woh) + w1l + lo(w1h)
+	// r2 = carry + hi(w1h) + w2l + lo(w2h)
+	// r3 = carry + hi(w2h) + w3l + lo(w3h)
+	// r4 = carry + hi(w3h)
+
+	XORQ  AX, AX   // clear the flags
+	ADOXQ R13, BX
+	ADOXQ CX, DI
+	ADCXQ SI, DI
+	ADOXQ R15, R9
+	ADCXQ R8, R9
+	ADOXQ R14, R11
+	ADCXQ R10, R11
+	ADOXQ AX, R12
+	ADCXQ AX, R12
+
+	// r[0] -> BX
+	// r[1] -> DI
+	// r[2] -> R9
+	// r[3] -> R11
+	// r[4] -> R12
+	// reduce using single-word Barrett
+	// see see Handbook of Applied Cryptography, Algorithm 14.42.
+	// mu=2^288 / q -> SI
+	MOVQ  $const_mu, SI
+	MOVQ  R11, AX
+	SHRQ  $32, R12, AX
+	MULQ  SI                       // high bits of res stored in DX
+	MULXQ ·qElement+0(SB), AX, SI
+	SUBQ  AX, BX
+	SBBQ  SI, DI
+	MULXQ ·qElement+16(SB), AX, SI
+	SBBQ  AX, R9
+	SBBQ  SI, R11
+	SBBQ  $0, R12
+	MULXQ ·qElement+8(SB), AX, SI
+	SUBQ  AX, DI
+	SBBQ  SI, R9
+	MULXQ ·qElement+24(SB), AX, SI
+	SBBQ  AX, R11
+	SBBQ  SI, R12
+	MOVQ  BX, R8
+	MOVQ  DI, R10
+	MOVQ  R9, R13
+	MOVQ  R11, CX
+	SUBQ  ·qElement+0(SB), BX
+	SBBQ  ·qElement+8(SB), DI
+	SBBQ  ·qElement+16(SB), R9
+	SBBQ  ·qElement+24(SB), R11
+	SBBQ  $0, R12
+	JCS   modReduced_11
+	MOVQ  BX, R8
+	MOVQ  DI, R10
+	MOVQ  R9, R13
+	MOVQ  R11, CX
+	SUBQ  ·qElement+0(SB), BX
+	SBBQ  ·qElement+8(SB), DI
+	SBBQ  ·qElement+16(SB), R9
+	SBBQ  ·qElement+24(SB), R11
+	SBBQ  $0, R12
+	JCS   modReduced_11
+	MOVQ  BX, R8
+	MOVQ  DI, R10
+	MOVQ  R9, R13
+	MOVQ  R11, CX
+
+modReduced_11:
+	MOVQ res+0(FP), SI
+	MOVQ R8, 0(SI)
+	MOVQ R10, 8(SI)
+	MOVQ R13, 16(SI)
+	MOVQ CX, 24(SI)
+
+done_8:
+	RET
+
+// innerProdVec(res, a,b *Element, n uint64) res = sum(a[0...n] * b[0...n])
+TEXT ·innerProdVec(SB), NOSPLIT, $0-32
+	MOVQ a+8(FP), R14
+	MOVQ b+16(FP), R15
+	MOVQ n+24(FP), CX
+
+	// Create mask for low dword in each qword
+	VPCMPEQB  Y0, Y0, Y0
+	VPMOVZXDQ Y0, Z5
+	VPXORQ    Z16, Z16, Z16
+	VMOVDQA64 Z16, Z17
+	VMOVDQA64 Z16, Z18
+	VMOVDQA64 Z16, Z19
+	VMOVDQA64 Z16, Z20
+	VMOVDQA64 Z16, Z21
+	VMOVDQA64 Z16, Z22
+	VMOVDQA64 Z16, Z23
+	VMOVDQA64 Z16, Z24
+	VMOVDQA64 Z16, Z25
+	VMOVDQA64 Z16, Z26
+	VMOVDQA64 Z16, Z27
+	VMOVDQA64 Z16, Z28
+	VMOVDQA64 Z16, Z29
+	VMOVDQA64 Z16, Z30
+	VMOVDQA64 Z16, Z31
+	TESTQ     CX, CX
+	JEQ       done_13       // n == 0, we are done
+
+loop_12:
+	TESTQ     CX, CX
+	JEQ       accumulate_14 // n == 0 we can accumulate
+	VPMOVZXDQ (R15), Z4
+	ADDQ      $32, R15
+
+	// we multiply and accumulate partial products of 4 bytes * 32 bytes
+#define MAC(in0, in1, in2) \
+	VPMULUDQ.BCST in0, Z4, Z2  \
+	VPSRLQ        $32, Z2, Z3  \
+	VPANDQ        Z5, Z2, Z2   \
+	VPADDQ        Z2, in1, in1 \
+	VPADDQ        Z3, in2, in2 \
+
+	MAC(0*4(R14), Z16, Z24)
+	MAC(1*4(R14), Z17, Z25)
+	MAC(2*4(R14), Z18, Z26)
+	MAC(3*4(R14), Z19, Z27)
+	MAC(4*4(R14), Z20, Z28)
+	MAC(5*4(R14), Z21, Z29)
+	MAC(6*4(R14), Z22, Z30)
+	MAC(7*4(R14), Z23, Z31)
+	ADDQ $32, R14
+	DECQ CX       // decrement n
+	JMP  loop_12
+
+accumulate_14:
+	// we accumulate the partial products into 544bits in Z1:Z0
+	MOVQ  $0x0000000000001555, AX
+	KMOVD AX, K1
+	MOVQ  $1, AX
+	KMOVD AX, K2
+
+	// store the least significant 32 bits of ACC (starts with A0L) in Z0
+	VALIGND.Z $16, Z16, Z16, K2, Z0
+	KSHIFTLW  $1, K2, K2
+	VPSRLQ    $32, Z16, Z2
+	VALIGND.Z $2, Z16, Z16, K1, Z16
+	VPADDQ    Z2, Z16, Z16
+	VPANDQ    Z5, Z24, Z2
+	VPADDQ    Z2, Z16, Z16
+	VPANDQ    Z5, Z17, Z2
+	VPADDQ    Z2, Z16, Z16
+	VALIGND   $15, Z16, Z16, K2, Z0
+	KSHIFTLW  $1, K2, K2
+
+	// macro to add partial products and store the result in Z0
+#define ADDPP(in0, in1, in2, in3, in4) \
+	VPSRLQ    $32, Z16, Z2              \
+	VALIGND.Z $2, Z16, Z16, K1, Z16     \
+	VPADDQ    Z2, Z16, Z16              \
+	VPSRLQ    $32, in0, in0             \
+	VPADDQ    in0, Z16, Z16             \
+	VPSRLQ    $32, in1, in1             \
+	VPADDQ    in1, Z16, Z16             \
+	VPANDQ    Z5, in2, Z2               \
+	VPADDQ    Z2, Z16, Z16              \
+	VPANDQ    Z5, in3, Z2               \
+	VPADDQ    Z2, Z16, Z16              \
+	VALIGND   $16-in4, Z16, Z16, K2, Z0 \
+	KADDW     K2, K2, K2                \
+
+	ADDPP(Z24, Z17, Z25, Z18, 2)
+	ADDPP(Z25, Z18, Z26, Z19, 3)
+	ADDPP(Z26, Z19, Z27, Z20, 4)
+	ADDPP(Z27, Z20, Z28, Z21, 5)
+	ADDPP(Z28, Z21, Z29, Z22, 6)
+	ADDPP(Z29, Z22, Z30, Z23, 7)
+	VPSRLQ    $32, Z16, Z2
+	VALIGND.Z $2, Z16, Z16, K1, Z16
+	VPADDQ    Z2, Z16, Z16
+	VPSRLQ    $32, Z30, Z30
+	VPADDQ    Z30, Z16, Z16
+	VPSRLQ    $32, Z23, Z23
+	VPADDQ    Z23, Z16, Z16
+	VPANDQ    Z5, Z31, Z2
+	VPADDQ    Z2, Z16, Z16
+	VALIGND   $16-8, Z16, Z16, K2, Z0
+	KSHIFTLW  $1, K2, K2
+	VPSRLQ    $32, Z16, Z2
+	VALIGND.Z $2, Z16, Z16, K1, Z16
+	VPADDQ    Z2, Z16, Z16
+	VPSRLQ    $32, Z31, Z31
+	VPADDQ    Z31, Z16, Z16
+	VALIGND   $16-9, Z16, Z16, K2, Z0
+	KSHIFTLW  $1, K2, K2
+
+#define ADDPP2(in0) \
+	VPSRLQ    $32, Z16, Z2              \
+	VALIGND.Z $2, Z16, Z16, K1, Z16     \
+	VPADDQ    Z2, Z16, Z16              \
+	VALIGND   $16-in0, Z16, Z16, K2, Z0 \
+	KSHIFTLW  $1, K2, K2                \
+
+	ADDPP2(10)
+	ADDPP2(11)
+	ADDPP2(12)
+	ADDPP2(13)
+	ADDPP2(14)
+	ADDPP2(15)
+	VPSRLQ      $32, Z16, Z2
+	VALIGND.Z   $2, Z16, Z16, K1, Z16
+	VPADDQ      Z2, Z16, Z16
+	VMOVDQA64.Z Z16, K1, Z1
+
+	// Extract the 4 least significant qwords of Z0
+	VMOVQ   X0, SI
+	VALIGNQ $1, Z0, Z1, Z0
+	VMOVQ   X0, DI
+	VALIGNQ $1, Z0, Z0, Z0
+	VMOVQ   X0, R8
+	VALIGNQ $1, Z0, Z0, Z0
+	VMOVQ   X0, R9
+	VALIGNQ $1, Z0, Z0, Z0
+	XORQ    BX, BX
+	MOVQ    $const_qInvNeg, DX
+	MULXQ   SI, DX, R10
+	MULXQ   ·qElement+0(SB), AX, R10
+	ADDQ    AX, SI
+	ADCQ    R10, DI
+	MULXQ   ·qElement+16(SB), AX, R10
+	ADCQ    AX, R8
+	ADCQ    R10, R9
+	ADCQ    $0, BX
+	MULXQ   ·qElement+8(SB), AX, R10
+	ADDQ    AX, DI
+	ADCQ    R10, R8
+	MULXQ   ·qElement+24(SB), AX, R10
+	ADCQ    AX, R9
+	ADCQ    R10, BX
+	ADCQ    $0, SI
+	MOVQ    $const_qInvNeg, DX
+	MULXQ   DI, DX, R10
+	MULXQ   ·qElement+0(SB), AX, R10
+	ADDQ    AX, DI
+	ADCQ    R10, R8
+	MULXQ   ·qElement+16(SB), AX, R10
+	ADCQ    AX, R9
+	ADCQ    R10, BX
+	ADCQ    $0, SI
+	MULXQ   ·qElement+8(SB), AX, R10
+	ADDQ    AX, R8
+	ADCQ    R10, R9
+	MULXQ   ·qElement+24(SB), AX, R10
+	ADCQ    AX, BX
+	ADCQ    R10, SI
+	ADCQ    $0, DI
+	MOVQ    $const_qInvNeg, DX
+	MULXQ   R8, DX, R10
+	MULXQ   ·qElement+0(SB), AX, R10
+	ADDQ    AX, R8
+	ADCQ    R10, R9
+	MULXQ   ·qElement+16(SB), AX, R10
+	ADCQ    AX, BX
+	ADCQ    R10, SI
+	ADCQ    $0, DI
+	MULXQ   ·qElement+8(SB), AX, R10
+	ADDQ    AX, R9
+	ADCQ    R10, BX
+	MULXQ   ·qElement+24(SB), AX, R10
+	ADCQ    AX, SI
+	ADCQ    R10, DI
+	ADCQ    $0, R8
+	MOVQ    $const_qInvNeg, DX
+	MULXQ   R9, DX, R10
+	MULXQ   ·qElement+0(SB), AX, R10
+	ADDQ    AX, R9
+	ADCQ    R10, BX
+	MULXQ   ·qElement+16(SB), AX, R10
+	ADCQ    AX, SI
+	ADCQ    R10, DI
+	ADCQ    $0, R8
+	MULXQ   ·qElement+8(SB), AX, R10
+	ADDQ    AX, BX
+	ADCQ    R10, SI
+	MULXQ   ·qElement+24(SB), AX, R10
+	ADCQ    AX, DI
+	ADCQ    R10, R8
+	ADCQ    $0, R9
+	VMOVQ   X0, AX
+	ADDQ    AX, BX
+	VALIGNQ $1, Z0, Z0, Z0
+	VMOVQ   X0, AX
+	ADCQ    AX, SI
+	VALIGNQ $1, Z0, Z0, Z0
+	VMOVQ   X0, AX
+	ADCQ    AX, DI
+	VALIGNQ $1, Z0, Z0, Z0
+	VMOVQ   X0, AX
+	ADCQ    AX, R8
+	VALIGNQ $1, Z0, Z0, Z0
+	VMOVQ   X0, AX
+	ADCQ    AX, R9
+
+	// Barrett reduction; see Handbook of Applied Cryptography, Algorithm 14.42.
+	MOVQ  R8, AX
+	SHRQ  $32, R9, AX
+	MOVQ  $const_mu, DX
+	MULQ  DX
+	MULXQ ·qElement+0(SB), AX, R10
+	SUBQ  AX, BX
+	SBBQ  R10, SI
+	MULXQ ·qElement+16(SB), AX, R10
+	SBBQ  AX, DI
+	SBBQ  R10, R8
+	SBBQ  $0, R9
+	MULXQ ·qElement+8(SB), AX, R10
+	SUBQ  AX, SI
+	SBBQ  R10, DI
+	MULXQ ·qElement+24(SB), AX, R10
+	SBBQ  AX, R8
+	SBBQ  R10, R9
+
+	// we need up to 2 conditional substractions to be < q
+	MOVQ res+0(FP), R11
+	MOVQ BX, 0(R11)
+	MOVQ SI, 8(R11)
+	MOVQ DI, 16(R11)
+	MOVQ R8, 24(R11)
+	SUBQ ·qElement+0(SB), BX
+	SBBQ ·qElement+8(SB), SI
+	SBBQ ·qElement+16(SB), DI
+	SBBQ ·qElement+24(SB), R8
+	SBBQ $0, R9
+	JCS  done_13
+	MOVQ BX, 0(R11)
+	MOVQ SI, 8(R11)
+	MOVQ DI, 16(R11)
+	MOVQ R8, 24(R11)
+	SUBQ ·qElement+0(SB), BX
+	SBBQ ·qElement+8(SB), SI
+	SBBQ ·qElement+16(SB), DI
+	SBBQ ·qElement+24(SB), R8
+	SBBQ $0, R9
+	JCS  done_13
+	MOVQ BX, 0(R11)
+	MOVQ SI, 8(R11)
+	MOVQ DI, 16(R11)
+	MOVQ R8, 24(R11)
+
+done_13:
+	RET
+
+TEXT ·scalarMulVec(SB), $8-40
+#define AVX_MUL_Q_LO() \
+	VPMULUDQ.BCST ·qElement+0(SB), Z9, Z10  \
+	VPADDQ        Z10, Z0, Z0               \
+	VPMULUDQ.BCST ·qElement+4(SB), Z9, Z11  \
+	VPADDQ        Z11, Z1, Z1               \
+	VPMULUDQ.BCST ·qElement+8(SB), Z9, Z12  \
+	VPADDQ        Z12, Z2, Z2               \
+	VPMULUDQ.BCST ·qElement+12(SB), Z9, Z13 \
+	VPADDQ        Z13, Z3, Z3               \
+
+#define AVX_MUL_Q_HI() \
+	VPMULUDQ.BCST ·qElement+16(SB), Z9, Z14 \
+	VPADDQ        Z14, Z4, Z4               \
+	VPMULUDQ.BCST ·qElement+20(SB), Z9, Z15 \
+	VPADDQ        Z15, Z5, Z5               \
+	VPMULUDQ.BCST ·qElement+24(SB), Z9, Z16 \
+	VPADDQ        Z16, Z6, Z6               \
+	VPMULUDQ.BCST ·qElement+28(SB), Z9, Z17 \
+	VPADDQ        Z17, Z7, Z7               \
+
+#define SHIFT_ADD_AND(in0, in1, in2, in3) \
+	VPSRLQ $32, in0, in1 \
+	VPADDQ in1, in2, in2 \
+	VPANDQ in3, in2, in0 \
+
+#define CARRY1() \
+	SHIFT_ADD_AND(Z0, Z10, Z1, Z8) \
+	SHIFT_ADD_AND(Z1, Z11, Z2, Z8) \
+	SHIFT_ADD_AND(Z2, Z12, Z3, Z8) \
+	SHIFT_ADD_AND(Z3, Z13, Z4, Z8) \
+
+#define CARRY2() \
+	SHIFT_ADD_AND(Z4, Z14, Z5, Z8) \
+	SHIFT_ADD_AND(Z5, Z15, Z6, Z8) \
+	SHIFT_ADD_AND(Z6, Z16, Z7, Z8) \
+	VPSRLQ $32, Z7, Z7             \
+
+#define CARRY3() \
+	VPSRLQ $32, Z0, Z10 \
+	VPANDQ Z8, Z0, Z0   \
+	VPADDQ Z10, Z1, Z1  \
+	VPSRLQ $32, Z1, Z11 \
+	VPANDQ Z8, Z1, Z1   \
+	VPADDQ Z11, Z2, Z2  \
+	VPSRLQ $32, Z2, Z12 \
+	VPANDQ Z8, Z2, Z2   \
+	VPADDQ Z12, Z3, Z3  \
+	VPSRLQ $32, Z3, Z13 \
+	VPANDQ Z8, Z3, Z3   \
+	VPADDQ Z13, Z4, Z4  \
+
+#define CARRY4() \
+	VPSRLQ $32, Z4, Z14 \
+	VPANDQ Z8, Z4, Z4   \
+	VPADDQ Z14, Z5, Z5  \
+	VPSRLQ $32, Z5, Z15 \
+	VPANDQ Z8, Z5, Z5   \
+	VPADDQ Z15, Z6, Z6  \
+	VPSRLQ $32, Z6, Z16 \
+	VPANDQ Z8, Z6, Z6   \
+	VPADDQ Z16, Z7, Z7  \
+
+	// t[0] -> R14
+	// t[1] -> R13
+	// t[2] -> CX
+	// t[3] -> BX
+	// y[0] -> DI
+	// y[1] -> R8
+	// y[2] -> R9
+	// y[3] -> R10
+	MOVQ res+0(FP), SI
+	MOVQ a+8(FP), R11
+	MOVQ b+16(FP), R15
+	MOVQ n+24(FP), R12
+	MOVQ 0(R15), DI
+	MOVQ 8(R15), R8
+	MOVQ 16(R15), R9
+	MOVQ 24(R15), R10
+	MOVQ R12, s0-8(SP)
+
+	// Create mask for low dword in each qword
+	VPCMPEQB  Y8, Y8, Y8
+	VPMOVZXDQ Y8, Z8
+	MOVQ      $0x5555, DX
+	KMOVD     DX, K1
+
+loop_16:
+	TESTQ     R12, R12
+	JEQ       done_15            // n == 0, we are done
+	MOVQ      0(R11), DX
+	VMOVDQU64 256+0*64(R11), Z16
+	VMOVDQU64 256+1*64(R11), Z17
+	VMOVDQU64 256+2*64(R11), Z18
+	VMOVDQU64 256+3*64(R11), Z19
+	VMOVDQU64 0(R15), Z24
+	VMOVDQU64 0(R15), Z25
+	VMOVDQU64 0(R15), Z26
+	VMOVDQU64 0(R15), Z27
+
+	// Transpose and expand x and y
+	VSHUFI64X2 $0x88, Z17, Z16, Z20
+	VSHUFI64X2 $0xdd, Z17, Z16, Z22
+	VSHUFI64X2 $0x88, Z19, Z18, Z21
+	VSHUFI64X2 $0xdd, Z19, Z18, Z23
+	VSHUFI64X2 $0x88, Z25, Z24, Z28
+	VSHUFI64X2 $0xdd, Z25, Z24, Z30
+	VSHUFI64X2 $0x88, Z27, Z26, Z29
+	VSHUFI64X2 $0xdd, Z27, Z26, Z31
+	VPERMQ     $0xd8, Z20, Z20
+	VPERMQ     $0xd8, Z21, Z21
+	VPERMQ     $0xd8, Z22, Z22
+	VPERMQ     $0xd8, Z23, Z23
+
+	// z[0] -> y * x[0]
+	MUL_WORD_0()
+	VPERMQ     $0xd8, Z28, Z28
+	VPERMQ     $0xd8, Z29, Z29
+	VPERMQ     $0xd8, Z30, Z30
+	VPERMQ     $0xd8, Z31, Z31
+	VSHUFI64X2 $0xd8, Z20, Z20, Z20
+	VSHUFI64X2 $0xd8, Z21, Z21, Z21
+	VSHUFI64X2 $0xd8, Z22, Z22, Z22
+	VSHUFI64X2 $0xd8, Z23, Z23, Z23
+
+	// z[0] -> y * x[1]
+	MOVQ       8(R11), DX
+	MUL_WORD_N()
+	VSHUFI64X2 $0xd8, Z28, Z28, Z28
+	VSHUFI64X2 $0xd8, Z29, Z29, Z29
+	VSHUFI64X2 $0xd8, Z30, Z30, Z30
+	VSHUFI64X2 $0xd8, Z31, Z31, Z31
+	VSHUFI64X2 $0x44, Z21, Z20, Z16
+	VSHUFI64X2 $0xee, Z21, Z20, Z18
+	VSHUFI64X2 $0x44, Z23, Z22, Z20
+	VSHUFI64X2 $0xee, Z23, Z22, Z22
+
+	// z[0] -> y * x[2]
+	MOVQ       16(R11), DX
+	MUL_WORD_N()
+	VSHUFI64X2 $0x44, Z29, Z28, Z24
+	VSHUFI64X2 $0xee, Z29, Z28, Z26
+	VSHUFI64X2 $0x44, Z31, Z30, Z28
+	VSHUFI64X2 $0xee, Z31, Z30, Z30
+	PREFETCHT0 1024(R11)
+	VPSRLQ     $32, Z16, Z17
+	VPSRLQ     $32, Z18, Z19
+	VPSRLQ     $32, Z20, Z21
+	VPSRLQ     $32, Z22, Z23
+	VPSRLQ     $32, Z24, Z25
+	VPSRLQ     $32, Z26, Z27
+	VPSRLQ     $32, Z28, Z29
+	VPSRLQ     $32, Z30, Z31
+
+	// z[0] -> y * x[3]
+	MOVQ   24(R11), DX
+	MUL_WORD_N()
+	VPANDQ Z8, Z16, Z16
+	VPANDQ Z8, Z18, Z18
+	VPANDQ Z8, Z20, Z20
+	VPANDQ Z8, Z22, Z22
+	VPANDQ Z8, Z24, Z24
+	VPANDQ Z8, Z26, Z26
+	VPANDQ Z8, Z28, Z28
+	VPANDQ Z8, Z30, Z30
+
+	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
+	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
+
+	// store output z[0]
+	MOVQ R14, 0(SI)
+	MOVQ R13, 8(SI)
+	MOVQ CX, 16(SI)
+	MOVQ BX, 24(SI)
+	ADDQ $32, R11
+	MOVQ 0(R11), DX
+
+	// For each 256-bit input value, each zmm register now represents a 32-bit input word zero-extended to 64 bits.
+	// Multiply y by doubleword 0 of x
+	VPMULUDQ      Z16, Z24, Z0
+	VPMULUDQ      Z16, Z25, Z1
+	VPMULUDQ      Z16, Z26, Z2
+	VPMULUDQ      Z16, Z27, Z3
+	VPMULUDQ      Z16, Z28, Z4
+	VPMULUDQ      Z16, Z29, Z5
+	VPMULUDQ      Z16, Z30, Z6
+	VPMULUDQ      Z16, Z31, Z7
+	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
+	VPSRLQ        $32, Z0, Z10
+	VPANDQ        Z8, Z0, Z0
+	VPADDQ        Z10, Z1, Z1
+	VPSRLQ        $32, Z1, Z11
+	VPANDQ        Z8, Z1, Z1
+	VPADDQ        Z11, Z2, Z2
+	VPSRLQ        $32, Z2, Z12
+	VPANDQ        Z8, Z2, Z2
+	VPADDQ        Z12, Z3, Z3
+	VPSRLQ        $32, Z3, Z13
+	VPANDQ        Z8, Z3, Z3
+	VPADDQ        Z13, Z4, Z4
+
+	// z[1] -> y * x[0]
+	MUL_WORD_0()
+	VPSRLQ        $32, Z4, Z14
+	VPANDQ        Z8, Z4, Z4
+	VPADDQ        Z14, Z5, Z5
+	VPSRLQ        $32, Z5, Z15
+	VPANDQ        Z8, Z5, Z5
+	VPADDQ        Z15, Z6, Z6
+	VPSRLQ        $32, Z6, Z16
+	VPANDQ        Z8, Z6, Z6
+	VPADDQ        Z16, Z7, Z7
+	VPMULUDQ.BCST ·qElement+0(SB), Z9, Z10
+	VPADDQ        Z10, Z0, Z0
+	VPMULUDQ.BCST ·qElement+4(SB), Z9, Z11
+	VPADDQ        Z11, Z1, Z1
+	VPMULUDQ.BCST ·qElement+8(SB), Z9, Z12
+	VPADDQ        Z12, Z2, Z2
+	VPMULUDQ.BCST ·qElement+12(SB), Z9, Z13
+	VPADDQ        Z13, Z3, Z3
+
+	// z[1] -> y * x[1]
+	MOVQ          8(R11), DX
+	MUL_WORD_N()
+	VPMULUDQ.BCST ·qElement+16(SB), Z9, Z14
+	VPADDQ        Z14, Z4, Z4
+	VPMULUDQ.BCST ·qElement+20(SB), Z9, Z15
+	VPADDQ        Z15, Z5, Z5
+	VPMULUDQ.BCST ·qElement+24(SB), Z9, Z16
+	VPADDQ        Z16, Z6, Z6
+	VPMULUDQ.BCST ·qElement+28(SB), Z9, Z10
+	VPADDQ        Z10, Z7, Z7
+	CARRY1()
+
+	// z[1] -> y * x[2]
+	MOVQ   16(R11), DX
+	MUL_WORD_N()
+	SHIFT_ADD_AND(Z4, Z14, Z5, Z8)
+	SHIFT_ADD_AND(Z5, Z15, Z6, Z8)
+	SHIFT_ADD_AND(Z6, Z16, Z7, Z8)
+	VPSRLQ $32, Z7, Z7
+
+	// Process doubleword 1 of x
+	VPMULUDQ Z17, Z24, Z10
+	VPADDQ   Z10, Z0, Z0
+	VPMULUDQ Z17, Z25, Z11
+	VPADDQ   Z11, Z1, Z1
+	VPMULUDQ Z17, Z26, Z12
+	VPADDQ   Z12, Z2, Z2
+	VPMULUDQ Z17, Z27, Z13
+	VPADDQ   Z13, Z3, Z3
+
+	// z[1] -> y * x[3]
+	MOVQ          24(R11), DX
+	MUL_WORD_N()
+	VPMULUDQ      Z17, Z28, Z14
+	VPADDQ        Z14, Z4, Z4
+	VPMULUDQ      Z17, Z29, Z15
+	VPADDQ        Z15, Z5, Z5
+	VPMULUDQ      Z17, Z30, Z16
+	VPADDQ        Z16, Z6, Z6
+	VPMULUDQ      Z17, Z31, Z17
+	VPADDQ        Z17, Z7, Z7
+	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
+
+	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
+	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
+
+	// store output z[1]
+	MOVQ R14, 32(SI)
+	MOVQ R13, 40(SI)
+	MOVQ CX, 48(SI)
+	MOVQ BX, 56(SI)
+	ADDQ $32, R11
+	MOVQ 0(R11), DX
+
+	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
+	VPSRLQ $32, Z0, Z10
+	VPANDQ Z8, Z0, Z0
+	VPADDQ Z10, Z1, Z1
+	VPSRLQ $32, Z1, Z11
+	VPANDQ Z8, Z1, Z1
+	VPADDQ Z11, Z2, Z2
+	VPSRLQ $32, Z2, Z12
+	VPANDQ Z8, Z2, Z2
+	VPADDQ Z12, Z3, Z3
+	VPSRLQ $32, Z3, Z13
+	VPANDQ Z8, Z3, Z3
+	VPADDQ Z13, Z4, Z4
+	CARRY4()
+
+	// z[2] -> y * x[0]
+	MUL_WORD_0()
+	AVX_MUL_Q_LO()
+	AVX_MUL_Q_HI()
+
+	// z[2] -> y * x[1]
+	MOVQ 8(R11), DX
+	MUL_WORD_N()
+	CARRY1()
+	CARRY2()
+
+	// z[2] -> y * x[2]
+	MOVQ 16(R11), DX
+	MUL_WORD_N()
+
+	// Process doubleword 2 of x
+	VPMULUDQ      Z18, Z24, Z10
+	VPADDQ        Z10, Z0, Z0
+	VPMULUDQ      Z18, Z25, Z11
+	VPADDQ        Z11, Z1, Z1
+	VPMULUDQ      Z18, Z26, Z12
+	VPADDQ        Z12, Z2, Z2
+	VPMULUDQ      Z18, Z27, Z13
+	VPADDQ        Z13, Z3, Z3
+	VPMULUDQ      Z18, Z28, Z14
+	VPADDQ        Z14, Z4, Z4
+	VPMULUDQ      Z18, Z29, Z15
+	VPADDQ        Z15, Z5, Z5
+	VPMULUDQ      Z18, Z30, Z16
+	VPADDQ        Z16, Z6, Z6
+	VPMULUDQ      Z18, Z31, Z17
+	VPADDQ        Z17, Z7, Z7
+	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
+
+	// z[2] -> y * x[3]
+	MOVQ 24(R11), DX
+	MUL_WORD_N()
+
+	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
+	CARRY3()
+
+	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
+	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
+
+	// store output z[2]
+	MOVQ R14, 64(SI)
+	MOVQ R13, 72(SI)
+	MOVQ CX, 80(SI)
+	MOVQ BX, 88(SI)
+	ADDQ $32, R11
+	MOVQ 0(R11), DX
+	CARRY4()
+	AVX_MUL_Q_LO()
+
+	// z[3] -> y * x[0]
+	MUL_WORD_0()
+	AVX_MUL_Q_HI()
+	CARRY1()
+	CARRY2()
+
+	// Process doubleword 3 of x
+	VPMULUDQ Z19, Z24, Z10
+	VPADDQ   Z10, Z0, Z0
+	VPMULUDQ Z19, Z25, Z11
+	VPADDQ   Z11, Z1, Z1
+	VPMULUDQ Z19, Z26, Z12
+	VPADDQ   Z12, Z2, Z2
+	VPMULUDQ Z19, Z27, Z13
+	VPADDQ   Z13, Z3, Z3
+
+	// z[3] -> y * x[1]
+	MOVQ     8(R11), DX
+	MUL_WORD_N()
+	VPMULUDQ Z19, Z28, Z14
+	VPADDQ   Z14, Z4, Z4
+	VPMULUDQ Z19, Z29, Z15
+	VPADDQ   Z15, Z5, Z5
+	VPMULUDQ Z19, Z30, Z16
+	VPADDQ   Z16, Z6, Z6
+	VPMULUDQ Z19, Z31, Z17
+	VPADDQ   Z17, Z7, Z7
+
+	// z[3] -> y * x[2]
+	MOVQ          16(R11), DX
+	MUL_WORD_N()
+	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
+	CARRY3()
+	CARRY4()
+
+	// z[3] -> y * x[3]
+	MOVQ 24(R11), DX
+	MUL_WORD_N()
+	AVX_MUL_Q_LO()
+	AVX_MUL_Q_HI()
+
+	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
+	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
+
+	// store output z[3]
+	MOVQ R14, 96(SI)
+	MOVQ R13, 104(SI)
+	MOVQ CX, 112(SI)
+	MOVQ BX, 120(SI)
+	ADDQ $32, R11
+	MOVQ 0(R11), DX
+
+	// Propagate carries and shift down by one dword
+	CARRY1()
+	CARRY2()
+
+	// Process doubleword 4 of x
+	VPMULUDQ Z20, Z24, Z10
+	VPADDQ   Z10, Z0, Z0
+	VPMULUDQ Z20, Z25, Z11
+	VPADDQ   Z11, Z1, Z1
+	VPMULUDQ Z20, Z26, Z12
+	VPADDQ   Z12, Z2, Z2
+	VPMULUDQ Z20, Z27, Z13
+	VPADDQ   Z13, Z3, Z3
+
+	// z[4] -> y * x[0]
+	MUL_WORD_0()
+	VPMULUDQ      Z20, Z28, Z14
+	VPADDQ        Z14, Z4, Z4
+	VPMULUDQ      Z20, Z29, Z15
+	VPADDQ        Z15, Z5, Z5
+	VPMULUDQ      Z20, Z30, Z16
+	VPADDQ        Z16, Z6, Z6
+	VPMULUDQ      Z20, Z31, Z17
+	VPADDQ        Z17, Z7, Z7
+	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
+
+	// z[4] -> y * x[1]
+	MOVQ 8(R11), DX
+	MUL_WORD_N()
+
+	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
+	CARRY3()
+	CARRY4()
+
+	// z[4] -> y * x[2]
+	MOVQ 16(R11), DX
+	MUL_WORD_N()
+
+	// zmm7 keeps all 64 bits
+	AVX_MUL_Q_LO()
+	AVX_MUL_Q_HI()
+
+	// z[4] -> y * x[3]
+	MOVQ 24(R11), DX
+	MUL_WORD_N()
+
+	// Propagate carries and shift down by one dword
+	CARRY1()
+	CARRY2()
+
+	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
+	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
+
+	// store output z[4]
+	MOVQ R14, 128(SI)
+	MOVQ R13, 136(SI)
+	MOVQ CX, 144(SI)
+	MOVQ BX, 152(SI)
+	ADDQ $32, R11
+	MOVQ 0(R11), DX
+
+	// Process doubleword 5 of x
+	VPMULUDQ Z21, Z24, Z10
+	VPADDQ   Z10, Z0, Z0
+	VPMULUDQ Z21, Z25, Z11
+	VPADDQ   Z11, Z1, Z1
+	VPMULUDQ Z21, Z26, Z12
+	VPADDQ   Z12, Z2, Z2
+	VPMULUDQ Z21, Z27, Z13
+	VPADDQ   Z13, Z3, Z3
+	VPMULUDQ Z21, Z28, Z14
+	VPADDQ   Z14, Z4, Z4
+	VPMULUDQ Z21, Z29, Z15
+	VPADDQ   Z15, Z5, Z5
+	VPMULUDQ Z21, Z30, Z16
+	VPADDQ   Z16, Z6, Z6
+	VPMULUDQ Z21, Z31, Z17
+	VPADDQ   Z17, Z7, Z7
+
+	// z[5] -> y * x[0]
+	MUL_WORD_0()
+	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
+
+	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
+	CARRY3()
+	CARRY4()
+
+	// z[5] -> y * x[1]
+	MOVQ 8(R11), DX
+	MUL_WORD_N()
+	AVX_MUL_Q_LO()
+	AVX_MUL_Q_HI()
+
+	// z[5] -> y * x[2]
+	MOVQ 16(R11), DX
+	MUL_WORD_N()
+	CARRY1()
+	CARRY2()
+
+	// z[5] -> y * x[3]
+	MOVQ 24(R11), DX
+	MUL_WORD_N()
+
+	// Process doubleword 6 of x
+	VPMULUDQ      Z22, Z24, Z10
+	VPADDQ        Z10, Z0, Z0
+	VPMULUDQ      Z22, Z25, Z11
+	VPADDQ        Z11, Z1, Z1
+	VPMULUDQ      Z22, Z26, Z12
+	VPADDQ        Z12, Z2, Z2
+	VPMULUDQ      Z22, Z27, Z13
+	VPADDQ        Z13, Z3, Z3
+	VPMULUDQ      Z22, Z28, Z14
+	VPADDQ        Z14, Z4, Z4
+	VPMULUDQ      Z22, Z29, Z15
+	VPADDQ        Z15, Z5, Z5
+	VPMULUDQ      Z22, Z30, Z16
+	VPADDQ        Z16, Z6, Z6
+	VPMULUDQ      Z22, Z31, Z17
+	VPADDQ        Z17, Z7, Z7
+	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
+
+	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
+	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
+
+	// store output z[5]
+	MOVQ R14, 160(SI)
+	MOVQ R13, 168(SI)
+	MOVQ CX, 176(SI)
+	MOVQ BX, 184(SI)
+	ADDQ $32, R11
+	MOVQ 0(R11), DX
+
+	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
+	CARRY3()
+	CARRY4()
+
+	// z[6] -> y * x[0]
+	MUL_WORD_0()
+	AVX_MUL_Q_LO()
+	AVX_MUL_Q_HI()
+
+	// z[6] -> y * x[1]
+	MOVQ 8(R11), DX
+	MUL_WORD_N()
+	CARRY1()
+	CARRY2()
+
+	// z[6] -> y * x[2]
+	MOVQ 16(R11), DX
+	MUL_WORD_N()
+
+	// Process doubleword 7 of x
+	VPMULUDQ      Z23, Z24, Z10
+	VPADDQ        Z10, Z0, Z0
+	VPMULUDQ      Z23, Z25, Z11
+	VPADDQ        Z11, Z1, Z1
+	VPMULUDQ      Z23, Z26, Z12
+	VPADDQ        Z12, Z2, Z2
+	VPMULUDQ      Z23, Z27, Z13
+	VPADDQ        Z13, Z3, Z3
+	VPMULUDQ      Z23, Z28, Z14
+	VPADDQ        Z14, Z4, Z4
+	VPMULUDQ      Z23, Z29, Z15
+	VPADDQ        Z15, Z5, Z5
+	VPMULUDQ      Z23, Z30, Z16
+	VPADDQ        Z16, Z6, Z6
+	VPMULUDQ      Z23, Z31, Z17
+	VPADDQ        Z17, Z7, Z7
+	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
+
+	// z[6] -> y * x[3]
+	MOVQ 24(R11), DX
+	MUL_WORD_N()
+	CARRY3()
+
+	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
+	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
+
+	// store output z[6]
+	MOVQ R14, 192(SI)
+	MOVQ R13, 200(SI)
+	MOVQ CX, 208(SI)
+	MOVQ BX, 216(SI)
+	ADDQ $32, R11
+	MOVQ 0(R11), DX
+	CARRY4()
+	AVX_MUL_Q_LO()
+	AVX_MUL_Q_HI()
+
+	// z[7] -> y * x[0]
+	MUL_WORD_0()
+	CARRY1()
+	CARRY2()
+
+	// z[7] -> y * x[1]
+	MOVQ 8(R11), DX
+	MUL_WORD_N()
+
+	// Conditional subtraction of the modulus
+	VPERMD.BCST.Z ·qElement+0(SB), Z8, K1, Z10
+	VPERMD.BCST.Z ·qElement+4(SB), Z8, K1, Z11
+	VPERMD.BCST.Z ·qElement+8(SB), Z8, K1, Z12
+	VPERMD.BCST.Z ·qElement+12(SB), Z8, K1, Z13
+	VPERMD.BCST.Z ·qElement+16(SB), Z8, K1, Z14
+	VPERMD.BCST.Z ·qElement+20(SB), Z8, K1, Z15
+	VPERMD.BCST.Z ·qElement+24(SB), Z8, K1, Z16
+	VPERMD.BCST.Z ·qElement+28(SB), Z8, K1, Z17
+	VPSUBQ        Z10, Z0, Z10
+	VPSRLQ        $63, Z10, Z20
+	VPANDQ        Z8, Z10, Z10
+	VPSUBQ        Z11, Z1, Z11
+	VPSUBQ        Z20, Z11, Z11
+	VPSRLQ        $63, Z11, Z21
+	VPANDQ        Z8, Z11, Z11
+	VPSUBQ        Z12, Z2, Z12
+	VPSUBQ        Z21, Z12, Z12
+	VPSRLQ        $63, Z12, Z22
+	VPANDQ        Z8, Z12, Z12
+	VPSUBQ        Z13, Z3, Z13
+	VPSUBQ        Z22, Z13, Z13
+	VPSRLQ        $63, Z13, Z23
+	VPANDQ        Z8, Z13, Z13
+	VPSUBQ        Z14, Z4, Z14
+	VPSUBQ        Z23, Z14, Z14
+	VPSRLQ        $63, Z14, Z24
+	VPANDQ        Z8, Z14, Z14
+	VPSUBQ        Z15, Z5, Z15
+	VPSUBQ        Z24, Z15, Z15
+	VPSRLQ        $63, Z15, Z25
+	VPANDQ        Z8, Z15, Z15
+	VPSUBQ        Z16, Z6, Z16
+	VPSUBQ        Z25, Z16, Z16
+	VPSRLQ        $63, Z16, Z26
+	VPANDQ        Z8, Z16, Z16
+	VPSUBQ        Z17, Z7, Z17
+	VPSUBQ        Z26, Z17, Z17
+	VPMOVQ2M      Z17, K2
+	KNOTB         K2, K2
+	VMOVDQU64     Z10, K2, Z0
+	VMOVDQU64     Z11, K2, Z1
+	VMOVDQU64     Z12, K2, Z2
+	VMOVDQU64     Z13, K2, Z3
+	VMOVDQU64     Z14, K2, Z4
+
+	// z[7] -> y * x[2]
+	MOVQ      16(R11), DX
+	MUL_WORD_N()
+	VMOVDQU64 Z15, K2, Z5
+	VMOVDQU64 Z16, K2, Z6
+	VMOVDQU64 Z17, K2, Z7
+
+	// Transpose results back
+	VALIGND   $0, ·pattern1+0(SB), Z11, Z11
+	VALIGND   $0, ·pattern2+0(SB), Z12, Z12
+	VALIGND   $0, ·pattern3+0(SB), Z13, Z13
+	VALIGND   $0, ·pattern4+0(SB), Z14, Z14
+	VPSLLQ    $32, Z1, Z1
+	VPORQ     Z1, Z0, Z0
+	VPSLLQ    $32, Z3, Z3
+	VPORQ     Z3, Z2, Z1
+	VPSLLQ    $32, Z5, Z5
+	VPORQ     Z5, Z4, Z2
+	VPSLLQ    $32, Z7, Z7
+	VPORQ     Z7, Z6, Z3
+	VMOVDQU64 Z0, Z4
+	VMOVDQU64 Z2, Z6
+
+	// z[7] -> y * x[3]
+	MOVQ     24(R11), DX
+	MUL_WORD_N()
+	VPERMT2Q Z1, Z11, Z0
+	VPERMT2Q Z4, Z12, Z1
+	VPERMT2Q Z3, Z11, Z2
+	VPERMT2Q Z6, Z12, Z3
+
+	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
+	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
+
+	// store output z[7]
+	MOVQ      R14, 224(SI)
+	MOVQ      R13, 232(SI)
+	MOVQ      CX, 240(SI)
+	MOVQ      BX, 248(SI)
+	ADDQ      $288, R11
+	VMOVDQU64 Z0, Z4
+	VMOVDQU64 Z1, Z5
+	VPERMT2Q  Z2, Z13, Z0
+	VPERMT2Q  Z4, Z14, Z2
+	VPERMT2Q  Z3, Z13, Z1
+	VPERMT2Q  Z5, Z14, Z3
+
+	// Save AVX-512 results
+	VMOVDQU64 Z0, 256+0*64(SI)
+	VMOVDQU64 Z2, 256+1*64(SI)
+	VMOVDQU64 Z1, 256+2*64(SI)
+	VMOVDQU64 Z3, 256+3*64(SI)
+	ADDQ      $512, SI
+	MOVQ      s0-8(SP), R12
+	DECQ      R12              // decrement n
+	MOVQ      R12, s0-8(SP)
+	JMP       loop_16
+
+done_15:
+	RET
+
+TEXT ·mulVec(SB), $8-40
+	// t[0] -> R14
+	// t[1] -> R13
+	// t[2] -> CX
+	// t[3] -> BX
+	// y[0] -> DI
+	// y[1] -> R8
+	// y[2] -> R9
+	// y[3] -> R10
+	MOVQ res+0(FP), SI
+	MOVQ a+8(FP), R11
+	MOVQ b+16(FP), R15
+	MOVQ n+24(FP), R12
+	MOVQ R12, s0-8(SP)
+
+	// Create mask for low dword in each qword
+	VPCMPEQB  Y8, Y8, Y8
+	VPMOVZXDQ Y8, Z8
+	MOVQ      $0x5555, DX
+	KMOVD     DX, K1
+
+loop_18:
+	TESTQ     R12, R12
+	JEQ       done_17            // n == 0, we are done
+	MOVQ      0(R11), DX
+	VMOVDQU64 256+0*64(R11), Z16
+	VMOVDQU64 256+1*64(R11), Z17
+	VMOVDQU64 256+2*64(R11), Z18
+	VMOVDQU64 256+3*64(R11), Z19
+
+	// load input y[0]
+	MOVQ      0(R15), DI
+	MOVQ      8(R15), R8
+	MOVQ      16(R15), R9
+	MOVQ      24(R15), R10
+	VMOVDQU64 256+0*64(R15), Z24
+	VMOVDQU64 256+1*64(R15), Z25
+	VMOVDQU64 256+2*64(R15), Z26
+	VMOVDQU64 256+3*64(R15), Z27
+
+	// Transpose and expand x and y
+	VSHUFI64X2 $0x88, Z17, Z16, Z20
+	VSHUFI64X2 $0xdd, Z17, Z16, Z22
+	VSHUFI64X2 $0x88, Z19, Z18, Z21
+	VSHUFI64X2 $0xdd, Z19, Z18, Z23
+	VSHUFI64X2 $0x88, Z25, Z24, Z28
+	VSHUFI64X2 $0xdd, Z25, Z24, Z30
+	VSHUFI64X2 $0x88, Z27, Z26, Z29
+	VSHUFI64X2 $0xdd, Z27, Z26, Z31
+	VPERMQ     $0xd8, Z20, Z20
+	VPERMQ     $0xd8, Z21, Z21
+	VPERMQ     $0xd8, Z22, Z22
+	VPERMQ     $0xd8, Z23, Z23
+
+	// z[0] -> y * x[0]
+	MUL_WORD_0()
+	VPERMQ     $0xd8, Z28, Z28
+	VPERMQ     $0xd8, Z29, Z29
+	VPERMQ     $0xd8, Z30, Z30
+	VPERMQ     $0xd8, Z31, Z31
+	VSHUFI64X2 $0xd8, Z20, Z20, Z20
+	VSHUFI64X2 $0xd8, Z21, Z21, Z21
+	VSHUFI64X2 $0xd8, Z22, Z22, Z22
+	VSHUFI64X2 $0xd8, Z23, Z23, Z23
+
+	// z[0] -> y * x[1]
+	MOVQ       8(R11), DX
+	MUL_WORD_N()
+	VSHUFI64X2 $0xd8, Z28, Z28, Z28
+	VSHUFI64X2 $0xd8, Z29, Z29, Z29
+	VSHUFI64X2 $0xd8, Z30, Z30, Z30
+	VSHUFI64X2 $0xd8, Z31, Z31, Z31
+	VSHUFI64X2 $0x44, Z21, Z20, Z16
+	VSHUFI64X2 $0xee, Z21, Z20, Z18
+	VSHUFI64X2 $0x44, Z23, Z22, Z20
+	VSHUFI64X2 $0xee, Z23, Z22, Z22
+
+	// z[0] -> y * x[2]
+	MOVQ       16(R11), DX
+	MUL_WORD_N()
+	VSHUFI64X2 $0x44, Z29, Z28, Z24
+	VSHUFI64X2 $0xee, Z29, Z28, Z26
+	VSHUFI64X2 $0x44, Z31, Z30, Z28
+	VSHUFI64X2 $0xee, Z31, Z30, Z30
+	PREFETCHT0 1024(R11)
+	VPSRLQ     $32, Z16, Z17
+	VPSRLQ     $32, Z18, Z19
+	VPSRLQ     $32, Z20, Z21
+	VPSRLQ     $32, Z22, Z23
+	VPSRLQ     $32, Z24, Z25
+	VPSRLQ     $32, Z26, Z27
+	VPSRLQ     $32, Z28, Z29
+	VPSRLQ     $32, Z30, Z31
+
+	// z[0] -> y * x[3]
+	MOVQ   24(R11), DX
+	MUL_WORD_N()
+	VPANDQ Z8, Z16, Z16
+	VPANDQ Z8, Z18, Z18
+	VPANDQ Z8, Z20, Z20
+	VPANDQ Z8, Z22, Z22
+	VPANDQ Z8, Z24, Z24
+	VPANDQ Z8, Z26, Z26
+	VPANDQ Z8, Z28, Z28
+	VPANDQ Z8, Z30, Z30
+
+	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
+	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
+
+	// store output z[0]
+	MOVQ R14, 0(SI)
+	MOVQ R13, 8(SI)
+	MOVQ CX, 16(SI)
+	MOVQ BX, 24(SI)
+	ADDQ $32, R11
+	MOVQ 0(R11), DX
+
+	// For each 256-bit input value, each zmm register now represents a 32-bit input word zero-extended to 64 bits.
+	// Multiply y by doubleword 0 of x
+	VPMULUDQ   Z16, Z24, Z0
+	VPMULUDQ   Z16, Z25, Z1
+	VPMULUDQ   Z16, Z26, Z2
+	VPMULUDQ   Z16, Z27, Z3
+	VPMULUDQ   Z16, Z28, Z4
+	PREFETCHT0 1024(R15)
+	VPMULUDQ   Z16, Z29, Z5
+	VPMULUDQ   Z16, Z30, Z6
+	VPMULUDQ   Z16, Z31, Z7
+
+	// load input y[1]
+	MOVQ          32(R15), DI
+	MOVQ          40(R15), R8
+	MOVQ          48(R15), R9
+	MOVQ          56(R15), R10
+	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
+	VPSRLQ        $32, Z0, Z10
+	VPANDQ        Z8, Z0, Z0
+	VPADDQ        Z10, Z1, Z1
+	VPSRLQ        $32, Z1, Z11
+	VPANDQ        Z8, Z1, Z1
+	VPADDQ        Z11, Z2, Z2
+	VPSRLQ        $32, Z2, Z12
+	VPANDQ        Z8, Z2, Z2
+	VPADDQ        Z12, Z3, Z3
+	VPSRLQ        $32, Z3, Z13
+	VPANDQ        Z8, Z3, Z3
+	VPADDQ        Z13, Z4, Z4
+
+	// z[1] -> y * x[0]
+	MUL_WORD_0()
+	VPSRLQ        $32, Z4, Z14
+	VPANDQ        Z8, Z4, Z4
+	VPADDQ        Z14, Z5, Z5
+	VPSRLQ        $32, Z5, Z15
+	VPANDQ        Z8, Z5, Z5
+	VPADDQ        Z15, Z6, Z6
+	VPSRLQ        $32, Z6, Z16
+	VPANDQ        Z8, Z6, Z6
+	VPADDQ        Z16, Z7, Z7
+	VPMULUDQ.BCST ·qElement+0(SB), Z9, Z10
+	VPADDQ        Z10, Z0, Z0
+	VPMULUDQ.BCST ·qElement+4(SB), Z9, Z11
+	VPADDQ        Z11, Z1, Z1
+	VPMULUDQ.BCST ·qElement+8(SB), Z9, Z12
+	VPADDQ        Z12, Z2, Z2
+	VPMULUDQ.BCST ·qElement+12(SB), Z9, Z13
+	VPADDQ        Z13, Z3, Z3
+
+	// z[1] -> y * x[1]
+	MOVQ          8(R11), DX
+	MUL_WORD_N()
+	VPMULUDQ.BCST ·qElement+16(SB), Z9, Z14
+	VPADDQ        Z14, Z4, Z4
+	VPMULUDQ.BCST ·qElement+20(SB), Z9, Z15
+	VPADDQ        Z15, Z5, Z5
+	VPMULUDQ.BCST ·qElement+24(SB), Z9, Z16
+	VPADDQ        Z16, Z6, Z6
+	VPMULUDQ.BCST ·qElement+28(SB), Z9, Z10
+	VPADDQ        Z10, Z7, Z7
+	CARRY1()
+
+	// z[1] -> y * x[2]
+	MOVQ   16(R11), DX
+	MUL_WORD_N()
+	SHIFT_ADD_AND(Z4, Z14, Z5, Z8)
+	SHIFT_ADD_AND(Z5, Z15, Z6, Z8)
+	SHIFT_ADD_AND(Z6, Z16, Z7, Z8)
+	VPSRLQ $32, Z7, Z7
+
+	// Process doubleword 1 of x
+	VPMULUDQ Z17, Z24, Z10
+	VPADDQ   Z10, Z0, Z0
+	VPMULUDQ Z17, Z25, Z11
+	VPADDQ   Z11, Z1, Z1
+	VPMULUDQ Z17, Z26, Z12
+	VPADDQ   Z12, Z2, Z2
+	VPMULUDQ Z17, Z27, Z13
+	VPADDQ   Z13, Z3, Z3
+
+	// z[1] -> y * x[3]
+	MOVQ          24(R11), DX
+	MUL_WORD_N()
+	VPMULUDQ      Z17, Z28, Z14
+	VPADDQ        Z14, Z4, Z4
+	VPMULUDQ      Z17, Z29, Z15
+	VPADDQ        Z15, Z5, Z5
+	VPMULUDQ      Z17, Z30, Z16
+	VPADDQ        Z16, Z6, Z6
+	VPMULUDQ      Z17, Z31, Z17
+	VPADDQ        Z17, Z7, Z7
+	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
+
+	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
+	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
+
+	// store output z[1]
+	MOVQ R14, 32(SI)
+	MOVQ R13, 40(SI)
+	MOVQ CX, 48(SI)
+	MOVQ BX, 56(SI)
+	ADDQ $32, R11
+	MOVQ 0(R11), DX
+
+	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
+	VPSRLQ $32, Z0, Z10
+	VPANDQ Z8, Z0, Z0
+	VPADDQ Z10, Z1, Z1
+	VPSRLQ $32, Z1, Z11
+	VPANDQ Z8, Z1, Z1
+	VPADDQ Z11, Z2, Z2
+	VPSRLQ $32, Z2, Z12
+	VPANDQ Z8, Z2, Z2
+	VPADDQ Z12, Z3, Z3
+
+	// load input y[2]
+	MOVQ   64(R15), DI
+	MOVQ   72(R15), R8
+	MOVQ   80(R15), R9
+	MOVQ   88(R15), R10
+	VPSRLQ $32, Z3, Z13
+	VPANDQ Z8, Z3, Z3
+	VPADDQ Z13, Z4, Z4
+	CARRY4()
+
+	// z[2] -> y * x[0]
+	MUL_WORD_0()
+	AVX_MUL_Q_LO()
+	AVX_MUL_Q_HI()
+
+	// z[2] -> y * x[1]
+	MOVQ 8(R11), DX
+	MUL_WORD_N()
+	CARRY1()
+	CARRY2()
+
+	// z[2] -> y * x[2]
+	MOVQ 16(R11), DX
+	MUL_WORD_N()
+
+	// Process doubleword 2 of x
+	VPMULUDQ      Z18, Z24, Z10
+	VPADDQ        Z10, Z0, Z0
+	VPMULUDQ      Z18, Z25, Z11
+	VPADDQ        Z11, Z1, Z1
+	VPMULUDQ      Z18, Z26, Z12
+	VPADDQ        Z12, Z2, Z2
+	VPMULUDQ      Z18, Z27, Z13
+	VPADDQ        Z13, Z3, Z3
+	VPMULUDQ      Z18, Z28, Z14
+	VPADDQ        Z14, Z4, Z4
+	VPMULUDQ      Z18, Z29, Z15
+	VPADDQ        Z15, Z5, Z5
+	VPMULUDQ      Z18, Z30, Z16
+	VPADDQ        Z16, Z6, Z6
+	VPMULUDQ      Z18, Z31, Z17
+	VPADDQ        Z17, Z7, Z7
+	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
+
+	// z[2] -> y * x[3]
+	MOVQ 24(R11), DX
+	MUL_WORD_N()
+
+	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
+	CARRY3()
+
+	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
+	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
+
+	// store output z[2]
+	MOVQ R14, 64(SI)
+	MOVQ R13, 72(SI)
+	MOVQ CX, 80(SI)
+	MOVQ BX, 88(SI)
+	ADDQ $32, R11
+	MOVQ 0(R11), DX
+
+	// load input y[3]
+	MOVQ 96(R15), DI
+	MOVQ 104(R15), R8
+	MOVQ 112(R15), R9
+	MOVQ 120(R15), R10
+	CARRY4()
+	AVX_MUL_Q_LO()
+
+	// z[3] -> y * x[0]
+	MUL_WORD_0()
+	AVX_MUL_Q_HI()
+	CARRY1()
+	CARRY2()
+
+	// Process doubleword 3 of x
+	VPMULUDQ Z19, Z24, Z10
+	VPADDQ   Z10, Z0, Z0
+	VPMULUDQ Z19, Z25, Z11
+	VPADDQ   Z11, Z1, Z1
+	VPMULUDQ Z19, Z26, Z12
+	VPADDQ   Z12, Z2, Z2
+	VPMULUDQ Z19, Z27, Z13
+	VPADDQ   Z13, Z3, Z3
+
+	// z[3] -> y * x[1]
+	MOVQ     8(R11), DX
+	MUL_WORD_N()
+	VPMULUDQ Z19, Z28, Z14
+	VPADDQ   Z14, Z4, Z4
+	VPMULUDQ Z19, Z29, Z15
+	VPADDQ   Z15, Z5, Z5
+	VPMULUDQ Z19, Z30, Z16
+	VPADDQ   Z16, Z6, Z6
+	VPMULUDQ Z19, Z31, Z17
+	VPADDQ   Z17, Z7, Z7
+
+	// z[3] -> y * x[2]
+	MOVQ          16(R11), DX
+	MUL_WORD_N()
+	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
+	CARRY3()
+	CARRY4()
+
+	// z[3] -> y * x[3]
+	MOVQ 24(R11), DX
+	MUL_WORD_N()
+	AVX_MUL_Q_LO()
+	AVX_MUL_Q_HI()
+
+	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
+	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
+
+	// store output z[3]
+	MOVQ R14, 96(SI)
+	MOVQ R13, 104(SI)
+	MOVQ CX, 112(SI)
+	MOVQ BX, 120(SI)
+	ADDQ $32, R11
+	MOVQ 0(R11), DX
+
+	// Propagate carries and shift down by one dword
+	CARRY1()
+	CARRY2()
+
+	// load input y[4]
+	MOVQ 128(R15), DI
+	MOVQ 136(R15), R8
+	MOVQ 144(R15), R9
+	MOVQ 152(R15), R10
+
+	// Process doubleword 4 of x
+	VPMULUDQ Z20, Z24, Z10
+	VPADDQ   Z10, Z0, Z0
+	VPMULUDQ Z20, Z25, Z11
+	VPADDQ   Z11, Z1, Z1
+	VPMULUDQ Z20, Z26, Z12
+	VPADDQ   Z12, Z2, Z2
+	VPMULUDQ Z20, Z27, Z13
+	VPADDQ   Z13, Z3, Z3
+
+	// z[4] -> y * x[0]
+	MUL_WORD_0()
+	VPMULUDQ      Z20, Z28, Z14
+	VPADDQ        Z14, Z4, Z4
+	VPMULUDQ      Z20, Z29, Z15
+	VPADDQ        Z15, Z5, Z5
+	VPMULUDQ      Z20, Z30, Z16
+	VPADDQ        Z16, Z6, Z6
+	VPMULUDQ      Z20, Z31, Z17
+	VPADDQ        Z17, Z7, Z7
+	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
+
+	// z[4] -> y * x[1]
+	MOVQ 8(R11), DX
+	MUL_WORD_N()
+
+	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
+	CARRY3()
+	CARRY4()
+
+	// z[4] -> y * x[2]
+	MOVQ 16(R11), DX
+	MUL_WORD_N()
+
+	// zmm7 keeps all 64 bits
+	AVX_MUL_Q_LO()
+	AVX_MUL_Q_HI()
+
+	// z[4] -> y * x[3]
+	MOVQ 24(R11), DX
+	MUL_WORD_N()
+
+	// Propagate carries and shift down by one dword
+	CARRY1()
+	CARRY2()
+
+	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
+	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
+
+	// store output z[4]
+	MOVQ R14, 128(SI)
+	MOVQ R13, 136(SI)
+	MOVQ CX, 144(SI)
+	MOVQ BX, 152(SI)
+	ADDQ $32, R11
+	MOVQ 0(R11), DX
+
+	// Process doubleword 5 of x
+	VPMULUDQ Z21, Z24, Z10
+	VPADDQ   Z10, Z0, Z0
+	VPMULUDQ Z21, Z25, Z11
+	VPADDQ   Z11, Z1, Z1
+	VPMULUDQ Z21, Z26, Z12
+	VPADDQ   Z12, Z2, Z2
+	VPMULUDQ Z21, Z27, Z13
+	VPADDQ   Z13, Z3, Z3
+
+	// load input y[5]
+	MOVQ     160(R15), DI
+	MOVQ     168(R15), R8
+	MOVQ     176(R15), R9
+	MOVQ     184(R15), R10
+	VPMULUDQ Z21, Z28, Z14
+	VPADDQ   Z14, Z4, Z4
+	VPMULUDQ Z21, Z29, Z15
+	VPADDQ   Z15, Z5, Z5
+	VPMULUDQ Z21, Z30, Z16
+	VPADDQ   Z16, Z6, Z6
+	VPMULUDQ Z21, Z31, Z17
+	VPADDQ   Z17, Z7, Z7
+
+	// z[5] -> y * x[0]
+	MUL_WORD_0()
+	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
+
+	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
+	CARRY3()
+	CARRY4()
+
+	// z[5] -> y * x[1]
+	MOVQ 8(R11), DX
+	MUL_WORD_N()
+	AVX_MUL_Q_LO()
+	AVX_MUL_Q_HI()
+
+	// z[5] -> y * x[2]
+	MOVQ 16(R11), DX
+	MUL_WORD_N()
+	CARRY1()
+	CARRY2()
+
+	// z[5] -> y * x[3]
+	MOVQ 24(R11), DX
+	MUL_WORD_N()
+
+	// Process doubleword 6 of x
+	VPMULUDQ      Z22, Z24, Z10
+	VPADDQ        Z10, Z0, Z0
+	VPMULUDQ      Z22, Z25, Z11
+	VPADDQ        Z11, Z1, Z1
+	VPMULUDQ      Z22, Z26, Z12
+	VPADDQ        Z12, Z2, Z2
+	VPMULUDQ      Z22, Z27, Z13
+	VPADDQ        Z13, Z3, Z3
+	VPMULUDQ      Z22, Z28, Z14
+	VPADDQ        Z14, Z4, Z4
+	VPMULUDQ      Z22, Z29, Z15
+	VPADDQ        Z15, Z5, Z5
+	VPMULUDQ      Z22, Z30, Z16
+	VPADDQ        Z16, Z6, Z6
+	VPMULUDQ      Z22, Z31, Z17
+	VPADDQ        Z17, Z7, Z7
+	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
+
+	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
+	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
+
+	// store output z[5]
+	MOVQ R14, 160(SI)
+	MOVQ R13, 168(SI)
+	MOVQ CX, 176(SI)
+	MOVQ BX, 184(SI)
+	ADDQ $32, R11
+	MOVQ 0(R11), DX
+
+	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
+	CARRY3()
+
+	// load input y[6]
+	MOVQ 192(R15), DI
+	MOVQ 200(R15), R8
+	MOVQ 208(R15), R9
+	MOVQ 216(R15), R10
+	CARRY4()
+
+	// z[6] -> y * x[0]
+	MUL_WORD_0()
+	AVX_MUL_Q_LO()
+	AVX_MUL_Q_HI()
+
+	// z[6] -> y * x[1]
+	MOVQ 8(R11), DX
+	MUL_WORD_N()
+	CARRY1()
+	CARRY2()
+
+	// z[6] -> y * x[2]
+	MOVQ 16(R11), DX
+	MUL_WORD_N()
+
+	// Process doubleword 7 of x
+	VPMULUDQ      Z23, Z24, Z10
+	VPADDQ        Z10, Z0, Z0
+	VPMULUDQ      Z23, Z25, Z11
+	VPADDQ        Z11, Z1, Z1
+	VPMULUDQ      Z23, Z26, Z12
+	VPADDQ        Z12, Z2, Z2
+	VPMULUDQ      Z23, Z27, Z13
+	VPADDQ        Z13, Z3, Z3
+	VPMULUDQ      Z23, Z28, Z14
+	VPADDQ        Z14, Z4, Z4
+	VPMULUDQ      Z23, Z29, Z15
+	VPADDQ        Z15, Z5, Z5
+	VPMULUDQ      Z23, Z30, Z16
+	VPADDQ        Z16, Z6, Z6
+	VPMULUDQ      Z23, Z31, Z17
+	VPADDQ        Z17, Z7, Z7
+	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
+
+	// z[6] -> y * x[3]
+	MOVQ 24(R11), DX
+	MUL_WORD_N()
+	CARRY3()
+
+	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
+	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
+
+	// store output z[6]
+	MOVQ R14, 192(SI)
+	MOVQ R13, 200(SI)
+	MOVQ CX, 208(SI)
+	MOVQ BX, 216(SI)
+	ADDQ $32, R11
+	MOVQ 0(R11), DX
+	CARRY4()
+
+	// load input y[7]
+	MOVQ 224(R15), DI
+	MOVQ 232(R15), R8
+	MOVQ 240(R15), R9
+	MOVQ 248(R15), R10
+	AVX_MUL_Q_LO()
+	AVX_MUL_Q_HI()
+
+	// z[7] -> y * x[0]
+	MUL_WORD_0()
+	CARRY1()
+	CARRY2()
+
+	// z[7] -> y * x[1]
+	MOVQ 8(R11), DX
+	MUL_WORD_N()
+
+	// Conditional subtraction of the modulus
+	VPERMD.BCST.Z ·qElement+0(SB), Z8, K1, Z10
+	VPERMD.BCST.Z ·qElement+4(SB), Z8, K1, Z11
+	VPERMD.BCST.Z ·qElement+8(SB), Z8, K1, Z12
+	VPERMD.BCST.Z ·qElement+12(SB), Z8, K1, Z13
+	VPERMD.BCST.Z ·qElement+16(SB), Z8, K1, Z14
+	VPERMD.BCST.Z ·qElement+20(SB), Z8, K1, Z15
+	VPERMD.BCST.Z ·qElement+24(SB), Z8, K1, Z16
+	VPERMD.BCST.Z ·qElement+28(SB), Z8, K1, Z17
+	VPSUBQ        Z10, Z0, Z10
+	VPSRLQ        $63, Z10, Z20
+	VPANDQ        Z8, Z10, Z10
+	VPSUBQ        Z11, Z1, Z11
+	VPSUBQ        Z20, Z11, Z11
+	VPSRLQ        $63, Z11, Z21
+	VPANDQ        Z8, Z11, Z11
+	VPSUBQ        Z12, Z2, Z12
+	VPSUBQ        Z21, Z12, Z12
+	VPSRLQ        $63, Z12, Z22
+	VPANDQ        Z8, Z12, Z12
+	VPSUBQ        Z13, Z3, Z13
+	VPSUBQ        Z22, Z13, Z13
+	VPSRLQ        $63, Z13, Z23
+	VPANDQ        Z8, Z13, Z13
+	VPSUBQ        Z14, Z4, Z14
+	VPSUBQ        Z23, Z14, Z14
+	VPSRLQ        $63, Z14, Z24
+	VPANDQ        Z8, Z14, Z14
+	VPSUBQ        Z15, Z5, Z15
+	VPSUBQ        Z24, Z15, Z15
+	VPSRLQ        $63, Z15, Z25
+	VPANDQ        Z8, Z15, Z15
+	VPSUBQ        Z16, Z6, Z16
+	VPSUBQ        Z25, Z16, Z16
+	VPSRLQ        $63, Z16, Z26
+	VPANDQ        Z8, Z16, Z16
+	VPSUBQ        Z17, Z7, Z17
+	VPSUBQ        Z26, Z17, Z17
+	VPMOVQ2M      Z17, K2
+	KNOTB         K2, K2
+	VMOVDQU64     Z10, K2, Z0
+	VMOVDQU64     Z11, K2, Z1
+	VMOVDQU64     Z12, K2, Z2
+	VMOVDQU64     Z13, K2, Z3
+	VMOVDQU64     Z14, K2, Z4
+
+	// z[7] -> y * x[2]
+	MOVQ      16(R11), DX
+	MUL_WORD_N()
+	VMOVDQU64 Z15, K2, Z5
+	VMOVDQU64 Z16, K2, Z6
+	VMOVDQU64 Z17, K2, Z7
+
+	// Transpose results back
+	VALIGND   $0, ·pattern1+0(SB), Z11, Z11
+	VALIGND   $0, ·pattern2+0(SB), Z12, Z12
+	VALIGND   $0, ·pattern3+0(SB), Z13, Z13
+	VALIGND   $0, ·pattern4+0(SB), Z14, Z14
+	VPSLLQ    $32, Z1, Z1
+	VPORQ     Z1, Z0, Z0
+	VPSLLQ    $32, Z3, Z3
+	VPORQ     Z3, Z2, Z1
+	VPSLLQ    $32, Z5, Z5
+	VPORQ     Z5, Z4, Z2
+	VPSLLQ    $32, Z7, Z7
+	VPORQ     Z7, Z6, Z3
+	VMOVDQU64 Z0, Z4
+	VMOVDQU64 Z2, Z6
+
+	// z[7] -> y * x[3]
+	MOVQ     24(R11), DX
+	MUL_WORD_N()
+	VPERMT2Q Z1, Z11, Z0
+	VPERMT2Q Z4, Z12, Z1
+	VPERMT2Q Z3, Z11, Z2
+	VPERMT2Q Z6, Z12, Z3
+
+	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
+	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
+
+	// store output z[7]
+	MOVQ      R14, 224(SI)
+	MOVQ      R13, 232(SI)
+	MOVQ      CX, 240(SI)
+	MOVQ      BX, 248(SI)
+	ADDQ      $288, R11
+	VMOVDQU64 Z0, Z4
+	VMOVDQU64 Z1, Z5
+	VPERMT2Q  Z2, Z13, Z0
+	VPERMT2Q  Z4, Z14, Z2
+	VPERMT2Q  Z3, Z13, Z1
+	VPERMT2Q  Z5, Z14, Z3
+
+	// Save AVX-512 results
+	VMOVDQU64 Z0, 256+0*64(SI)
+	VMOVDQU64 Z2, 256+1*64(SI)
+	VMOVDQU64 Z1, 256+2*64(SI)
+	VMOVDQU64 Z3, 256+3*64(SI)
+	ADDQ      $512, SI
+	ADDQ      $512, R15
+	MOVQ      s0-8(SP), R12
+	DECQ      R12              // decrement n
+	MOVQ      R12, s0-8(SP)
+	JMP       loop_18
+
+done_17:
+	RET
diff --git a/tmp/gnark-crypto-new/field/asm/element_4w/element_4w_arm64.h b/field/asm/element_4w/element_4w_arm64.h
new file mode 100644
index 0000000000..fce96e3000
--- /dev/null
+++ b/field/asm/element_4w/element_4w_arm64.h
@@ -0,0 +1,163 @@
+// Code generated by gnark-crypto/generator. DO NOT EDIT.
+#include "textflag.h"
+#include "funcdata.h"
+#include "go_asm.h"
+
+// butterfly(a, b *Element)
+// a, b = a+b, a-b
+TEXT ·Butterfly(SB), NOFRAME|NOSPLIT, $0-16
+	LDP  x+0(FP), (R16, R17)
+	LDP  0(R16), (R0, R1)
+	LDP  16(R16), (R2, R3)
+	LDP  0(R17), (R4, R5)
+	LDP  16(R17), (R6, R7)
+	ADDS R0, R4, R8
+	ADCS R1, R5, R9
+	ADCS R2, R6, R10
+	ADC  R3, R7, R11
+	SUBS R4, R0, R4
+	SBCS R5, R1, R5
+	SBCS R6, R2, R6
+	SBCS R7, R3, R7
+	LDP  ·qElement+0(SB), (R0, R1)
+	CSEL CS, ZR, R0, R12
+	CSEL CS, ZR, R1, R13
+	LDP  ·qElement+16(SB), (R2, R3)
+	CSEL CS, ZR, R2, R14
+	CSEL CS, ZR, R3, R15
+
+	// add q if underflow, 0 if not
+	ADDS R4, R12, R4
+	ADCS R5, R13, R5
+	STP  (R4, R5), 0(R17)
+	ADCS R6, R14, R6
+	ADC  R7, R15, R7
+	STP  (R6, R7), 16(R17)
+
+	// q = t - q
+	SUBS R0, R8, R0
+	SBCS R1, R9, R1
+	SBCS R2, R10, R2
+	SBCS R3, R11, R3
+
+	// if no borrow, return q, else return t
+	CSEL CS, R0, R8, R8
+	CSEL CS, R1, R9, R9
+	STP  (R8, R9), 0(R16)
+	CSEL CS, R2, R10, R10
+	CSEL CS, R3, R11, R11
+	STP  (R10, R11), 16(R16)
+	RET
+
+// mul(res, x, y *Element)
+// Algorithm 2 of Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS
+// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
+TEXT ·mul(SB), NOFRAME|NOSPLIT, $0-24
+#define DIVSHIFT() \
+	MUL   R13, R12, R0 \
+	ADDS  R0, R6, R6   \
+	MUL   R14, R12, R0 \
+	ADCS  R0, R7, R7   \
+	MUL   R15, R12, R0 \
+	ADCS  R0, R8, R8   \
+	MUL   R16, R12, R0 \
+	ADCS  R0, R9, R9   \
+	ADC   R10, ZR, R10 \
+	UMULH R13, R12, R0 \
+	ADDS  R0, R7, R6   \
+	UMULH R14, R12, R0 \
+	ADCS  R0, R8, R7   \
+	UMULH R15, R12, R0 \
+	ADCS  R0, R9, R8   \
+	UMULH R16, R12, R0 \
+	ADCS  R0, R10, R9  \
+
+#define MUL_WORD_N() \
+	MUL   R2, R1, R0   \
+	ADDS  R0, R6, R6   \
+	MUL   R6, R11, R12 \
+	MUL   R3, R1, R0   \
+	ADCS  R0, R7, R7   \
+	MUL   R4, R1, R0   \
+	ADCS  R0, R8, R8   \
+	MUL   R5, R1, R0   \
+	ADCS  R0, R9, R9   \
+	ADC   ZR, ZR, R10  \
+	UMULH R2, R1, R0   \
+	ADDS  R0, R7, R7   \
+	UMULH R3, R1, R0   \
+	ADCS  R0, R8, R8   \
+	UMULH R4, R1, R0   \
+	ADCS  R0, R9, R9   \
+	UMULH R5, R1, R0   \
+	ADC   R0, R10, R10 \
+	DIVSHIFT()         \
+
+#define MUL_WORD_0() \
+	MUL   R2, R1, R6   \
+	MUL   R3, R1, R7   \
+	MUL   R4, R1, R8   \
+	MUL   R5, R1, R9   \
+	UMULH R2, R1, R0   \
+	ADDS  R0, R7, R7   \
+	UMULH R3, R1, R0   \
+	ADCS  R0, R8, R8   \
+	UMULH R4, R1, R0   \
+	ADCS  R0, R9, R9   \
+	UMULH R5, R1, R0   \
+	ADC   R0, ZR, R10  \
+	MUL   R6, R11, R12 \
+	DIVSHIFT()         \
+
+	MOVD y+16(FP), R17
+	MOVD x+8(FP), R0
+	LDP  0(R0), (R2, R3)
+	LDP  16(R0), (R4, R5)
+	MOVD 0(R17), R1
+	MOVD $const_qInvNeg, R11
+	LDP  ·qElement+0(SB), (R13, R14)
+	LDP  ·qElement+16(SB), (R15, R16)
+	MUL_WORD_0()
+	MOVD 8(R17), R1
+	MUL_WORD_N()
+	MOVD 16(R17), R1
+	MUL_WORD_N()
+	MOVD 24(R17), R1
+	MUL_WORD_N()
+
+	// reduce if necessary
+	SUBS R13, R6, R13
+	SBCS R14, R7, R14
+	SBCS R15, R8, R15
+	SBCS R16, R9, R16
+	MOVD res+0(FP), R0
+	CSEL CS, R13, R6, R6
+	CSEL CS, R14, R7, R7
+	STP  (R6, R7), 0(R0)
+	CSEL CS, R15, R8, R8
+	CSEL CS, R16, R9, R9
+	STP  (R8, R9), 16(R0)
+	RET
+
+// reduce(res *Element)
+TEXT ·reduce(SB), NOFRAME|NOSPLIT, $0-8
+	LDP  ·qElement+0(SB), (R4, R5)
+	LDP  ·qElement+16(SB), (R6, R7)
+	MOVD res+0(FP), R8
+	LDP  0(R8), (R0, R1)
+	LDP  16(R8), (R2, R3)
+
+	// q = t - q
+	SUBS R4, R0, R4
+	SBCS R5, R1, R5
+	SBCS R6, R2, R6
+	SBCS R7, R3, R7
+
+	// if no borrow, return q, else return t
+	CSEL CS, R4, R0, R0
+	CSEL CS, R5, R1, R1
+	STP  (R0, R1), 0(R8)
+	CSEL CS, R6, R2, R2
+	CSEL CS, R7, R3, R3
+	STP  (R2, R3), 16(R8)
+	RET
diff --git a/field/asm/element_5w/BUILD.bazel b/field/asm/element_5w/BUILD.bazel
index 3a7884134a..efb69e8ef2 100644
--- a/field/asm/element_5w/BUILD.bazel
+++ b/field/asm/element_5w/BUILD.bazel
@@ -15,3 +15,8 @@ alias(
     actual = ":element_5w",
     visibility = ["//visibility:public"],
 )
+
+exports_files([
+    "element_5w_amd64.h",
+])
+
diff --git a/tmp/gnark-crypto-new/field/asm/element_5w/element_5w_amd64.h b/field/asm/element_5w/element_5w_amd64.h
new file mode 100644
index 0000000000..53a8c76771
--- /dev/null
+++ b/field/asm/element_5w/element_5w_amd64.h
@@ -0,0 +1,563 @@
+// Code generated by gnark-crypto/generator. DO NOT EDIT.
+#include "textflag.h"
+#include "funcdata.h"
+#include "go_asm.h"
+
+#define REDUCE(ra0, ra1, ra2, ra3, ra4, rb0, rb1, rb2, rb3, rb4) \
+	MOVQ    ra0, rb0;              \
+	SUBQ    ·qElement(SB), ra0;    \
+	MOVQ    ra1, rb1;              \
+	SBBQ    ·qElement+8(SB), ra1;  \
+	MOVQ    ra2, rb2;              \
+	SBBQ    ·qElement+16(SB), ra2; \
+	MOVQ    ra3, rb3;              \
+	SBBQ    ·qElement+24(SB), ra3; \
+	MOVQ    ra4, rb4;              \
+	SBBQ    ·qElement+32(SB), ra4; \
+	CMOVQCS rb0, ra0;              \
+	CMOVQCS rb1, ra1;              \
+	CMOVQCS rb2, ra2;              \
+	CMOVQCS rb3, ra3;              \
+	CMOVQCS rb4, ra4;              \
+
+TEXT ·reduce(SB), NOSPLIT, $0-8
+	MOVQ res+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+
+	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	RET
+
+// MulBy3(x *Element)
+TEXT ·MulBy3(SB), NOSPLIT, $0-8
+	MOVQ x+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+
+	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
+
+	ADDQ 0(AX), DX
+	ADCQ 8(AX), CX
+	ADCQ 16(AX), BX
+	ADCQ 24(AX), SI
+	ADCQ 32(AX), DI
+
+	// reduce element(DX,CX,BX,SI,DI) using temp registers (R13,R14,R15,R8,R9)
+	REDUCE(DX,CX,BX,SI,DI,R13,R14,R15,R8,R9)
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	RET
+
+// MulBy5(x *Element)
+TEXT ·MulBy5(SB), NOSPLIT, $0-8
+	MOVQ x+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+
+	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
+
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+
+	// reduce element(DX,CX,BX,SI,DI) using temp registers (R13,R14,R15,R8,R9)
+	REDUCE(DX,CX,BX,SI,DI,R13,R14,R15,R8,R9)
+
+	ADDQ 0(AX), DX
+	ADCQ 8(AX), CX
+	ADCQ 16(AX), BX
+	ADCQ 24(AX), SI
+	ADCQ 32(AX), DI
+
+	// reduce element(DX,CX,BX,SI,DI) using temp registers (R10,R11,R12,R13,R14)
+	REDUCE(DX,CX,BX,SI,DI,R10,R11,R12,R13,R14)
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	RET
+
+// MulBy13(x *Element)
+TEXT ·MulBy13(SB), $16-8
+	MOVQ x+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+
+	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
+
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+
+	// reduce element(DX,CX,BX,SI,DI) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP))
+	REDUCE(DX,CX,BX,SI,DI,R13,R14,R15,s0-8(SP),s1-16(SP))
+
+	MOVQ DX, R13
+	MOVQ CX, R14
+	MOVQ BX, R15
+	MOVQ SI, s0-8(SP)
+	MOVQ DI, s1-16(SP)
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+
+	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
+
+	ADDQ R13, DX
+	ADCQ R14, CX
+	ADCQ R15, BX
+	ADCQ s0-8(SP), SI
+	ADCQ s1-16(SP), DI
+
+	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
+
+	ADDQ 0(AX), DX
+	ADCQ 8(AX), CX
+	ADCQ 16(AX), BX
+	ADCQ 24(AX), SI
+	ADCQ 32(AX), DI
+
+	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	RET
+
+// Butterfly(a, b *Element) sets a = a + b; b = a - b
+TEXT ·Butterfly(SB), $24-16
+	MOVQ    a+0(FP), AX
+	MOVQ    0(AX), CX
+	MOVQ    8(AX), BX
+	MOVQ    16(AX), SI
+	MOVQ    24(AX), DI
+	MOVQ    32(AX), R8
+	MOVQ    CX, R9
+	MOVQ    BX, R10
+	MOVQ    SI, R11
+	MOVQ    DI, R12
+	MOVQ    R8, R13
+	XORQ    AX, AX
+	MOVQ    b+8(FP), DX
+	ADDQ    0(DX), CX
+	ADCQ    8(DX), BX
+	ADCQ    16(DX), SI
+	ADCQ    24(DX), DI
+	ADCQ    32(DX), R8
+	SUBQ    0(DX), R9
+	SBBQ    8(DX), R10
+	SBBQ    16(DX), R11
+	SBBQ    24(DX), R12
+	SBBQ    32(DX), R13
+	MOVQ    CX, R14
+	MOVQ    BX, R15
+	MOVQ    SI, s0-8(SP)
+	MOVQ    DI, s1-16(SP)
+	MOVQ    R8, s2-24(SP)
+	MOVQ    $const_q0, CX
+	MOVQ    $const_q1, BX
+	MOVQ    $const_q2, SI
+	MOVQ    $const_q3, DI
+	MOVQ    $const_q4, R8
+	CMOVQCC AX, CX
+	CMOVQCC AX, BX
+	CMOVQCC AX, SI
+	CMOVQCC AX, DI
+	CMOVQCC AX, R8
+	ADDQ    CX, R9
+	ADCQ    BX, R10
+	ADCQ    SI, R11
+	ADCQ    DI, R12
+	ADCQ    R8, R13
+	MOVQ    R14, CX
+	MOVQ    R15, BX
+	MOVQ    s0-8(SP), SI
+	MOVQ    s1-16(SP), DI
+	MOVQ    s2-24(SP), R8
+	MOVQ    R9, 0(DX)
+	MOVQ    R10, 8(DX)
+	MOVQ    R11, 16(DX)
+	MOVQ    R12, 24(DX)
+	MOVQ    R13, 32(DX)
+
+	// reduce element(CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13)
+	REDUCE(CX,BX,SI,DI,R8,R9,R10,R11,R12,R13)
+
+	MOVQ a+0(FP), AX
+	MOVQ CX, 0(AX)
+	MOVQ BX, 8(AX)
+	MOVQ SI, 16(AX)
+	MOVQ DI, 24(AX)
+	MOVQ R8, 32(AX)
+	RET
+
+// mul(res, x, y *Element)
+TEXT ·mul(SB), $24-24
+
+	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
+	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
+	// See github.com/Consensys/gnark-crypto/field/generator for more comments.
+
+	NO_LOCAL_POINTERS
+	CMPB ·supportAdx(SB), $1
+	JNE  noAdx_1
+	MOVQ x+8(FP), DI
+
+	// x[0] -> R9
+	// x[1] -> R10
+	// x[2] -> R11
+	MOVQ 0(DI), R9
+	MOVQ 8(DI), R10
+	MOVQ 16(DI), R11
+	MOVQ y+16(FP), R12
+
+	// A -> BP
+	// t[0] -> R14
+	// t[1] -> R13
+	// t[2] -> CX
+	// t[3] -> BX
+	// t[4] -> SI
+#define MACC(in0, in1, in2) \
+	ADCXQ in0, in1     \
+	MULXQ in2, AX, in0 \
+	ADOXQ AX, in1      \
+
+#define DIV_SHIFT() \
+	MOVQ  $const_qInvNeg, DX        \
+	IMULQ R14, DX                   \
+	XORQ  AX, AX                    \
+	MULXQ ·qElement+0(SB), AX, R8   \
+	ADCXQ R14, AX                   \
+	MOVQ  R8, R14                   \
+	MACC(R13, R14, ·qElement+8(SB)) \
+	MACC(CX, R13, ·qElement+16(SB)) \
+	MACC(BX, CX, ·qElement+24(SB))  \
+	MACC(SI, BX, ·qElement+32(SB))  \
+	MOVQ  $0, AX                    \
+	ADCXQ AX, SI                    \
+	ADOXQ BP, SI                    \
+
+#define MUL_WORD_0() \
+	XORQ  AX, AX         \
+	MULXQ R9, R14, R13   \
+	MULXQ R10, AX, CX    \
+	ADOXQ AX, R13        \
+	MULXQ R11, AX, BX    \
+	ADOXQ AX, CX         \
+	MULXQ 24(DI), AX, SI \
+	ADOXQ AX, BX         \
+	MULXQ 32(DI), AX, BP \
+	ADOXQ AX, SI         \
+	MOVQ  $0, AX         \
+	ADOXQ AX, BP         \
+	DIV_SHIFT()          \
+
+#define MUL_WORD_N() \
+	XORQ  AX, AX         \
+	MULXQ R9, AX, BP     \
+	ADOXQ AX, R14        \
+	MACC(BP, R13, R10)   \
+	MACC(BP, CX, R11)    \
+	MACC(BP, BX, 24(DI)) \
+	MACC(BP, SI, 32(DI)) \
+	MOVQ  $0, AX         \
+	ADCXQ AX, BP         \
+	ADOXQ AX, BP         \
+	DIV_SHIFT()          \
+
+	// mul body
+	MOVQ 0(R12), DX
+	MUL_WORD_0()
+	MOVQ 8(R12), DX
+	MUL_WORD_N()
+	MOVQ 16(R12), DX
+	MUL_WORD_N()
+	MOVQ 24(R12), DX
+	MUL_WORD_N()
+	MOVQ 32(R12), DX
+	MUL_WORD_N()
+
+	// reduce element(R14,R13,CX,BX,SI) using temp registers (R8,DI,R12,R9,R10)
+	REDUCE(R14,R13,CX,BX,SI,R8,DI,R12,R9,R10)
+
+	MOVQ res+0(FP), AX
+	MOVQ R14, 0(AX)
+	MOVQ R13, 8(AX)
+	MOVQ CX, 16(AX)
+	MOVQ BX, 24(AX)
+	MOVQ SI, 32(AX)
+	RET
+
+noAdx_1:
+	MOVQ res+0(FP), AX
+	MOVQ AX, (SP)
+	MOVQ x+8(FP), AX
+	MOVQ AX, 8(SP)
+	MOVQ y+16(FP), AX
+	MOVQ AX, 16(SP)
+	CALL ·_mulGeneric(SB)
+	RET
+
+TEXT ·fromMont(SB), $8-8
+	NO_LOCAL_POINTERS
+
+	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
+	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
+	// when y = 1 we have:
+	// for i=0 to N-1
+	// 		t[i] = x[i]
+	// for i=0 to N-1
+	// 		m := t[0]*q'[0] mod W
+	// 		C,_ := t[0] + m*q[0]
+	// 		for j=1 to N-1
+	// 		    (C,t[j-1]) := t[j] + m*q[j] + C
+	// 		t[N-1] = C
+	CMPB ·supportAdx(SB), $1
+	JNE  noAdx_2
+	MOVQ res+0(FP), DX
+	MOVQ 0(DX), R14
+	MOVQ 8(DX), R13
+	MOVQ 16(DX), CX
+	MOVQ 24(DX), BX
+	MOVQ 32(DX), SI
+	XORQ DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R13, R14
+	MULXQ ·qElement+8(SB), AX, R13
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R13
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R13
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+	MOVQ  $0, AX
+	ADCXQ AX, SI
+	ADOXQ AX, SI
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R13, R14
+	MULXQ ·qElement+8(SB), AX, R13
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R13
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R13
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+	MOVQ  $0, AX
+	ADCXQ AX, SI
+	ADOXQ AX, SI
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R13, R14
+	MULXQ ·qElement+8(SB), AX, R13
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R13
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R13
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+	MOVQ  $0, AX
+	ADCXQ AX, SI
+	ADOXQ AX, SI
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R13, R14
+	MULXQ ·qElement+8(SB), AX, R13
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R13
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R13
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+	MOVQ  $0, AX
+	ADCXQ AX, SI
+	ADOXQ AX, SI
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R13, R14
+	MULXQ ·qElement+8(SB), AX, R13
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R13
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R13
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+	MOVQ  $0, AX
+	ADCXQ AX, SI
+	ADOXQ AX, SI
+
+	// reduce element(R14,R13,CX,BX,SI) using temp registers (DI,R8,R9,R10,R11)
+	REDUCE(R14,R13,CX,BX,SI,DI,R8,R9,R10,R11)
+
+	MOVQ res+0(FP), AX
+	MOVQ R14, 0(AX)
+	MOVQ R13, 8(AX)
+	MOVQ CX, 16(AX)
+	MOVQ BX, 24(AX)
+	MOVQ SI, 32(AX)
+	RET
+
+noAdx_2:
+	MOVQ res+0(FP), AX
+	MOVQ AX, (SP)
+	CALL ·_fromMontGeneric(SB)
+	RET
diff --git a/field/asm/element_6w/BUILD.bazel b/field/asm/element_6w/BUILD.bazel
index 78d607a6f8..fe413b3326 100644
--- a/field/asm/element_6w/BUILD.bazel
+++ b/field/asm/element_6w/BUILD.bazel
@@ -16,3 +16,9 @@ alias(
     actual = ":element_6w",
     visibility = ["//visibility:public"],
 )
+
+exports_files([
+    "element_6w_amd64.h",
+    "element_6w_arm64.h",
+])
+
diff --git a/tmp/gnark-crypto-new/field/asm/element_6w/element_6w_amd64.h b/field/asm/element_6w/element_6w_amd64.h
new file mode 100644
index 0000000000..1b4f66c04f
--- /dev/null
+++ b/field/asm/element_6w/element_6w_amd64.h
@@ -0,0 +1,670 @@
+// Code generated by gnark-crypto/generator. DO NOT EDIT.
+#include "textflag.h"
+#include "funcdata.h"
+#include "go_asm.h"
+
+#define REDUCE(ra0, ra1, ra2, ra3, ra4, ra5, rb0, rb1, rb2, rb3, rb4, rb5) \
+	MOVQ    ra0, rb0;              \
+	SUBQ    ·qElement(SB), ra0;    \
+	MOVQ    ra1, rb1;              \
+	SBBQ    ·qElement+8(SB), ra1;  \
+	MOVQ    ra2, rb2;              \
+	SBBQ    ·qElement+16(SB), ra2; \
+	MOVQ    ra3, rb3;              \
+	SBBQ    ·qElement+24(SB), ra3; \
+	MOVQ    ra4, rb4;              \
+	SBBQ    ·qElement+32(SB), ra4; \
+	MOVQ    ra5, rb5;              \
+	SBBQ    ·qElement+40(SB), ra5; \
+	CMOVQCS rb0, ra0;              \
+	CMOVQCS rb1, ra1;              \
+	CMOVQCS rb2, ra2;              \
+	CMOVQCS rb3, ra3;              \
+	CMOVQCS rb4, ra4;              \
+	CMOVQCS rb5, ra5;              \
+
+TEXT ·reduce(SB), NOSPLIT, $0-8
+	MOVQ res+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	MOVQ 40(AX), R8
+
+	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13,R14)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14)
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	MOVQ R8, 40(AX)
+	RET
+
+// MulBy3(x *Element)
+TEXT ·MulBy3(SB), NOSPLIT, $0-8
+	MOVQ x+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	MOVQ 40(AX), R8
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+
+	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13,R14)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14)
+
+	ADDQ 0(AX), DX
+	ADCQ 8(AX), CX
+	ADCQ 16(AX), BX
+	ADCQ 24(AX), SI
+	ADCQ 32(AX), DI
+	ADCQ 40(AX), R8
+
+	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R15,R9,R10,R11,R12,R13)
+	REDUCE(DX,CX,BX,SI,DI,R8,R15,R9,R10,R11,R12,R13)
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	MOVQ R8, 40(AX)
+	RET
+
+// MulBy5(x *Element)
+TEXT ·MulBy5(SB), NOSPLIT, $0-8
+	MOVQ x+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	MOVQ 40(AX), R8
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+
+	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13,R14)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14)
+
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+
+	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R15,R9,R10,R11,R12,R13)
+	REDUCE(DX,CX,BX,SI,DI,R8,R15,R9,R10,R11,R12,R13)
+
+	ADDQ 0(AX), DX
+	ADCQ 8(AX), CX
+	ADCQ 16(AX), BX
+	ADCQ 24(AX), SI
+	ADCQ 32(AX), DI
+	ADCQ 40(AX), R8
+
+	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R14,R15,R9,R10,R11,R12)
+	REDUCE(DX,CX,BX,SI,DI,R8,R14,R15,R9,R10,R11,R12)
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	MOVQ R8, 40(AX)
+	RET
+
+// MulBy13(x *Element)
+TEXT ·MulBy13(SB), $40-8
+	MOVQ x+0(FP), AX
+	MOVQ 0(AX), DX
+	MOVQ 8(AX), CX
+	MOVQ 16(AX), BX
+	MOVQ 24(AX), SI
+	MOVQ 32(AX), DI
+	MOVQ 40(AX), R8
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+
+	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13,R14)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14)
+
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+
+	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP))
+	REDUCE(DX,CX,BX,SI,DI,R8,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP))
+
+	MOVQ DX, R15
+	MOVQ CX, s0-8(SP)
+	MOVQ BX, s1-16(SP)
+	MOVQ SI, s2-24(SP)
+	MOVQ DI, s3-32(SP)
+	MOVQ R8, s4-40(SP)
+	ADDQ DX, DX
+	ADCQ CX, CX
+	ADCQ BX, BX
+	ADCQ SI, SI
+	ADCQ DI, DI
+	ADCQ R8, R8
+
+	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13,R14)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14)
+
+	ADDQ R15, DX
+	ADCQ s0-8(SP), CX
+	ADCQ s1-16(SP), BX
+	ADCQ s2-24(SP), SI
+	ADCQ s3-32(SP), DI
+	ADCQ s4-40(SP), R8
+
+	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13,R14)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14)
+
+	ADDQ 0(AX), DX
+	ADCQ 8(AX), CX
+	ADCQ 16(AX), BX
+	ADCQ 24(AX), SI
+	ADCQ 32(AX), DI
+	ADCQ 40(AX), R8
+
+	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13,R14)
+	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14)
+
+	MOVQ DX, 0(AX)
+	MOVQ CX, 8(AX)
+	MOVQ BX, 16(AX)
+	MOVQ SI, 24(AX)
+	MOVQ DI, 32(AX)
+	MOVQ R8, 40(AX)
+	RET
+
+// Butterfly(a, b *Element) sets a = a + b; b = a - b
+TEXT ·Butterfly(SB), $48-16
+	MOVQ    a+0(FP), AX
+	MOVQ    0(AX), CX
+	MOVQ    8(AX), BX
+	MOVQ    16(AX), SI
+	MOVQ    24(AX), DI
+	MOVQ    32(AX), R8
+	MOVQ    40(AX), R9
+	MOVQ    CX, R10
+	MOVQ    BX, R11
+	MOVQ    SI, R12
+	MOVQ    DI, R13
+	MOVQ    R8, R14
+	MOVQ    R9, R15
+	XORQ    AX, AX
+	MOVQ    b+8(FP), DX
+	ADDQ    0(DX), CX
+	ADCQ    8(DX), BX
+	ADCQ    16(DX), SI
+	ADCQ    24(DX), DI
+	ADCQ    32(DX), R8
+	ADCQ    40(DX), R9
+	SUBQ    0(DX), R10
+	SBBQ    8(DX), R11
+	SBBQ    16(DX), R12
+	SBBQ    24(DX), R13
+	SBBQ    32(DX), R14
+	SBBQ    40(DX), R15
+	MOVQ    CX, s0-8(SP)
+	MOVQ    BX, s1-16(SP)
+	MOVQ    SI, s2-24(SP)
+	MOVQ    DI, s3-32(SP)
+	MOVQ    R8, s4-40(SP)
+	MOVQ    R9, s5-48(SP)
+	MOVQ    $const_q0, CX
+	MOVQ    $const_q1, BX
+	MOVQ    $const_q2, SI
+	MOVQ    $const_q3, DI
+	MOVQ    $const_q4, R8
+	MOVQ    $const_q5, R9
+	CMOVQCC AX, CX
+	CMOVQCC AX, BX
+	CMOVQCC AX, SI
+	CMOVQCC AX, DI
+	CMOVQCC AX, R8
+	CMOVQCC AX, R9
+	ADDQ    CX, R10
+	ADCQ    BX, R11
+	ADCQ    SI, R12
+	ADCQ    DI, R13
+	ADCQ    R8, R14
+	ADCQ    R9, R15
+	MOVQ    s0-8(SP), CX
+	MOVQ    s1-16(SP), BX
+	MOVQ    s2-24(SP), SI
+	MOVQ    s3-32(SP), DI
+	MOVQ    s4-40(SP), R8
+	MOVQ    s5-48(SP), R9
+	MOVQ    R10, 0(DX)
+	MOVQ    R11, 8(DX)
+	MOVQ    R12, 16(DX)
+	MOVQ    R13, 24(DX)
+	MOVQ    R14, 32(DX)
+	MOVQ    R15, 40(DX)
+
+	// reduce element(CX,BX,SI,DI,R8,R9) using temp registers (R10,R11,R12,R13,R14,R15)
+	REDUCE(CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15)
+
+	MOVQ a+0(FP), AX
+	MOVQ CX, 0(AX)
+	MOVQ BX, 8(AX)
+	MOVQ SI, 16(AX)
+	MOVQ DI, 24(AX)
+	MOVQ R8, 32(AX)
+	MOVQ R9, 40(AX)
+	RET
+
+// mul(res, x, y *Element)
+TEXT ·mul(SB), $24-24
+
+	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
+	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
+	// See github.com/Consensys/gnark-crypto/field/generator for more comments.
+
+	NO_LOCAL_POINTERS
+	CMPB ·supportAdx(SB), $1
+	JNE  noAdx_1
+	MOVQ x+8(FP), R8
+
+	// x[0] -> R10
+	// x[1] -> R11
+	// x[2] -> R12
+	MOVQ 0(R8), R10
+	MOVQ 8(R8), R11
+	MOVQ 16(R8), R12
+	MOVQ y+16(FP), R13
+
+	// A -> BP
+	// t[0] -> R14
+	// t[1] -> R15
+	// t[2] -> CX
+	// t[3] -> BX
+	// t[4] -> SI
+	// t[5] -> DI
+#define MACC(in0, in1, in2) \
+	ADCXQ in0, in1     \
+	MULXQ in2, AX, in0 \
+	ADOXQ AX, in1      \
+
+#define DIV_SHIFT() \
+	MOVQ  $const_qInvNeg, DX        \
+	IMULQ R14, DX                   \
+	XORQ  AX, AX                    \
+	MULXQ ·qElement+0(SB), AX, R9   \
+	ADCXQ R14, AX                   \
+	MOVQ  R9, R14                   \
+	MACC(R15, R14, ·qElement+8(SB)) \
+	MACC(CX, R15, ·qElement+16(SB)) \
+	MACC(BX, CX, ·qElement+24(SB))  \
+	MACC(SI, BX, ·qElement+32(SB))  \
+	MACC(DI, SI, ·qElement+40(SB))  \
+	MOVQ  $0, AX                    \
+	ADCXQ AX, DI                    \
+	ADOXQ BP, DI                    \
+
+#define MUL_WORD_0() \
+	XORQ  AX, AX         \
+	MULXQ R10, R14, R15  \
+	MULXQ R11, AX, CX    \
+	ADOXQ AX, R15        \
+	MULXQ R12, AX, BX    \
+	ADOXQ AX, CX         \
+	MULXQ 24(R8), AX, SI \
+	ADOXQ AX, BX         \
+	MULXQ 32(R8), AX, DI \
+	ADOXQ AX, SI         \
+	MULXQ 40(R8), AX, BP \
+	ADOXQ AX, DI         \
+	MOVQ  $0, AX         \
+	ADOXQ AX, BP         \
+	DIV_SHIFT()          \
+
+#define MUL_WORD_N() \
+	XORQ  AX, AX         \
+	MULXQ R10, AX, BP    \
+	ADOXQ AX, R14        \
+	MACC(BP, R15, R11)   \
+	MACC(BP, CX, R12)    \
+	MACC(BP, BX, 24(R8)) \
+	MACC(BP, SI, 32(R8)) \
+	MACC(BP, DI, 40(R8)) \
+	MOVQ  $0, AX         \
+	ADCXQ AX, BP         \
+	ADOXQ AX, BP         \
+	DIV_SHIFT()          \
+
+	// mul body
+	MOVQ 0(R13), DX
+	MUL_WORD_0()
+	MOVQ 8(R13), DX
+	MUL_WORD_N()
+	MOVQ 16(R13), DX
+	MUL_WORD_N()
+	MOVQ 24(R13), DX
+	MUL_WORD_N()
+	MOVQ 32(R13), DX
+	MUL_WORD_N()
+	MOVQ 40(R13), DX
+	MUL_WORD_N()
+
+	// reduce element(R14,R15,CX,BX,SI,DI) using temp registers (R9,R8,R13,R10,R11,R12)
+	REDUCE(R14,R15,CX,BX,SI,DI,R9,R8,R13,R10,R11,R12)
+
+	MOVQ res+0(FP), AX
+	MOVQ R14, 0(AX)
+	MOVQ R15, 8(AX)
+	MOVQ CX, 16(AX)
+	MOVQ BX, 24(AX)
+	MOVQ SI, 32(AX)
+	MOVQ DI, 40(AX)
+	RET
+
+noAdx_1:
+	MOVQ res+0(FP), AX
+	MOVQ AX, (SP)
+	MOVQ x+8(FP), AX
+	MOVQ AX, 8(SP)
+	MOVQ y+16(FP), AX
+	MOVQ AX, 16(SP)
+	CALL ·_mulGeneric(SB)
+	RET
+
+TEXT ·fromMont(SB), $8-8
+	NO_LOCAL_POINTERS
+
+	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
+	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
+	// when y = 1 we have:
+	// for i=0 to N-1
+	// 		t[i] = x[i]
+	// for i=0 to N-1
+	// 		m := t[0]*q'[0] mod W
+	// 		C,_ := t[0] + m*q[0]
+	// 		for j=1 to N-1
+	// 		    (C,t[j-1]) := t[j] + m*q[j] + C
+	// 		t[N-1] = C
+	CMPB ·supportAdx(SB), $1
+	JNE  noAdx_2
+	MOVQ res+0(FP), DX
+	MOVQ 0(DX), R14
+	MOVQ 8(DX), R15
+	MOVQ 16(DX), CX
+	MOVQ 24(DX), BX
+	MOVQ 32(DX), SI
+	MOVQ 40(DX), DI
+	XORQ DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+	MOVQ  $0, AX
+	ADCXQ AX, DI
+	ADOXQ AX, DI
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+	MOVQ  $0, AX
+	ADCXQ AX, DI
+	ADOXQ AX, DI
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+	MOVQ  $0, AX
+	ADCXQ AX, DI
+	ADOXQ AX, DI
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+	MOVQ  $0, AX
+	ADCXQ AX, DI
+	ADOXQ AX, DI
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+	MOVQ  $0, AX
+	ADCXQ AX, DI
+	ADOXQ AX, DI
+	XORQ  DX, DX
+
+	// m := t[0]*q'[0] mod W
+	MOVQ  $const_qInvNeg, DX
+	IMULQ R14, DX
+	XORQ  AX, AX
+
+	// C,_ := t[0] + m*q[0]
+	MULXQ ·qElement+0(SB), AX, BP
+	ADCXQ R14, AX
+	MOVQ  BP, R14
+
+	// (C,t[0]) := t[1] + m*q[1] + C
+	ADCXQ R15, R14
+	MULXQ ·qElement+8(SB), AX, R15
+	ADOXQ AX, R14
+
+	// (C,t[1]) := t[2] + m*q[2] + C
+	ADCXQ CX, R15
+	MULXQ ·qElement+16(SB), AX, CX
+	ADOXQ AX, R15
+
+	// (C,t[2]) := t[3] + m*q[3] + C
+	ADCXQ BX, CX
+	MULXQ ·qElement+24(SB), AX, BX
+	ADOXQ AX, CX
+
+	// (C,t[3]) := t[4] + m*q[4] + C
+	ADCXQ SI, BX
+	MULXQ ·qElement+32(SB), AX, SI
+	ADOXQ AX, BX
+
+	// (C,t[4]) := t[5] + m*q[5] + C
+	ADCXQ DI, SI
+	MULXQ ·qElement+40(SB), AX, DI
+	ADOXQ AX, SI
+	MOVQ  $0, AX
+	ADCXQ AX, DI
+	ADOXQ AX, DI
+
+	// reduce element(R14,R15,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12,R13)
+	REDUCE(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13)
+
+	MOVQ res+0(FP), AX
+	MOVQ R14, 0(AX)
+	MOVQ R15, 8(AX)
+	MOVQ CX, 16(AX)
+	MOVQ BX, 24(AX)
+	MOVQ SI, 32(AX)
+	MOVQ DI, 40(AX)
+	RET
+
+noAdx_2:
+	MOVQ res+0(FP), AX
+	MOVQ AX, (SP)
+	CALL ·_fromMontGeneric(SB)
+	RET
diff --git a/tmp/gnark-crypto-new/field/asm/element_6w/element_6w_arm64.h b/field/asm/element_6w/element_6w_arm64.h
new file mode 100644
index 0000000000..7dbd7ecaf3
--- /dev/null
+++ b/field/asm/element_6w/element_6w_arm64.h
@@ -0,0 +1,220 @@
+// Code generated by gnark-crypto/generator. DO NOT EDIT.
+#include "textflag.h"
+#include "funcdata.h"
+#include "go_asm.h"
+
+// butterfly(a, b *Element)
+// a, b = a+b, a-b
+TEXT ·Butterfly(SB), NOFRAME|NOSPLIT, $0-16
+	LDP  x+0(FP), (R25, R26)
+	LDP  0(R25), (R0, R1)
+	LDP  16(R25), (R2, R3)
+	LDP  32(R25), (R4, R5)
+	LDP  0(R26), (R6, R7)
+	LDP  16(R26), (R8, R9)
+	LDP  32(R26), (R10, R11)
+	ADDS R0, R6, R12
+	ADCS R1, R7, R13
+	ADCS R2, R8, R14
+	ADCS R3, R9, R15
+	ADCS R4, R10, R16
+	ADC  R5, R11, R17
+	SUBS R6, R0, R6
+	SBCS R7, R1, R7
+	SBCS R8, R2, R8
+	SBCS R9, R3, R9
+	SBCS R10, R4, R10
+	SBCS R11, R5, R11
+	LDP  ·qElement+0(SB), (R0, R1)
+	CSEL CS, ZR, R0, R19
+	CSEL CS, ZR, R1, R20
+	LDP  ·qElement+16(SB), (R2, R3)
+	CSEL CS, ZR, R2, R21
+	CSEL CS, ZR, R3, R22
+	LDP  ·qElement+32(SB), (R4, R5)
+	CSEL CS, ZR, R4, R23
+	CSEL CS, ZR, R5, R24
+
+	// add q if underflow, 0 if not
+	ADDS R6, R19, R6
+	ADCS R7, R20, R7
+	STP  (R6, R7), 0(R26)
+	ADCS R8, R21, R8
+	ADCS R9, R22, R9
+	STP  (R8, R9), 16(R26)
+	ADCS R10, R23, R10
+	ADC  R11, R24, R11
+	STP  (R10, R11), 32(R26)
+
+	// q = t - q
+	SUBS R0, R12, R0
+	SBCS R1, R13, R1
+	SBCS R2, R14, R2
+	SBCS R3, R15, R3
+	SBCS R4, R16, R4
+	SBCS R5, R17, R5
+
+	// if no borrow, return q, else return t
+	CSEL CS, R0, R12, R12
+	CSEL CS, R1, R13, R13
+	STP  (R12, R13), 0(R25)
+	CSEL CS, R2, R14, R14
+	CSEL CS, R3, R15, R15
+	STP  (R14, R15), 16(R25)
+	CSEL CS, R4, R16, R16
+	CSEL CS, R5, R17, R17
+	STP  (R16, R17), 32(R25)
+	RET
+
+// mul(res, x, y *Element)
+// Algorithm 2 of Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS
+// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
+TEXT ·mul(SB), NOFRAME|NOSPLIT, $0-24
+#define DIVSHIFT() \
+	MUL   R17, R16, R0 \
+	ADDS  R0, R8, R8   \
+	MUL   R19, R16, R0 \
+	ADCS  R0, R9, R9   \
+	MUL   R20, R16, R0 \
+	ADCS  R0, R10, R10 \
+	MUL   R21, R16, R0 \
+	ADCS  R0, R11, R11 \
+	MUL   R22, R16, R0 \
+	ADCS  R0, R12, R12 \
+	MUL   R23, R16, R0 \
+	ADCS  R0, R13, R13 \
+	ADC   R14, ZR, R14 \
+	UMULH R17, R16, R0 \
+	ADDS  R0, R9, R8   \
+	UMULH R19, R16, R0 \
+	ADCS  R0, R10, R9  \
+	UMULH R20, R16, R0 \
+	ADCS  R0, R11, R10 \
+	UMULH R21, R16, R0 \
+	ADCS  R0, R12, R11 \
+	UMULH R22, R16, R0 \
+	ADCS  R0, R13, R12 \
+	UMULH R23, R16, R0 \
+	ADCS  R0, R14, R13 \
+
+#define MUL_WORD_N() \
+	MUL   R2, R1, R0   \
+	ADDS  R0, R8, R8   \
+	MUL   R8, R15, R16 \
+	MUL   R3, R1, R0   \
+	ADCS  R0, R9, R9   \
+	MUL   R4, R1, R0   \
+	ADCS  R0, R10, R10 \
+	MUL   R5, R1, R0   \
+	ADCS  R0, R11, R11 \
+	MUL   R6, R1, R0   \
+	ADCS  R0, R12, R12 \
+	MUL   R7, R1, R0   \
+	ADCS  R0, R13, R13 \
+	ADC   ZR, ZR, R14  \
+	UMULH R2, R1, R0   \
+	ADDS  R0, R9, R9   \
+	UMULH R3, R1, R0   \
+	ADCS  R0, R10, R10 \
+	UMULH R4, R1, R0   \
+	ADCS  R0, R11, R11 \
+	UMULH R5, R1, R0   \
+	ADCS  R0, R12, R12 \
+	UMULH R6, R1, R0   \
+	ADCS  R0, R13, R13 \
+	UMULH R7, R1, R0   \
+	ADC   R0, R14, R14 \
+	DIVSHIFT()         \
+
+#define MUL_WORD_0() \
+	MUL   R2, R1, R8   \
+	MUL   R3, R1, R9   \
+	MUL   R4, R1, R10  \
+	MUL   R5, R1, R11  \
+	MUL   R6, R1, R12  \
+	MUL   R7, R1, R13  \
+	UMULH R2, R1, R0   \
+	ADDS  R0, R9, R9   \
+	UMULH R3, R1, R0   \
+	ADCS  R0, R10, R10 \
+	UMULH R4, R1, R0   \
+	ADCS  R0, R11, R11 \
+	UMULH R5, R1, R0   \
+	ADCS  R0, R12, R12 \
+	UMULH R6, R1, R0   \
+	ADCS  R0, R13, R13 \
+	UMULH R7, R1, R0   \
+	ADC   R0, ZR, R14  \
+	MUL   R8, R15, R16 \
+	DIVSHIFT()         \
+
+	MOVD y+16(FP), R24
+	MOVD x+8(FP), R0
+	LDP  0(R0), (R2, R3)
+	LDP  16(R0), (R4, R5)
+	LDP  32(R0), (R6, R7)
+	MOVD 0(R24), R1
+	MOVD $const_qInvNeg, R15
+	LDP  ·qElement+0(SB), (R17, R19)
+	LDP  ·qElement+16(SB), (R20, R21)
+	LDP  ·qElement+32(SB), (R22, R23)
+	MUL_WORD_0()
+	MOVD 8(R24), R1
+	MUL_WORD_N()
+	MOVD 16(R24), R1
+	MUL_WORD_N()
+	MOVD 24(R24), R1
+	MUL_WORD_N()
+	MOVD 32(R24), R1
+	MUL_WORD_N()
+	MOVD 40(R24), R1
+	MUL_WORD_N()
+
+	// reduce if necessary
+	SUBS R17, R8, R17
+	SBCS R19, R9, R19
+	SBCS R20, R10, R20
+	SBCS R21, R11, R21
+	SBCS R22, R12, R22
+	SBCS R23, R13, R23
+	MOVD res+0(FP), R0
+	CSEL CS, R17, R8, R8
+	CSEL CS, R19, R9, R9
+	STP  (R8, R9), 0(R0)
+	CSEL CS, R20, R10, R10
+	CSEL CS, R21, R11, R11
+	STP  (R10, R11), 16(R0)
+	CSEL CS, R22, R12, R12
+	CSEL CS, R23, R13, R13
+	STP  (R12, R13), 32(R0)
+	RET
+
+// reduce(res *Element)
+TEXT ·reduce(SB), NOFRAME|NOSPLIT, $0-8
+	LDP  ·qElement+0(SB), (R6, R7)
+	LDP  ·qElement+16(SB), (R8, R9)
+	LDP  ·qElement+32(SB), (R10, R11)
+	MOVD res+0(FP), R12
+	LDP  0(R12), (R0, R1)
+	LDP  16(R12), (R2, R3)
+	LDP  32(R12), (R4, R5)
+
+	// q = t - q
+	SUBS R6, R0, R6
+	SBCS R7, R1, R7
+	SBCS R8, R2, R8
+	SBCS R9, R3, R9
+	SBCS R10, R4, R10
+	SBCS R11, R5, R11
+
+	// if no borrow, return q, else return t
+	CSEL CS, R6, R0, R0
+	CSEL CS, R7, R1, R1
+	STP  (R0, R1), 0(R12)
+	CSEL CS, R8, R2, R2
+	CSEL CS, R9, R3, R3
+	STP  (R2, R3), 16(R12)
+	CSEL CS, R10, R4, R4
+	CSEL CS, R11, R5, R5
+	STP  (R4, R5), 32(R12)
+	RET
